\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\new@tpo@label[2]{}
\@nameuse{bbl@beforestart}
\catcode `"\active 
\catcode `<\active 
\catcode `>\active 
\@nameuse{es@quoting}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\BKM@entry[2]{}
\BKM@entry{id=1,dest={7469746C652E31},srcline={10}}{545C33353574756C6F}
\babel@aux{spanish}{}
\BKM@entry{id=2,dest={446564696361746F7269612E31},srcline={9}}{446564696361746F726961}
\BKM@entry{id=3,dest={746F632E30},srcline={8}}{5C3331356E646963652067656E6572616C}
\BKM@entry{id=4,dest={636861707465722A2E37},srcline={8}}{41677261646563696D69656E746F73}
\@writefile{toc}{\contentsline {chapter}{\nonumberline Agradecimientos}{\es@scroman  {xiii}}{chapter*.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\BKM@entry{id=5,dest={636861707465722A2E38},srcline={9}}{4162737472616374}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{\nonumberline Abstract}{\es@scroman  {xv}}{chapter*.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\babel@aux{spanish}{}
\BKM@entry{id=6,dest={636861707465722A2E39},srcline={11}}{526573756D656E}
\@writefile{toc}{\contentsline {chapter}{\nonumberline Resumen}{\es@scroman  {xvii}}{chapter*.9}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\BKM@entry{id=7,dest={706172742E31},srcline={235}}{416E5C3334316C6973697320646520526564657320436F6E766F6C7563696F6E616C6573}
\@writefile{toc}{\contentsline {part}{\numberline {I}Análisis de Redes Convolucionales}{1}{part.1}\protected@file@percent }
\BKM@entry{id=8,dest={636861707465722E31},srcline={5}}{496E74726F64756363695C3336336E}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introducción}{3}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Los tres coches deben identificarse como iguales, aunque se ecuentren desplazados.}}{3}{figure.1.1}\protected@file@percent }
\newlabel{fig:invarianza_traslaciones}{{1.1}{3}{Los tres coches deben identificarse como iguales, aunque se ecuentren desplazados}{figure.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Acción de un difeomorfismo en una rejilla.}}{4}{figure.1.2}\protected@file@percent }
\newlabel{fig:difeomorfismo}{{1.2}{4}{Acción de un difeomorfismo en una rejilla}{figure.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Todas las imágenes deberían clasificarse como 5, pese a las deformaciones.}}{4}{figure.1.3}\protected@file@percent }
\newlabel{fig:deformaciones_5}{{1.3}{4}{Todas las imágenes deberían clasificarse como 5, pese a las deformaciones}{figure.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Deformación excesiva que permite confundir el 1 con el 2 cuando se le aplica el difeomorfismo. Por eso nos centramos en \emph  {``pequeñas''} deformaciones, para no alterar la identidad del objeto en la imagen.}}{4}{figure.1.4}\protected@file@percent }
\newlabel{fig:deformaciones_1}{{1.4}{4}{Deformación excesiva que permite confundir el 1 con el 2 cuando se le aplica el difeomorfismo. Por eso nos centramos en \entrecomillado {pequeñas} deformaciones, para no alterar la identidad del objeto en la imagen}{figure.1.4}{}}
\newlabel{eq::distancia}{{1.1}{5}{}{equation.1.0.1}{}}
\citation{doi:10.1137/S0036141002404838}
\newlabel{def::Lipschitz_cont}{{1.0.7}{6}{}{teorema.1.0.7}{}}
\newlabel{eq::Lipschitz_condition}{{1.2}{6}{}{equation.1.0.2}{}}
\@writefile{brf}{\backcite{doi:10.1137/S0036141002404838}{{6}{1.0.7}{equation.1.0.2}}}
\MT@newlabel{eq::distancia}
\MT@newlabel{eq::Lipschitz_condition}
\BKM@entry{id=9,dest={636861707465722E32},srcline={4}}{4D6F64656C697A6163695C3336336E204D6174656D5C3334317469636120646520756E6120526564204E6575726F6E616C20436F6E766F6C7563696F6E616C}
\citation{GroupInvariantScattering}
\BKM@entry{id=10,dest={73656374696F6E2E322E31},srcline={21}}{446520466F75726965722061206C6173206F6E64656C65746173206465204C6974746C65776F6F642D50616C6579}
\BKM@entry{id=11,dest={73756273656374696F6E2E322E312E31},srcline={23}}{456C206D5C33363364756C6F206465206C61205472616E73666F726D61646120646520466F7572696572}
\citation{DigitalImageProcessing}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Modelización Matemática de una Red Neuronal Convolucional}{7}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{brf}{\backcite{GroupInvariantScattering}{{7}{2}{chapter.2}}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}De Fourier a las ondeletas de Littlewood-Paley}{7}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}El módulo de la Transformada de Fourier}{7}{subsection.2.1.1}\protected@file@percent }
\@writefile{brf}{\backcite{DigitalImageProcessing}{{7}{2.1.1}{subsection.2.1.1}}}
\newlabel{lema::invarianza_traslaciones}{{2.1.2}{8}{}{teorema.2.1.2}{}}
\newlabel{lemma:TF_inestable_difeomorfismos}{{2.1.3}{8}{}{teorema.2.1.3}{}}
\newlabel{eq::lipschitz_condition}{{2.1}{8}{}{equation.2.1.1}{}}
\newlabel{eq:res_auxiliar_1}{{2.2}{9}{}{equation.2.1.2}{}}
\newlabel{eq:res_auxiliar_2}{{2.3}{9}{}{equation.2.1.3}{}}
\MT@newlabel{eq:res_auxiliar_1}
\MT@newlabel{eq:res_auxiliar_2}
\newlabel{eq::1.1}{{2.1.3}{11}{}{equation.2.1.3}{}}
\newlabel{eq::Plancharel}{{2.4}{11}{}{equation.2.1.4}{}}
\MT@newlabel{eq::lipschitz_condition}
\BKM@entry{id=12,dest={73756273656374696F6E2E322E312E32},srcline={217}}{416C7465726E61746976613A204C6173206F6E64656C65746173}
\citation{MallatWavelets}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Como podemos ver en la imagen, para los valores $\epsilon =0.1$, $\xi =100$ y $M=5$ ambas funciones tienen soporte casi disjunto de manera que la diferencia entre ellas en el intervalo $[-5,5]$ coincide prácticamente con $g_1(t)$.}}{12}{figure.2.1}\protected@file@percent }
\newlabel{fig:Grafica_funciones}{{2.1}{12}{Como podemos ver en la imagen, para los valores $\epsilon =0.1$, $\xi =100$ y $M=5$ ambas funciones tienen soporte casi disjunto de manera que la diferencia entre ellas en el intervalo $[-5,5]$ coincide prácticamente con $g_1(t)$}{figure.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Alternativa: Las ondeletas}{12}{subsection.2.1.2}\protected@file@percent }
\@writefile{brf}{\backcite{MallatWavelets}{{12}{2.1.2}{subsection.2.1.2}}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Representación gráfica de la ondeleta de Haar.}}{13}{figure.2.2}\protected@file@percent }
\newlabel{fig:Ondeleta_de_Haar}{{2.2}{13}{Representación gráfica de la ondeleta de Haar}{figure.2.2}{}}
\citation{MallatWavelets}
\citation{HaarBasis}
\citation{HaarBasis}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces En el ejemplo $1$ podemos ver un ejemplo de filtro generado por el soporte de una ondeleta para la detección de líneas bordes verticales, en la imagen $2$ podemos ver un ejemplo de filtro generado por el soporte de una ondeleta para la detección de bordes horizontales, y en el ejemplo $3$ para las diagonales. Todas tienen soporte cuadrado.}}{14}{figure.2.3}\protected@file@percent }
\newlabel{fig:base_haar}{{2.3}{14}{En el ejemplo $1$ podemos ver un ejemplo de filtro generado por el soporte de una ondeleta para la detección de líneas bordes verticales, en la imagen $2$ podemos ver un ejemplo de filtro generado por el soporte de una ondeleta para la detección de bordes horizontales, y en el ejemplo $3$ para las diagonales. Todas tienen soporte cuadrado}{figure.2.3}{}}
\@writefile{brf}{\backcite{MallatWavelets}{{14}{2}{Hfootnote.3}}}
\BKM@entry{id=13,dest={73756273656374696F6E2E322E312E33},srcline={334}}{4C61205472616E73666F726D616461206465204C6974746C65776F6F642D50616C6579}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Ejemplos de aplicar la base de Haar a dos imágenes.Los filtros resaltan los bordes en tres direcciones, horizontal (derecha) vertical (abajo) y en diagonal (abajo derecha). Imagen extraida de \cite  {HaarBasis}.}}{15}{figure.2.4}\protected@file@percent }
\@writefile{brf}{\backcite{HaarBasis}{{15}{2.4}{figure.2.4}}}
\newlabel{fig:ejemplo_haar}{{2.4}{15}{Ejemplos de aplicar la base de Haar a dos imágenes.Los filtros resaltan los bordes en tres direcciones, horizontal (derecha) vertical (abajo) y en diagonal (abajo derecha). Imagen extraida de \cite {HaarBasis}}{figure.2.4}{}}
\citation{WAVELETS}
\citation{WAVELETS}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}La Transformada de Littlewood-Paley}{16}{subsection.2.1.3}\protected@file@percent }
\newlabel{ch:seccion12}{{2.1.3}{16}{La Transformada de Littlewood-Paley}{subsection.2.1.3}{}}
\newlabel{Teorema::Convolucion}{{2.1.5}{16}{}{teorema.2.1.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Como podemos ver, en el caso de la izquierda la ondleta (en color azul) se ve afectada por una menor escala, lo que le permitirá detectar con mayores frecuencias los cambios que se producen en la señal al convolucionar con esta a lo largo del tiempo. En cambio, en el segundo caso, se ve afectada por una escala mayo, lo que le impedirá detectar con tanta precusion los cambios que se produzcan en la señal. Imagen extraida de \cite  {WAVELETS}.}}{17}{figure.2.5}\protected@file@percent }
\@writefile{brf}{\backcite{WAVELETS}{{17}{2.5}{figure.2.5}}}
\newlabel{fig:scattering_propagator}{{2.5}{17}{Como podemos ver, en el caso de la izquierda la ondleta (en color azul) se ve afectada por una menor escala, lo que le permitirá detectar con mayores frecuencias los cambios que se producen en la señal al convolucionar con esta a lo largo del tiempo. En cambio, en el segundo caso, se ve afectada por una escala mayo, lo que le impedirá detectar con tanta precusion los cambios que se produzcan en la señal. Imagen extraida de \cite {WAVELETS}}{figure.2.5}{}}
\newlabel{eq::norma}{{2.5}{17}{La Transformada de Littlewood-Paley}{equation.2.1.5}{}}
\newlabel{unitario}{{2.1.6}{18}{}{teorema.2.1.6}{}}
\newlabel{eq::1.2}{{2.6}{18}{}{equation.2.1.6}{}}
\MT@newlabel{eq::1.2}
\newlabel{eq::1.3}{{2.7}{18}{La Transformada de Littlewood-Paley}{equation.2.1.7}{}}
\MT@newlabel{eq::1.2}
\MT@newlabel{eq::1.2}
\MT@newlabel{eq::1.3}
\BKM@entry{id=14,dest={73756273656374696F6E2E322E312E34},srcline={512}}{436F6E76656E696F73207061726120667574757261732073656363696F6E6573}
\MT@newlabel{eq::1.2}
\MT@newlabel{eq::1.3}
\MT@newlabel{eq::1.3}
\MT@newlabel{eq::Plancharel}
\MT@newlabel{eq::norma}
\MT@newlabel{eq::1.3}
\MT@newlabel{eq::1.3}
\MT@newlabel{eq::1.3}
\MT@newlabel{eq::1.2}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Convenios para futuras secciones}{19}{subsection.2.1.4}\protected@file@percent }
\MT@newlabel{eq::1.2}
\BKM@entry{id=15,dest={73656374696F6E2E322E32},srcline={534}}{456C206F70657261646F722064652064697370657273695C3336336E20736F62726520756E2063616D696E6F206F7264656E61646F}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}El operador de dispersión sobre un camino ordenado}{20}{section.2.2}\protected@file@percent }
\newlabel{lema:Invarianza_traslaciones_integral}{{2.2.1}{20}{}{teorema.2.2.1}{}}
\BKM@entry{id=16,dest={73756273656374696F6E2E322E322E31},srcline={595}}{456A656D706C6F2070617261206F6274656E657220636F6566696369656E74657320696E76617269616E74657320706F7220747261736C6163696F6E6573}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Ejemplo para obtener coeficientes invariantes por traslaciones}{21}{subsection.2.2.1}\protected@file@percent }
\newlabel{eq::1.4}{{2.8}{21}{Ejemplo para obtener coeficientes invariantes por traslaciones}{equation.2.2.8}{}}
\BKM@entry{id=17,dest={73756273656374696F6E2E322E322E32},srcline={632}}{456C206F70657261646F72206D5C33363364756C6F2E}
\citation{JBrunaOperatorsCommutingDiff}
\citation{GroupInvariantScattering}
\citation{bruna2013invariant}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}El operador módulo.}{22}{subsection.2.2.2}\protected@file@percent }
\@writefile{brf}{\backcite{JBrunaOperatorsCommutingDiff}{{22}{2.2.2}{subsection.2.2.2}}}
\@writefile{brf}{\backcite{GroupInvariantScattering}{{22}{2.2.2}{subsection.2.2.2}}}
\@writefile{brf}{\backcite{bruna2013invariant}{{22}{2.2.2}{subsection.2.2.2}}}
\MT@newlabel{eq::1.4}
\BKM@entry{id=18,dest={73756273656374696F6E2E322E322E33},srcline={713}}{50726F706965646164657320646520756E2063616D696E6F206465206672656375656E636961732E}
\BKM@entry{id=19,dest={73756273656374696F6E2E322E322E34},srcline={747}}{436F6E737472756363695C3336336E2064656C206F70657261646F722064652064697370657273695C3336336E2E}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Propiedades de un camino de frecuencias.}{24}{subsection.2.2.3}\protected@file@percent }
\newlabel{proposicionSumaCaminos}{{2.2.6}{24}{}{teorema.2.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Construcción del operador de dispersión.}{24}{subsection.2.2.4}\protected@file@percent }
\newlabel{def:S_barra}{{2.2.7}{24}{}{teorema.2.2.7}{}}
\BKM@entry{id=20,dest={73656374696F6E2E322E33},srcline={820}}{50726F70616761646F722064652064697370657273695C3336336E207920636F6E736572766163695C3336336E206465206C61204E6F726D61}
\BKM@entry{id=21,dest={73756273656374696F6E2E322E332E31},srcline={823}}{50726F6365736F2064652064697370657273695C3336336E2064656C2070726F70616761646F722E}
\citation{bruna2013invariant}
\citation{bruna2013invariant}
\BKM@entry{id=22,dest={73756273656374696F6E2E322E332E32},srcline={868}}{4469666572656E6369617320792073696D696C69747564657320636F6E20756E6120434E4E}
\citation{lecun2015deep}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Propagador de dispersión y conservación de la Norma}{26}{section.2.3}\protected@file@percent }
\newlabel{ch:seccion13}{{2.3}{26}{Propagador de dispersión y conservación de la Norma}{section.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Proceso de dispersión del propagador.}{26}{subsection.2.3.1}\protected@file@percent }
\newlabel{eq::1.5}{{2.9}{26}{Proceso de dispersión del propagador}{equation.2.3.9}{}}
\BKM@entry{id=23,dest={73756273656374696F6E2E322E332E33},srcline={884}}{52656C6163695C3336336E20636F6E2068657272616D69656E74617320636C5C333431736963617320646520766973695C3336336E20706F7220636F6D70757461646F72}
\citation{DistinctiveImageFeatures}
\citation{Daisy}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Un PD $U_J$ aplicado a un punto de una señal $f(x)$ calcula $U[\lambda _1]f(x)=|f(x)\ast \psi _{\lambda _1}|$ y como salida a la capa $m=0$ se promedian los coeficientes que han dado $0$ (por tener $2^j<2^{-J}$) obteniendo como salida $S_J[\emptyset ]f(x)=f(x)\ast \phi _{2^J}$ (como se puede ver en la flecha negra). Después se aplica de nuevo $U_J$ a cada coeficiente $U[\lambda _1]f(x)$ del paso anterior ($m=1$) $U[\lambda _1,\lambda 2]f(x)$ obteniendo como salida $S_J[\lambda _1]f(x)=U[\lambda _1]f(x) \ast \phi _{2^J}$. Se repite este proceso de manera recursiva para cada coeficiente $U[p]f(x)$ y obteniendo como resultado $S_J[p]f(x)=U[p]f(x) \ast \phi _{2^J}$. Imagen extraida de \cite  {bruna2013invariant}.}}{27}{figure.2.6}\protected@file@percent }
\@writefile{brf}{\backcite{bruna2013invariant}{{27}{2.6}{figure.2.6}}}
\newlabel{fig:scattering_propagator}{{2.6}{27}{Un PD $U_J$ aplicado a un punto de una señal $f(x)$ calcula $U[\lambda _1]f(x)=|f(x)\ast \psi _{\lambda _1}|$ y como salida a la capa $m=0$ se promedian los coeficientes que han dado $0$ (por tener $2^j<2^{-J}$) obteniendo como salida $S_J[\emptyset ]f(x)=f(x)\ast \phi _{2^J}$ (como se puede ver en la flecha negra). Después se aplica de nuevo $U_J$ a cada coeficiente $U[\lambda _1]f(x)$ del paso anterior ($m=1$) $U[\lambda _1,\lambda 2]f(x)$ obteniendo como salida $S_J[\lambda _1]f(x)=U[\lambda _1]f(x) \ast \phi _{2^J}$. Se repite este proceso de manera recursiva para cada coeficiente $U[p]f(x)$ y obteniendo como resultado $S_J[p]f(x)=U[p]f(x) \ast \phi _{2^J}$. Imagen extraida de \cite {bruna2013invariant}}{figure.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Diferencias y similitudes con una CNN}{27}{subsection.2.3.2}\protected@file@percent }
\@writefile{brf}{\backcite{lecun2015deep}{{27}{2.3.2}{subsection.2.3.2}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Relación con herramientas clásicas de visión por computador}{27}{subsection.2.3.3}\protected@file@percent }
\@writefile{brf}{\backcite{DistinctiveImageFeatures}{{27}{2.3.3}{subsection.2.3.3}}}
\BKM@entry{id=24,dest={73756273656374696F6E2E322E332E34},srcline={888}}{4F70657261646F72206E6F20657870616E7369766F2E}
\@writefile{brf}{\backcite{Daisy}{{28}{2.3.3}{subsection.2.3.3}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Operador no expansivo.}{28}{subsection.2.3.4}\protected@file@percent }
\newlabel{proposicion::NoExpansiva}{{2.3.1}{28}{}{teorema.2.3.1}{}}
\MT@newlabel{eq::1.5}
\BKM@entry{id=25,dest={73756273656374696F6E2E322E332E35},srcline={959}}{436F6E736572766163695C3336336E206465206C61206E6F726D612E}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Conservación de la norma.}{29}{subsection.2.3.5}\protected@file@percent }
\newlabel{lema::Cota_inferior}{{2.3.2}{29}{}{teorema.2.3.2}{}}
\citation{GroupInvariantScattering}
\newlabel{eq::1.6}{{2.10}{30}{}{equation.2.3.10}{}}
\newlabel{eq::1.7}{{2.11}{30}{}{equation.2.3.11}{}}
\newlabel{lema::Admisibilidad}{{2.3.4}{30}{}{teorema.2.3.4}{}}
\MT@newlabel{eq::1.7}
\newlabel{eq::1.9}{{2.12}{30}{}{equation.2.3.12}{}}
\@writefile{brf}{\backcite{GroupInvariantScattering}{{30}{2.3.5}{equation.2.3.12}}}
\newlabel{teoremaOndeletasAdmisibles}{{2.3.5}{30}{}{teorema.2.3.5}{}}
\MT@newlabel{eq::1.6}
\MT@newlabel{eq::1.9}
\newlabel{eq::1.8}{{2.13}{31}{Conservación de la norma}{equation.2.3.13}{}}
\MT@newlabel{eq::1.8}
\BKM@entry{id=26,dest={73756273656374696F6E2E322E332E36},srcline={1146}}{436F6E636C7573696F6E6573206578747261696461732064656C2074656F72656D61}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.6}Conclusiones extraidas del teorema}{33}{subsection.2.3.6}\protected@file@percent }
\BKM@entry{id=27,dest={636861707465722E33},srcline={3}}{496E76617269616E7A6120706F7220547261736C6163696F6E6573}
\BKM@entry{id=28,dest={73656374696F6E2E332E31},srcline={7}}{4E6F20657870616E736976696461642064656C206F70657261646F722064652076656E74616E6120656E20636F6E6A756E746F732064652063616D696E6F73}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Invarianza por Traslaciones}{35}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:seccion14}{{3}{35}{Invarianza por Traslaciones}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}No expansividad del operador de ventana en conjuntos de caminos}{35}{section.3.1}\protected@file@percent }
\newlabel{eq::1.10}{{3.1}{35}{}{equation.3.1.1}{}}
\MT@newlabel{eq::1.10}
\newlabel{eq::1.11}{{3.2}{35}{No expansividad del operador de ventana en conjuntos de caminos}{equation.3.1.2}{}}
\MT@newlabel{eq::1.11}
\MT@newlabel{eq::1.10}
\MT@newlabel{eq::1.11}
\BKM@entry{id=29,dest={73656374696F6E2E332E32},srcline={142}}{496E76617269616E7A6120706F7220747261736C6163696F6E6573}
\MT@newlabel{eq::1.11}
\citation{SchurLemma}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Invarianza por traslaciones}{38}{section.3.2}\protected@file@percent }
\MT@newlabel{eq::1.7}
\newlabel{lema::constante}{{3.2.1}{38}{}{teorema.3.2.1}{}}
\@writefile{brf}{\backcite{SchurLemma}{{38}{3.2}{teorema.3.2.1}}}
\newlabel{invarianzaTraslaciones}{{3.2.2}{40}{}{teorema.3.2.2}{}}
\MT@newlabel{eq::1.7}
\MT@newlabel{eq::1.9}
\BKM@entry{id=30,dest={636861707465722E34},srcline={3}}{436F6E636C7573696F6E657320792054726162616A6F732066757475726F73}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Conclusiones y Trabajos futuros}{43}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\BKM@entry{id=31,dest={706172742E32},srcline={242}}{4C6F63616C697A6163695C3336336E206465206C616E646D61726B7320636566616C6F6D5C333531747269636F7320706F72206D6564696F20646520745C333531636E69636173206465206665772D73686F74206C6561726E696E67}
\@writefile{toc}{\contentsline {part}{\numberline {II}Localización de landmarks cefalométricos por medio de técnicas de few-shot learning}{45}{part.2}\protected@file@percent }
\BKM@entry{id=32,dest={636861707465722E35},srcline={4}}{496E74726F64756363695C3336336E}
\BKM@entry{id=33,dest={73656374696F6E2E352E31},srcline={22}}{4465736372697063695C3336336E2064656C2070726F626C656D61}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Introducción}{47}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:Introduccion_informatica}{{5}{47}{Introducción}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Descripción del problema}{47}{section.5.1}\protected@file@percent }
\citation{damas2020handbook}
\citation{damas2020handbook}
\citation{damas2020handbook}
\citation{damas2020handbook}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Ejemplo del proceso a seguir por el antropólogo forense en la superposición cráneo facial. Imagen extraida de \cite  {damas2020handbook}.}}{48}{figure.5.1}\protected@file@percent }
\@writefile{brf}{\backcite{damas2020handbook}{{48}{5.1}{figure.5.1}}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces En esta imagen podemos ver algunos de los landmarks que se van a estudiar marcados tanto en rostro (cefalométricos) como en cráneo (craneométricos). Imagen extraida de \cite  {damas2020handbook}.}}{49}{figure.5.2}\protected@file@percent }
\@writefile{brf}{\backcite{damas2020handbook}{{49}{5.2}{figure.5.2}}}
\BKM@entry{id=34,dest={73656374696F6E2E352E32},srcline={100}}{4D6F7469766163695C3336336E}
\citation{Huete2015PastPA}
\citation{bermejo2021automatic}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Motivación}{50}{section.5.2}\protected@file@percent }
\@writefile{brf}{\backcite{Huete2015PastPA}{{50}{5.2}{section.5.2}}}
\@writefile{brf}{\backcite{bermejo2021automatic}{{50}{5.2}{section.5.2}}}
\citation{browatzki20203fabrec}
\BKM@entry{id=35,dest={73656374696F6E2E352E33},srcline={131}}{4F626A657469766F73}
\@writefile{brf}{\backcite{browatzki20203fabrec}{{51}{5.2}{section.5.2}}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Objetivos}{51}{section.5.3}\protected@file@percent }
\BKM@entry{id=36,dest={73656374696F6E2E352E34},srcline={144}}{506C616E696669636163695C3336336E}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Planificación}{52}{section.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Diseño en cascada retroalimentado empleado.}}{52}{figure.5.3}\protected@file@percent }
\newlabel{Fig::Ciclo de vida}{{5.3}{52}{Diseño en cascada retroalimentado empleado}{figure.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Planificación original.}}{53}{figure.5.4}\protected@file@percent }
\newlabel{Fig::Planificacion original}{{5.4}{53}{Planificación original}{figure.5.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Planificación original.}}{54}{figure.5.5}\protected@file@percent }
\newlabel{Fig::Planificacion final}{{5.5}{54}{Planificación original}{figure.5.5}{}}
\BKM@entry{id=37,dest={636861707465722E36},srcline={2}}{46756E64616D656E746F732054655C3336337269636F732079204D5C333531746F646F73}
\BKM@entry{id=38,dest={73656374696F6E2E362E31},srcline={6}}{417072656E64697A616A65204175746F6D5C3334317469636F}
\BKM@entry{id=39,dest={73756273656374696F6E2E362E312E31},srcline={36}}{417072656E64697A616A6520537570657276697361646F}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Fundamentos Teóricos y Métodos}{55}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Aprendizaje Automático}{55}{section.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Aprendizaje Supervisado}{56}{subsection.6.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1.1}Regresión}{56}{subsubsection.6.1.1.1}\protected@file@percent }
\newlabel{section::Regresion}{{6.1.1.1}{56}{Regresión}{subsubsection.6.1.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1.2}Gradiente Descendente}{57}{subsubsection.6.1.1.2}\protected@file@percent }
\BKM@entry{id=40,dest={73756273656374696F6E2E362E312E32},srcline={158}}{417072656E64697A616A65206E6F20537570657276697361646F}
\BKM@entry{id=41,dest={73756273656374696F6E2E362E312E33},srcline={161}}{417072656E64697A616A65204175746F6D5C3334317469636F20656E20657374652054726162616A6F}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1.3}Gradiente Descendente Estocástico}{58}{subsubsection.6.1.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1.4}Clasificación}{58}{subsubsection.6.1.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}Aprendizaje no Supervisado}{58}{subsection.6.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.3}Aprendizaje Automático en este Trabajo}{58}{subsection.6.1.3}\protected@file@percent }
\BKM@entry{id=42,dest={73656374696F6E2E362E32},srcline={169}}{566973695C3336336E20706F7220436F6D70757461646F72}
\citation{rosenfeld1988computer}
\BKM@entry{id=43,dest={73656374696F6E2E362E33},srcline={180}}{44656570204C6561726E696E67}
\BKM@entry{id=44,dest={73756273656374696F6E2E362E332E31},srcline={187}}{5265646573204E6575726F6E616C6573}
\citation{Goodfellow-et-al-2016}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Visión por Computador}{59}{section.6.2}\protected@file@percent }
\@writefile{brf}{\backcite{rosenfeld1988computer}{{59}{6.2}{section.6.2}}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Deep Learning}{59}{section.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Redes Neuronales}{59}{subsection.6.3.1}\protected@file@percent }
\newlabel{sub:redes_neuronales}{{6.3.1}{59}{Redes Neuronales}{subsection.6.3.1}{}}
\citation{sharma2017activation}
\@writefile{brf}{\backcite{Goodfellow-et-al-2016}{{60}{6.3.1}{subsection.6.3.1}}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Red neuronal con una capa oculta. Formalmente podría describirse como $f'(x)=(f_3(f_2(f_1(x))))$, dónde $f_1$ hace referencia a la capa de entrada a la red, $f_2$ a la capa oculta y $f_3$ a la capa de salida.}}{60}{figure.6.1}\protected@file@percent }
\newlabel{fig:red_neuronal_capa_oculta}{{6.1}{60}{Red neuronal con una capa oculta. Formalmente podría describirse como $f'(x)=(f_3(f_2(f_1(x))))$, dónde $f_1$ hace referencia a la capa de entrada a la red, $f_2$ a la capa oculta y $f_3$ a la capa de salida}{figure.6.1}{}}
\@writefile{brf}{\backcite{sharma2017activation}{{60}{6.3.1}{figure.6.1}}}
\BKM@entry{id=45,dest={73756273656374696F6E2E362E332E32},srcline={256}}{4261636B2050726F7061676174696F6E}
\citation{StanfordCourse}
\citation{StanfordCourse}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Back Propagation}{61}{subsection.6.3.2}\protected@file@percent }
\citation{StanfordCourse}
\citation{StanfordCourse}
\BKM@entry{id=46,dest={73656374696F6E2E362E34},srcline={290}}{5265646573204E6575726F6E616C657320436F6E766F6C7563696F6E616C6573}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Ejemplo de Grafo computacional junto con la salida para una entrada concreta $x=-2$,$y=5$, $z=-4$. La imagen ha sido extraída del curso \cite  {StanfordCourse}.}}{62}{figure.6.2}\protected@file@percent }
\@writefile{brf}{\backcite{StanfordCourse}{{62}{6.2}{figure.6.2}}}
\newlabel{fig:GrafoComputacional}{{6.2}{62}{Ejemplo de Grafo computacional junto con la salida para una entrada concreta $x=-2$,$y=5$, $z=-4$. La imagen ha sido extraída del curso \cite {StanfordCourse}}{figure.6.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Redes Neuronales Convolucionales}{62}{section.6.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Como podemos ver en la imagen superior, en primer lugar se renombra la salida de la operación $x+y$ por $q$, de manera que $f=qz$. Tras esto se empiezan a calcular las derivadas parciales correspondientes desde el final hacia la entrada, aplicando cuando sea necesario la regla de la cadena hasta obtener la derivada de cada nodo en la imagen de abajo. Las imágenes han sido extraídas de \cite  {StanfordCourse}}}{63}{figure.6.3}\protected@file@percent }
\@writefile{brf}{\backcite{StanfordCourse}{{63}{6.3}{figure.6.3}}}
\newlabel{fig:Ejemplo BP}{{6.3}{63}{Como podemos ver en la imagen superior, en primer lugar se renombra la salida de la operación $x+y$ por $q$, de manera que $f=qz$. Tras esto se empiezan a calcular las derivadas parciales correspondientes desde el final hacia la entrada, aplicando cuando sea necesario la regla de la cadena hasta obtener la derivada de cada nodo en la imagen de abajo. Las imágenes han sido extraídas de \cite {StanfordCourse}}{figure.6.3}{}}
\BKM@entry{id=47,dest={73756273656374696F6E2E362E342E31},srcline={342}}{4361706120436F6E766F6C7563696F6E616C}
\citation{StanfordCourse}
\citation{StanfordCourse}
\citation{StanfordCourse}
\citation{StanfordCourse}
\BKM@entry{id=48,dest={73756273656374696F6E2E362E342E32},srcline={375}}{4361706120646520506F6F6C696E67}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Capa Convolucional}{64}{subsection.6.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Ejemplo de cálculo de mapa de activación en una capa convolucional para un determinado volumen de entrada. El parámetro depth nos dice la cantidad de mapas de activación generamos para el volumen de entrada, o dicho de otro modo, el número de filtros que aplicamos. La imagen ha sido extraída de \cite  {StanfordCourse}}}{65}{figure.6.4}\protected@file@percent }
\@writefile{brf}{\backcite{StanfordCourse}{{65}{6.4}{figure.6.4}}}
\newlabel{fig:mapa_activacion}{{6.4}{65}{Ejemplo de cálculo de mapa de activación en una capa convolucional para un determinado volumen de entrada. El parámetro depth nos dice la cantidad de mapas de activación generamos para el volumen de entrada, o dicho de otro modo, el número de filtros que aplicamos. La imagen ha sido extraída de \cite {StanfordCourse}}{figure.6.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Sucesión de varias capas convolucionales + ReLU que describen la estructura básica de una CNN. Cabe destacar como la produndidad de los filtros es siemore la misma que la del volumen de entrada, de acuerdo a lo que hemos dicho anteriormente. La imagen han sido extraída de \cite  {StanfordCourse}}}{65}{figure.6.5}\protected@file@percent }
\@writefile{brf}{\backcite{StanfordCourse}{{65}{6.5}{figure.6.5}}}
\newlabel{fig:estructura_convnet}{{6.5}{65}{Sucesión de varias capas convolucionales + ReLU que describen la estructura básica de una CNN. Cabe destacar como la produndidad de los filtros es siemore la misma que la del volumen de entrada, de acuerdo a lo que hemos dicho anteriormente. La imagen han sido extraída de \cite {StanfordCourse}}{figure.6.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}Capa de Pooling}{65}{subsection.6.4.2}\protected@file@percent }
\citation{StanfordCourse}
\citation{StanfordCourse}
\BKM@entry{id=49,dest={73756273656374696F6E2E362E342E33},srcline={396}}{4361706120546F74616C6D656E746520436F6E656374616461205C2846756C6C7920436F6E65637465645C29}
\BKM@entry{id=50,dest={73756273656374696F6E2E362E342E34},srcline={403}}{4261746368204E6F726D616C697A6174696F6E}
\BKM@entry{id=51,dest={73756273656374696F6E2E362E342E35},srcline={406}}{4F7074696D697A61646F72204164616D}
\BKM@entry{id=52,dest={73756273656374696F6E2E362E342E36},srcline={409}}{50726F6365736F20646520656E7472656E616D69656E746F20646520756E6120434E4E}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Ejemplo de capa de pooling usando la operación del máximo. La imagen han sido extraída de \cite  {StanfordCourse}}}{66}{figure.6.6}\protected@file@percent }
\@writefile{brf}{\backcite{StanfordCourse}{{66}{6.6}{figure.6.6}}}
\newlabel{fig:pooling}{{6.6}{66}{Ejemplo de capa de pooling usando la operación del máximo. La imagen han sido extraída de \cite {StanfordCourse}}{figure.6.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.3}Capa Totalmente Conectada (Fully Conected)}{66}{subsection.6.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.4}Batch Normalization}{66}{subsection.6.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.5}Optimizador Adam}{66}{subsection.6.4.5}\protected@file@percent }
\BKM@entry{id=53,dest={73756273656374696F6E2E362E342E37},srcline={423}}{45766F6C7563695C3336336E206465206C617320434E4E}
\citation{EvolutionCNN}
\citation{lecun1998gradient}
\citation{lecun1998gradient}
\citation{lecun1998gradient}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.6}Proceso de entrenamiento de una CNN}{67}{subsection.6.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.7}Evolución de las CNN}{67}{subsection.6.4.7}\protected@file@percent }
\@writefile{brf}{\backcite{EvolutionCNN}{{67}{6.4.7}{subsection.6.4.7}}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.7.1}LeNet-5}{67}{subsubsection.6.4.7.1}\protected@file@percent }
\@writefile{brf}{\backcite{lecun1998gradient}{{67}{6.4.7.1}{subsubsection.6.4.7.1}}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Arquitectura de la red LeNet. Como podemos observar tiene capas convolucionales, capas de average pooling y capas totalmente conectadas al final. Imagen extraída de \cite  {lecun1998gradient}.}}{67}{figure.6.7}\protected@file@percent }
\@writefile{brf}{\backcite{lecun1998gradient}{{67}{6.7}{figure.6.7}}}
\newlabel{fig:LeNet}{{6.7}{67}{Arquitectura de la red LeNet. Como podemos observar tiene capas convolucionales, capas de average pooling y capas totalmente conectadas al final. Imagen extraída de \cite {lecun1998gradient}}{figure.6.7}{}}
\citation{krizhevsky2012imagenet}
\citation{krizhevsky2012imagenet}
\citation{krizhevsky2012imagenet}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.7.2}AlexNet}{68}{subsubsection.6.4.7.2}\protected@file@percent }
\@writefile{brf}{\backcite{krizhevsky2012imagenet}{{68}{6.4.7.2}{subsubsection.6.4.7.2}}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces Arquitectura de la red AlexNet. Destaca que parece estar \emph  {``partida''} en dos mitades, esto es porque por primera vez se usaron las GPUs para entrenar la red de manera que una GPU realizaba la parte superior de la arquitectura y otra la parte inferior. Imagen extraída de \cite  {krizhevsky2012imagenet}.}}{68}{figure.6.8}\protected@file@percent }
\@writefile{brf}{\backcite{krizhevsky2012imagenet}{{68}{6.8}{figure.6.8}}}
\newlabel{fig:AlexNet}{{6.8}{68}{Arquitectura de la red AlexNet. Destaca que parece estar \entrecomillado {partida} en dos mitades, esto es porque por primera vez se usaron las GPUs para entrenar la red de manera que una GPU realizaba la parte superior de la arquitectura y otra la parte inferior. Imagen extraída de \cite {krizhevsky2012imagenet}}{figure.6.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.7.3}GoogLeNet}{68}{subsubsection.6.4.7.3}\protected@file@percent }
\citation{szegedy2015going}
\citation{simonyan2014very}
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces Ejemplos de módulos Inception.}}{69}{figure.6.9}\protected@file@percent }
\newlabel{fig:inception}{{6.9}{69}{Ejemplos de módulos Inception}{figure.6.9}{}}
\@writefile{brf}{\backcite{szegedy2015going}{{69}{6.4.7.3}{figure.6.9}}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.10}{\ignorespaces Arquitectura de GoogLeNet usando módulos Inception.}}{69}{figure.6.10}\protected@file@percent }
\newlabel{fig:GoogLeNet}{{6.10}{69}{Arquitectura de GoogLeNet usando módulos Inception}{figure.6.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.7.4}VGG-16}{69}{subsubsection.6.4.7.4}\protected@file@percent }
\@writefile{brf}{\backcite{simonyan2014very}{{69}{6.4.7.4}{subsubsection.6.4.7.4}}}
\citation{he2016deep}
\citation{he2016deep}
\citation{he2016deep}
\citation{StanfordCourse}
\citation{StanfordCourse}
\BKM@entry{id=54,dest={73656374696F6E2E362E35},srcline={537}}{4175746F656E636F64657273}
\@writefile{lof}{\contentsline {figure}{\numberline {6.11}{\ignorespaces Arquitectura de VGG-16.}}{70}{figure.6.11}\protected@file@percent }
\newlabel{fig:VGG}{{6.11}{70}{Arquitectura de VGG-16}{figure.6.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.7.5}ResNet}{70}{subsubsection.6.4.7.5}\protected@file@percent }
\@writefile{brf}{\backcite{he2016deep}{{70}{6.4.7.5}{subsubsection.6.4.7.5}}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.12}{\ignorespaces Bloque residual de una ResNet. Como vemos, se suma el tensor $x$ con el tensor $\mathcal  {F}(x)$ que surge unas etapas después. Imagen extraída de \cite  {he2016deep}.}}{70}{figure.6.12}\protected@file@percent }
\@writefile{brf}{\backcite{he2016deep}{{70}{6.12}{figure.6.12}}}
\newlabel{fig:Resnet}{{6.12}{70}{Bloque residual de una ResNet. Como vemos, se suma el tensor $x$ con el tensor $\mathcal {F}(x)$ que surge unas etapas después. Imagen extraída de \cite {he2016deep}}{figure.6.12}{}}
\BKM@entry{id=55,dest={73756273656374696F6E2E362E352E31},srcline={540}}{496E74726F64756363695C3336336E}
\citation{autoencoders2017}
\citation{autoencoders2017}
\citation{autoencoders2017}
\BKM@entry{id=56,dest={73756273656374696F6E2E362E352E32},srcline={566}}{45766F6C7563695C3336336E206465206C6F73204175746F656E636F64657273}
\citation{GAN}
\@writefile{lof}{\contentsline {figure}{\numberline {6.13}{\ignorespaces Resumen de los ganadores del concurso ILSVRC hasta 2015 con la aparición de Resnet. Imagen extraída de \cite  {StanfordCourse}.}}{71}{figure.6.13}\protected@file@percent }
\@writefile{brf}{\backcite{StanfordCourse}{{71}{6.13}{figure.6.13}}}
\newlabel{fig:ImageNet}{{6.13}{71}{Resumen de los ganadores del concurso ILSVRC hasta 2015 con la aparición de Resnet. Imagen extraída de \cite {StanfordCourse}}{figure.6.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Autoencoders}{71}{section.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}Introducción}{71}{subsection.6.5.1}\protected@file@percent }
\@writefile{brf}{\backcite{autoencoders2017}{{71}{6.5.1}{subsection.6.5.1}}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.14}{\ignorespaces Esquema de un Autoencoder básico. Imagen extraída de \cite  {autoencoders2017}.}}{71}{figure.6.14}\protected@file@percent }
\@writefile{brf}{\backcite{autoencoders2017}{{71}{6.14}{figure.6.14}}}
\newlabel{fig:Autoencoder}{{6.14}{71}{Esquema de un Autoencoder básico. Imagen extraída de \cite {autoencoders2017}}{figure.6.14}{}}
\citation{autoencoders2017}
\citation{autoencoders2017}
\citation{autoencoders2017}
\citation{autoencoders2017}
\citation{AAE}
\citation{AAE}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.2}Evolución de los Autoencoders}{72}{subsection.6.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.2.1}Generative Adversarial Networks (GANs)}{72}{subsubsection.6.5.2.1}\protected@file@percent }
\@writefile{brf}{\backcite{GAN}{{72}{6.5.2.1}{subsubsection.6.5.2.1}}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.15}{\ignorespaces Ejemplo de una Generative Network que pretende aprender la distribución de probabilidad de un conjunto de imágenes de perros. Imagen extraída de \cite  {autoencoders2017}.}}{72}{figure.6.15}\protected@file@percent }
\@writefile{brf}{\backcite{autoencoders2017}{{72}{6.15}{figure.6.15}}}
\newlabel{fig:Generative Network}{{6.15}{72}{Ejemplo de una Generative Network que pretende aprender la distribución de probabilidad de un conjunto de imágenes de perros. Imagen extraída de \cite {autoencoders2017}}{figure.6.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.16}{\ignorespaces Resumen del proceso de entrenamiento de una GAN. Imagen extraída de \cite  {autoencoders2017}.}}{72}{figure.6.16}\protected@file@percent }
\@writefile{brf}{\backcite{autoencoders2017}{{72}{6.16}{figure.6.16}}}
\newlabel{fig:Generative Network}{{6.16}{72}{Resumen del proceso de entrenamiento de una GAN. Imagen extraída de \cite {autoencoders2017}}{figure.6.16}{}}
\citation{VAE}
\citation{VAE}
\citation{VAE}
\citation{VAE}
\@writefile{lof}{\contentsline {figure}{\numberline {6.17}{\ignorespaces Ejemplo de la arquitectura de una GAN para generar imágenes de dígitos manuscritos. Imagen extraída de \cite  {AAE}}}{73}{figure.6.17}\protected@file@percent }
\@writefile{brf}{\backcite{AAE}{{73}{6.17}{figure.6.17}}}
\newlabel{fig:GAN}{{6.17}{73}{Ejemplo de la arquitectura de una GAN para generar imágenes de dígitos manuscritos. Imagen extraída de \cite {AAE}}{figure.6.17}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.2.2}Variational Autoencoder (VAE)}{73}{subsubsection.6.5.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.18}{\ignorespaces Diferencia en el tratamiento de los datos por parte del encoder en una VAE y un Autoencoder clásico. Imagen extraída de \cite  {VAE}.}}{73}{figure.6.18}\protected@file@percent }
\@writefile{brf}{\backcite{VAE}{{73}{6.18}{figure.6.18}}}
\newlabel{fig:VAE_1}{{6.18}{73}{Diferencia en el tratamiento de los datos por parte del encoder en una VAE y un Autoencoder clásico. Imagen extraída de \cite {VAE}}{figure.6.18}{}}
\citation{AAE}
\citation{AAE}
\@writefile{lof}{\contentsline {figure}{\numberline {6.19}{\ignorespaces Diferencia en la función de pérdida de un Autoencoder clásico (imagen superior) y una VAE (imagen inferior). Destacamos el término KL regularizante que pretende que las distribuciones de los datos sigan una normal estándar. Imagen extraída de \cite  {VAE}.}}{74}{figure.6.19}\protected@file@percent }
\@writefile{brf}{\backcite{VAE}{{74}{6.19}{figure.6.19}}}
\newlabel{fig:VAE_2}{{6.19}{74}{Diferencia en la función de pérdida de un Autoencoder clásico (imagen superior) y una VAE (imagen inferior). Destacamos el término KL regularizante que pretende que las distribuciones de los datos sigan una normal estándar. Imagen extraída de \cite {VAE}}{figure.6.19}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.2.3}Adversarial Autoencoder(AAE)}{74}{subsubsection.6.5.2.3}\protected@file@percent }
\BKM@entry{id=57,dest={73656374696F6E2E362E36},srcline={655}}{545C333531636E6963617320656D706C6561646173}
\BKM@entry{id=58,dest={73756273656374696F6E2E362E362E31},srcline={658}}{4665772D73686F74204C6561726E696E6720792044617461204175676D656E746174696F6E}
\@writefile{lof}{\contentsline {figure}{\numberline {6.20}{\ignorespaces Esquema de la arquitectura de un AAE. El Encoder sería la red a la que entra la imagen $x$, el vector $z$ sería el vector latente, que sirve de entrada al Discriminador $D_{gauss}$ y finalmente el vector $z$ es la entrada del Decoder que reconstruye la imagen. Imagen extraída de \cite  {AAE}.}}{75}{figure.6.20}\protected@file@percent }
\@writefile{brf}{\backcite{AAE}{{75}{6.20}{figure.6.20}}}
\newlabel{fig:AAE}{{6.20}{75}{Esquema de la arquitectura de un AAE. El Encoder sería la red a la que entra la imagen $x$, el vector $z$ sería el vector latente, que sirve de entrada al Discriminador $D_{gauss}$ y finalmente el vector $z$ es la entrada del Decoder que reconstruye la imagen. Imagen extraída de \cite {AAE}}{figure.6.20}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Técnicas empleadas}{75}{section.6.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.1}Few-shot Learning y Data Augmentation}{75}{subsection.6.6.1}\protected@file@percent }
\newlabel{sub:data_augmentation}{{6.6.1}{75}{Few-shot Learning y Data Augmentation}{subsection.6.6.1}{}}
\BKM@entry{id=59,dest={636861707465722E37},srcline={2}}{45737461646F2064656C2041727465}
\BKM@entry{id=60,dest={73656374696F6E2E372E31},srcline={6}}{4C6F63616C697A6163695C3336336E206465206C616E646D61726B7320636566616C6F6D5C333531747269636F7320656E20696D5C33343167656E6573}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Estado del Arte}{77}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Localización de landmarks cefalométricos en imágenes}{77}{section.7.1}\protected@file@percent }
\BKM@entry{id=61,dest={73756273656374696F6E2E372E312E31},srcline={68}}{45766F6C7563695C3336336E20656E206C61206964656E74696669636163695C3336336E20666F72656E7365206465206C616E646D61726B7320636566616C6F6D5C333531747269636F73}
\citation{ibanez2011two}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Gráfica de publicaciones por año obtenida con la primera \textit  {keyword} el $14$ de Julio de $2022$. Destaca el notable incremento de papers a partir de 2012, año en que aparece la red AlexNet y comienza a ganar popularidad el Deep Learning en el tratamiento de imágenes.}}{78}{figure.7.1}\protected@file@percent }
\newlabel{fig:SCOPUS1}{{7.1}{78}{Gráfica de publicaciones por año obtenida con la primera \textit {keyword} el $14$ de Julio de $2022$. Destaca el notable incremento de papers a partir de 2012, año en que aparece la red AlexNet y comienza a ganar popularidad el Deep Learning en el tratamiento de imágenes}{figure.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Evolución en la identificación forense de landmarks cefalométricos}{78}{subsection.7.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1.1}Two diﬀerent approaches to handle landmark location uncertainty in skull-face overlay: coevolution vs fuzzy landmarks}{78}{subsubsection.7.1.1.1}\protected@file@percent }
\@writefile{brf}{\backcite{ibanez2011two}{{78}{7.1.1.1}{subsubsection.7.1.1.1}}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Gráfica de publicaciones por año obtenida con la segunda \textit  {keyword} el $14$ de Julio de $2022$. La tendencia es a un artículo por año, aunque destaca el pico de tres artículos en 2015 y el de cuatro en 2019.}}{79}{figure.7.2}\protected@file@percent }
\newlabel{fig:SCOPUS2}{{7.2}{79}{Gráfica de publicaciones por año obtenida con la segunda \textit {keyword} el $14$ de Julio de $2022$. La tendencia es a un artículo por año, aunque destaca el pico de tres artículos en 2015 y el de cuatro en 2019}{figure.7.2}{}}
\citation{asi2014automatic}
\citation{asi2014automatic}
\citation{asi2014automatic}
\citation{galvanek2015automated}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1.2}Automatic craniofacial anthropometry landmarks detection and measurements for the orbital region}{80}{subsubsection.7.1.1.2}\protected@file@percent }
\@writefile{brf}{\backcite{asi2014automatic}{{80}{7.1.1.2}{subsubsection.7.1.1.2}}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1.3}Automated facial landmark detection, comparison and visualization}{80}{subsubsection.7.1.1.3}\protected@file@percent }
\@writefile{brf}{\backcite{galvanek2015automated}{{80}{7.1.1.3}{subsubsection.7.1.1.3}}}
\citation{galvanek2015automated}
\citation{galvanek2015automated}
\citation{porto2019automatic}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces Ejemplo de filtros empleados en el artículo \cite  {asi2014automatic} de donde procede esta imagen.}}{81}{figure.7.3}\protected@file@percent }
\@writefile{brf}{\backcite{asi2014automatic}{{81}{7.3}{figure.7.3}}}
\newlabel{fig:asi2014}{{7.3}{81}{Ejemplo de filtros empleados en el artículo \cite {asi2014automatic} de donde procede esta imagen}{figure.7.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1.4}Automatic cephalometric landmarks detection on frontal faces: An approach based on supervised learning techniques}{81}{subsubsection.7.1.1.4}\protected@file@percent }
\@writefile{brf}{\backcite{porto2019automatic}{{81}{7.1.1.4}{subsubsection.7.1.1.4}}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Cara alineada con el plano horizontal de Frankfort. Imagen extraida de \url  {https://www.slideshare.net/NiharikaSupriya/cephalometrics-landmarkslines-and-planes-93890774 }}}{82}{figure.7.4}\protected@file@percent }
\newlabel{fig:Frankfort}{{7.4}{82}{Cara alineada con el plano horizontal de Frankfort. Imagen extraida de \url {https://www.slideshare.net/NiharikaSupriya/cephalometrics-landmarkslines-and-planes-93890774 }}{figure.7.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces Comparativa entre los landmarks marcados por el algoritmo (izquierda) y los marcados por un experto (derecha). Imagen extraida de \cite  {galvanek2015automated}.}}{82}{figure.7.5}\protected@file@percent }
\@writefile{brf}{\backcite{galvanek2015automated}{{82}{7.5}{figure.7.5}}}
\newlabel{fig:landmarks_comparativa}{{7.5}{82}{Comparativa entre los landmarks marcados por el algoritmo (izquierda) y los marcados por un experto (derecha). Imagen extraida de \cite {galvanek2015automated}}{figure.7.5}{}}
\citation{ImprovedfasterRCNN}
\BKM@entry{id=62,dest={73756273656374696F6E2E372E312E32},srcline={195}}{4E7565737472612070726F707565737461}
\citation{300W}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1.5}The Improved Faster R-CNN for Detecting Small Facial Landmarks on Vietnamese Human Face Based on Clinical Diagnosis}{83}{subsubsection.7.1.1.5}\protected@file@percent }
\@writefile{brf}{\backcite{ImprovedfasterRCNN}{{83}{7.1.1.5}{subsubsection.7.1.1.5}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}Nuestra propuesta}{83}{subsection.7.1.2}\protected@file@percent }
\citation{300W}
\citation{300W}
\citation{AFLW}
\citation{AFLW}
\citation{AFLW}
\@writefile{brf}{\backcite{300W}{{84}{7.1.2}{subsection.7.1.2}}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces Conjunto de landmarks anotados en el dataset 300W, en la imagen \textit  {a} contando el contorno del rostro son un total de 68 landmarks, en la imagen \textit  {b} son 51 en total. Imagen extraida de \cite  {300W}.}}{84}{figure.7.6}\protected@file@percent }
\@writefile{brf}{\backcite{300W}{{84}{7.6}{figure.7.6}}}
\newlabel{fig:300W}{{7.6}{84}{Conjunto de landmarks anotados en el dataset 300W, en la imagen \textit {a} contando el contorno del rostro son un total de 68 landmarks, en la imagen \textit {b} son 51 en total. Imagen extraida de \cite {300W}}{figure.7.6}{}}
\@writefile{brf}{\backcite{AFLW}{{85}{7.1.2}{figure.7.6}}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces Conjunto de landmarks anotados sobre un modelo $3$D que emplea el dataset AFLW. Imagen extraida de \cite  {AFLW}.}}{85}{figure.7.7}\protected@file@percent }
\@writefile{brf}{\backcite{AFLW}{{85}{7.7}{figure.7.7}}}
\newlabel{fig:AFLW}{{7.7}{85}{Conjunto de landmarks anotados sobre un modelo $3$D que emplea el dataset AFLW. Imagen extraida de \cite {AFLW}}{figure.7.7}{}}
\BKM@entry{id=63,dest={636861707465722E38},srcline={1}}{4461746F732079204D5C333531747269636173}
\BKM@entry{id=64,dest={73656374696F6E2E382E31},srcline={5}}{4461746F732064656C2070726F626C656D612079206672616D65776F726B20656D706C6561646F}
\BKM@entry{id=65,dest={73756273656374696F6E2E382E312E31},srcline={6}}{42617365206465206461746F732070726F706F7263696F6E616461}
\BKM@entry{id=66,dest={73756273656374696F6E2E382E312E32},srcline={33}}{52656420656D706C656164613A2033466162526563}
\citation{browatzki20203fabrec}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Datos y Métricas}{87}{chapter.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Datos del problema y framework empleado}{87}{section.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.1}Base de datos proporcionada}{87}{subsection.8.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces Histograma con la aparición de cada tipo de landmark en las imágenes del dataset.}}{88}{figure.8.1}\protected@file@percent }
\newlabel{fig:Histograma}{{8.1}{88}{Histograma con la aparición de cada tipo de landmark en las imágenes del dataset}{figure.8.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.2}Red empleada: 3FabRec}{88}{subsection.8.1.2}\protected@file@percent }
\@writefile{brf}{\backcite{browatzki20203fabrec}{{88}{8.1.2}{subsection.8.1.2}}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces Imagen resumen del framework 3FabRec. En ella podemos ver la estructura del \textit  {Adversarial Autoencoder}, dividido en un Encoder (región bajo la \textit  {E}) y un Generator (región bajo la \textit  {G}) }}{89}{figure.8.2}\protected@file@percent }
\newlabel{fig:3FabRec Resumen}{{8.2}{89}{Imagen resumen del framework 3FabRec. En ella podemos ver la estructura del \textit {Adversarial Autoencoder}, dividido en un Encoder (región bajo la \textit {E}) y un Generator (región bajo la \textit {G})}{figure.8.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.2.1}Arquitectura Adversarial Autoencoder}{89}{subsubsection.8.1.2.1}\protected@file@percent }
\BKM@entry{id=67,dest={73756273656374696F6E2E382E312E33},srcline={112}}{46756E63695C3336336E20646520705C3335317264696461}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces Bloques básicos que utilza la red ResNet-$18$ en sus capas. Se trata de una sucesión clásica de Convolución 3x3 + Batch Normalization + ReLU que se repite dos veces. En el primer caso los filtros de convolución no reducen las dimensiones del tensor añadiendo un padding de 1. En el segundo caso se reduce la dimensión del tensor a la mitad tras la primera convolución y se manteiene la dimensionalidad en la segunda. En el primer caso, la suma residual puede realizarse con el tensor x sin problema, en el segundo caso el tensor debe reducirse para que casen las dimensiones.}}{90}{figure.8.3}\protected@file@percent }
\newlabel{fig:bloque_encoder}{{8.3}{90}{Bloques básicos que utilza la red ResNet-$18$ en sus capas. Se trata de una sucesión clásica de Convolución 3x3 + Batch Normalization + ReLU que se repite dos veces. En el primer caso los filtros de convolución no reducen las dimensiones del tensor añadiendo un padding de 1. En el segundo caso se reduce la dimensión del tensor a la mitad tras la primera convolución y se manteiene la dimensionalidad en la segunda. En el primer caso, la suma residual puede realizarse con el tensor x sin problema, en el segundo caso el tensor debe reducirse para que casen las dimensiones}{figure.8.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.2.2}Interleaved Transfer Layer (ITL)}{90}{subsubsection.8.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.3}Función de pérdida}{90}{subsection.8.1.3}\protected@file@percent }
\BKM@entry{id=68,dest={73756273656374696F6E2E382E312E34},srcline={153}}{50726F6365736F20646520656E7472656E616D69656E746F206465206C6120726564}
\newlabel{eq::L2}{{8.1}{91}{Función de pérdida}{equation.8.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.4}Proceso de entrenamiento de la red}{91}{subsection.8.1.4}\protected@file@percent }
\BKM@entry{id=69,dest={73756273656374696F6E2E382E312E35},srcline={177}}{4261736573206465206461746F732075736164617320706F7220656C206672616D65776F726B}
\BKM@entry{id=70,dest={73656374696F6E2E382E32},srcline={196}}{4D65747269636173}
\BKM@entry{id=71,dest={73756273656374696F6E2E382E322E31},srcline={200}}{5353494D}
\citation{wang2004image}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.5}Bases de datos usadas por el framework}{92}{subsection.8.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Metricas}{92}{section.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.1}SSIM}{92}{subsection.8.2.1}\protected@file@percent }
\@writefile{brf}{\backcite{wang2004image}{{92}{8.2.1}{subsection.8.2.1}}}
\BKM@entry{id=72,dest={73756273656374696F6E2E382E322E32},srcline={245}}{4176657261676520706978656C206572726F72}
\BKM@entry{id=73,dest={73756273656374696F6E2E382E322E33},srcline={248}}{4D5345}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.2}Average pixel error}{93}{subsection.8.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.3}MSE}{93}{subsection.8.2.3}\protected@file@percent }
\MT@newlabel{eq::L2}
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces Ejemplo de paso de una imagen a través del Encoder. Cabe destacar que a partir de la Layer 1, todos los bloques tienen downsample.}}{94}{figure.8.4}\protected@file@percent }
\newlabel{fig:Paso_encoder}{{8.4}{94}{Ejemplo de paso de una imagen a través del Encoder. Cabe destacar que a partir de la Layer 1, todos los bloques tienen downsample}{figure.8.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.5}{\ignorespaces En primer lugar se aplica una convolución transpuesta que duplica las dimensiones del tensor de entrada y tras esto se sigue la misma estructura que en el bloque básico de la ResNet-$50$, la segunda convolución $3\times 3$ mantiene las dimensiones. Como consecuencia, para sumar el tensor de entrada con la salida del bloque se aumentan las dimensiones de este.}}{95}{figure.8.5}\protected@file@percent }
\newlabel{fig:Bloque_Decoder}{{8.5}{95}{En primer lugar se aplica una convolución transpuesta que duplica las dimensiones del tensor de entrada y tras esto se sigue la misma estructura que en el bloque básico de la ResNet-$50$, la segunda convolución $3\times 3$ mantiene las dimensiones. Como consecuencia, para sumar el tensor de entrada con la salida del bloque se aumentan las dimensiones de este}{figure.8.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.6}{\ignorespaces Ejemplo del paso de un vector de $99$ dimensiones por el generador hasta reconstruirse la imagen de dimensiones $256 \times 256 \times 3$. La parte correspondiente al aprendizaje supervisado es la de los cuadrados azules, los cuadrados rojos corresponden a las \textit  {ITLS} de la parte supervisada que se intercalan entre cada dos capas y dan como resultado los mapas de calor de los landmarks predichos.}}{96}{figure.8.6}\protected@file@percent }
\newlabel{fig:Paso_Generator}{{8.6}{96}{Ejemplo del paso de un vector de $99$ dimensiones por el generador hasta reconstruirse la imagen de dimensiones $256 \times 256 \times 3$. La parte correspondiente al aprendizaje supervisado es la de los cuadrados azules, los cuadrados rojos corresponden a las \textit {ITLS} de la parte supervisada que se intercalan entre cada dos capas y dan como resultado los mapas de calor de los landmarks predichos}{figure.8.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.7}{\ignorespaces En la imagen superior vemos el discriminante que se emplea para los vectores producidos por el Encoder y en la imagen inferior vemos el discriminante que se emplea para las imágenes generadas por el Generador. En ambos casos se da como salida un valor entre $0$ y $1$ que hace referencia a la probabilidad de pertenecer a la distribución deseada en el primer caso o a seguir la distribución de los píxeles de las imágenes en el segundo caso.}}{97}{figure.8.7}\protected@file@percent }
\newlabel{fig:DGaussian}{{8.7}{97}{En la imagen superior vemos el discriminante que se emplea para los vectores producidos por el Encoder y en la imagen inferior vemos el discriminante que se emplea para las imágenes generadas por el Generador. En ambos casos se da como salida un valor entre $0$ y $1$ que hace referencia a la probabilidad de pertenecer a la distribución deseada en el primer caso o a seguir la distribución de los píxeles de las imágenes en el segundo caso}{figure.8.7}{}}
\BKM@entry{id=74,dest={636861707465722E39},srcline={1}}{506C616E696669636163695C3336336E206520696D706C656D656E746163695C3336336E}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Planificación e implementación}{99}{chapter.9}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\BKM@entry{id=75,dest={636861707465722E3130},srcline={1}}{4578706572696D656E746163695C3336336E}
\BKM@entry{id=76,dest={73656374696F6E2E31302E31},srcline={3}}{53657061726163695C3336336E20656E20636F6E6A756E746F7320646520656E7472656E616D69656E746F20792076616C69646163695C3336336E}
\BKM@entry{id=77,dest={73656374696F6E2E31302E32},srcline={20}}{4869705C333633746573697320696E696369616C657320792064657465726D696E6163695C3336336E2064652068697065727061725C3334316D6574726F732E}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}Experimentación}{101}{chapter.10}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {10.1}Separación en conjuntos de entrenamiento y validación}{101}{section.10.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10.2}Hipótesis iniciales y determinación de hiperparámetros.}{101}{section.10.2}\protected@file@percent }
\BKM@entry{id=78,dest={636861707465722E3131},srcline={1}}{436F6E636C7573696F6E657320792054726162616A6F732046757475726F73}
\@writefile{toc}{\contentsline {chapter}{\numberline {11}Conclusiones y Trabajos Futuros}{103}{chapter.11}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\BKM@entry{id=79,dest={636861707465722A2E3133},srcline={5}}{476C6F736172696F}
\@writefile{toc}{\contentsline {chapter}{Glosario}{105}{chapter*.13}\protected@file@percent }
\BKM@entry{id=80,dest={424D2D5265666572656E636961732E2D31},srcline={272}}{5265666572656E636961732065205C3331356E6469636573}
\bibstyle{alpha}
\bibdata{library.bib}
\BKM@entry{id=81,dest={636861707465722A2E3134},srcline={2}}{4269626C696F677261665C33353561}
\bibcite{asi2014automatic}{AIA{$^{+}$}14}
\bibcite{bruna2013invariant}{BM13}
\bibcite{bermejo2021automatic}{BTO{$^{+}$}21}
\bibcite{browatzki20203fabrec}{BW20}
\bibcite{damas2020handbook}{DCI20}
\bibcite{autoencoders2017}{Der17}
\bibcite{StanfordCourse}{{Fei}17}
\bibcite{Goodfellow-et-al-2016}{GBC16}
\bibcite{galvanek2015automated}{GFCS15}
\bibcite{DigitalImageProcessing}{Gon17}
\bibcite{EvolutionCNN}{Gup20}
\bibcite{Huete2015PastPA}{HIWK15}
\bibcite{ImprovedfasterRCNN}{HNATT22}
\bibcite{he2016deep}{HZRS16}
\bibcite{ibanez2011two}{ICD11}
\@writefile{toc}{\contentsline {chapter}{\nonumberline Bibliograf\'{\i }a}{107}{chapter*.14}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{JBrunaOperatorsCommutingDiff}{J.B12}
\bibcite{krizhevsky2012imagenet}{KSH12}
\bibcite{AFLW}{KWRB11}
\bibcite{lecun1998gradient}{LBBH98}
\bibcite{lecun2015deep}{LBH15}
\bibcite{DistinctiveImageFeatures}{Low04}
\bibcite{MallatWavelets}{Mal00}
\bibcite{GroupInvariantScattering}{Mal12}
\bibcite{HaarBasis}{PJDoM06}
\bibcite{porto2019automatic}{PLF{$^{+}$}19}
\bibcite{SchurLemma}{QV18}
\bibcite{AAE}{Ras20}
\bibcite{GAN}{Roc19a}
\bibcite{VAE}{Roc19b}
\bibcite{rosenfeld1988computer}{Ros88}
\bibcite{szegedy2015going}{SLJ{$^{+}$}15}
\bibcite{sharma2017activation}{SSA17}
\bibcite{300W}{STZP13}
\bibcite{simonyan2014very}{SZ14}
\bibcite{Daisy}{TLF10}
\bibcite{doi:10.1137/S0036141002404838}{TY05}
\bibcite{wang2004image}{WBSS04}
\bibcite{WAVELETS}{Wor}
\global\csname @altsecnumformattrue\endcsname
\global\@namedef{scr@dte@chapter@lastmaxnumwidth}{16.01686pt}
\global\@namedef{scr@dte@section@lastmaxnumwidth}{23.99994pt}
\global\@namedef{scr@dte@part@lastmaxnumwidth}{13.52026pt}
\global\@namedef{scr@dte@subsection@lastmaxnumwidth}{26.49994pt}
