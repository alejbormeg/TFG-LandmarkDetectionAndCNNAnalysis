\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\new@tpo@label[2]{}
\@nameuse{bbl@beforestart}
\catcode `"\active 
\catcode `<\active 
\catcode `>\active 
\@nameuse{es@quoting}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\BKM@entry[2]{}
\BKM@entry{id=1,dest={7469746C652E31},srcline={10}}{545C33353574756C6F}
\babel@aux{spanish}{}
\BKM@entry{id=2,dest={446564696361746F7269612E31},srcline={9}}{446564696361746F726961}
\BKM@entry{id=3,dest={746F632E30},srcline={8}}{5C3331356E646963652067656E6572616C}
\BKM@entry{id=4,dest={636861707465722A2E37},srcline={8}}{41677261646563696D69656E746F73}
\@writefile{toc}{\contentsline {chapter}{\nonumberline Agradecimientos}{\es@scroman  {xiii}}{chapter*.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\BKM@entry{id=5,dest={636861707465722A2E38},srcline={9}}{53756D6D617279}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{\nonumberline Summary}{\es@scroman  {xv}}{chapter*.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\babel@aux{spanish}{}
\BKM@entry{id=6,dest={636861707465722A2E39},srcline={11}}{496E74726F64756363695C3336336E}
\@writefile{toc}{\contentsline {chapter}{\nonumberline Introducción}{\es@scroman  {xvii}}{chapter*.9}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\BKM@entry{id=7,dest={706172742E31},srcline={235}}{416E5C3334316C6973697320646520526564657320436F6E766F6C7563696F6E616C6573}
\@writefile{toc}{\contentsline {part}{\numberline {I}Análisis de Redes Convolucionales}{1}{part.1}\protected@file@percent }
\BKM@entry{id=8,dest={636861707465722E31},srcline={5}}{496E74726F64756363695C3336336E}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introducción}{3}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Las tres estatuas deben identificarse como iguales, aunque se ecuentren desplazadas.}}{3}{figure.1.1}\protected@file@percent }
\newlabel{fig:invarianza_traslaciones}{{1.1}{3}{Las tres estatuas deben identificarse como iguales, aunque se ecuentren desplazadas}{figure.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Acción de un difeomorfismo en una rejilla.}}{4}{figure.1.2}\protected@file@percent }
\newlabel{fig:difeomorfismo}{{1.2}{4}{Acción de un difeomorfismo en una rejilla}{figure.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Todas las imágenes deberían clasificarse como 5, pese a las deformaciones.}}{4}{figure.1.3}\protected@file@percent }
\newlabel{fig:deformaciones_5}{{1.3}{4}{Todas las imágenes deberían clasificarse como 5, pese a las deformaciones}{figure.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Deformación excesiva que permite confundir el 1 con el 2 cuando se le aplica el difeomorfismo. Por eso nos centramos en \emph  {``pequeñas''} deformaciones, para no alterar la identidad del objeto en la imagen.}}{4}{figure.1.4}\protected@file@percent }
\newlabel{fig:deformaciones_1}{{1.4}{4}{Deformación excesiva que permite confundir el 1 con el 2 cuando se le aplica el difeomorfismo. Por eso nos centramos en \entrecomillado {pequeñas} deformaciones, para no alterar la identidad del objeto en la imagen}{figure.1.4}{}}
\newlabel{eq::distancia}{{1.1}{5}{}{equation.1.0.1}{}}
\citation{doi:10.1137/S0036141002404838}
\BKM@entry{id=9,dest={73756273656374696F6E2E312E302E31},srcline={154}}{4E6F746163695C3336336E}
\newlabel{def::Lipschitz_cont}{{1.7}{6}{}{definicion.1.7}{}}
\newlabel{eq::Lipschitz_condition}{{1.2}{6}{}{equation.1.0.2}{}}
\@writefile{brf}{\backcite{doi:10.1137/S0036141002404838}{{6}{1.7}{equation.1.0.2}}}
\MT@newlabel{eq::distancia}
\MT@newlabel{eq::Lipschitz_condition}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.0.1}Notación}{6}{subsection.1.0.1}\protected@file@percent }
\BKM@entry{id=10,dest={636861707465722E32},srcline={4}}{4D6F64656C697A6163695C3336336E204D6174656D5C3334317469636120646520756E6120526564204E6575726F6E616C20436F6E766F6C7563696F6E616C}
\BKM@entry{id=11,dest={73656374696F6E2E322E31},srcline={21}}{446520466F75726965722061206C6173206F6E64656C65746173206465204C6974746C65776F6F642D50616C6579}
\BKM@entry{id=12,dest={73756273656374696F6E2E322E312E31},srcline={23}}{456C206D5C33363364756C6F206465206C61205472616E73666F726D61646120646520466F7572696572}
\citation{DigitalImageProcessing}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Modelización Matemática de una Red Neuronal Convolucional}{7}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:seccion12}{{2}{7}{Modelización Matemática de una Red Neuronal Convolucional}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}De Fourier a las ondeletas de Littlewood-Paley}{7}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}El módulo de la Transformada de Fourier}{7}{subsection.2.1.1}\protected@file@percent }
\@writefile{brf}{\backcite{DigitalImageProcessing}{{7}{2.1.1}{subsection.2.1.1}}}
\newlabel{lema::invarianza_traslaciones}{{2.1}{8}{}{lema.2.1}{}}
\newlabel{lemma:TF_inestable_difeomorfismos}{{2.1}{8}{}{observacion.2.1}{}}
\newlabel{eq:res_auxiliar_1}{{2.1}{9}{}{equation.2.1.1}{}}
\newlabel{eq:res_auxiliar_2}{{2.2}{9}{}{equation.2.1.2}{}}
\MT@newlabel{eq:res_auxiliar_1}
\MT@newlabel{eq:res_auxiliar_2}
\newlabel{eq::1.1}{{2.1}{11}{}{equation.2.1.2}{}}
\newlabel{eq::Plancharel}{{2.3}{11}{}{equation.2.1.3}{}}
\BKM@entry{id=13,dest={73756273656374696F6E2E322E312E32},srcline={211}}{416C7465726E61746976613A204C6173206F6E64656C65746173}
\citation{MallatWavelets}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Como podemos ver en la imagen, para los valores $\epsilon =0.1$, $\xi =100$ y $M=5$ ambas funciones tienen soporte casi disjunto de manera que la diferencia entre ellas en el intervalo $[-5,5]$ coincide prácticamente con la integral de $g_1(t)$.}}{12}{figure.2.1}\protected@file@percent }
\newlabel{fig:Grafica_funciones}{{2.1}{12}{Como podemos ver en la imagen, para los valores $\epsilon =0.1$, $\xi =100$ y $M=5$ ambas funciones tienen soporte casi disjunto de manera que la diferencia entre ellas en el intervalo $[-5,5]$ coincide prácticamente con la integral de $g_1(t)$}{figure.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Alternativa: Las ondeletas}{12}{subsection.2.1.2}\protected@file@percent }
\@writefile{brf}{\backcite{MallatWavelets}{{12}{2.1.2}{subsection.2.1.2}}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Representación gráfica de la ondeleta de Haar.}}{13}{figure.2.2}\protected@file@percent }
\newlabel{fig:Ondeleta_de_Haar}{{2.2}{13}{Representación gráfica de la ondeleta de Haar}{figure.2.2}{}}
\citation{MallatWavelets}
\citation{HaarBasis}
\citation{HaarBasis}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Ejemplos de aplicar la base de Haar a dos imágenes \cite  {HaarBasis}.}}{14}{figure.2.3}\protected@file@percent }
\@writefile{brf}{\backcite{HaarBasis}{{14}{2.3}{figure.2.3}}}
\newlabel{fig:ejemplo_haar}{{2.3}{14}{Ejemplos de aplicar la base de Haar a dos imágenes \cite {HaarBasis}}{figure.2.3}{}}
\@writefile{brf}{\backcite{MallatWavelets}{{14}{2}{Hfootnote.3}}}
\BKM@entry{id=14,dest={73756273656374696F6E2E322E312E33},srcline={321}}{4C61205472616E73666F726D616461206465204C6974746C65776F6F642D50616C6579}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}La Transformada de Littlewood-Paley}{15}{subsection.2.1.3}\protected@file@percent }
\newlabel{Teorema::Convolucion}{{2.1}{15}{}{teorema.2.1}{}}
\newlabel{unitario}{{2.1}{17}{}{proposicion.2.1}{}}
\newlabel{eq::1.2}{{2.4}{17}{}{equation.2.1.4}{}}
\MT@newlabel{eq::1.2}
\newlabel{eq::1.3}{{2.5}{17}{La Transformada de Littlewood-Paley}{equation.2.1.5}{}}
\MT@newlabel{eq::1.2}
\MT@newlabel{eq::1.2}
\MT@newlabel{eq::1.3}
\MT@newlabel{eq::1.2}
\MT@newlabel{eq::1.3}
\MT@newlabel{eq::1.3}
\BKM@entry{id=15,dest={73756273656374696F6E2E322E312E34},srcline={480}}{436F6E76656E696F73207061726120667574757261732073656363696F6E6573}
\MT@newlabel{eq::Plancharel}
\MT@newlabel{eq::1.3}
\MT@newlabel{eq::1.3}
\MT@newlabel{eq::1.3}
\MT@newlabel{eq::1.2}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Convenios para futuras secciones}{18}{subsection.2.1.4}\protected@file@percent }
\MT@newlabel{eq::1.2}
\BKM@entry{id=16,dest={73656374696F6E2E322E32},srcline={502}}{456C206F70657261646F722064652064697370657273695C3336336E20736F62726520756E2063616D696E6F206F7264656E61646F}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}El operador de dispersión sobre un camino ordenado}{19}{section.2.2}\protected@file@percent }
\newlabel{lema:Invarianza_traslaciones_integral}{{2.2}{19}{}{lema.2.2}{}}
\BKM@entry{id=17,dest={73756273656374696F6E2E322E322E31},srcline={555}}{456A656D706C6F2070617261206F6274656E657220636F6566696369656E74657320696E76617269616E74657320706F7220747261736C6163696F6E6573}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Ejemplo para obtener coeficientes invariantes por traslaciones}{20}{subsection.2.2.1}\protected@file@percent }
\newlabel{eq::1.4}{{2.6}{20}{Ejemplo para obtener coeficientes invariantes por traslaciones}{equation.2.2.6}{}}
\BKM@entry{id=18,dest={73756273656374696F6E2E322E322E32},srcline={592}}{456C206F70657261646F72206D5C33363364756C6F2E}
\citation{JBrunaOperatorsCommutingDiff}
\citation{bruna2013invariant}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}El operador módulo.}{21}{subsection.2.2.2}\protected@file@percent }
\@writefile{brf}{\backcite{JBrunaOperatorsCommutingDiff}{{21}{2.2.2}{subsection.2.2.2}}}
\@writefile{brf}{\backcite{bruna2013invariant}{{21}{2.2.2}{subsection.2.2.2}}}
\MT@newlabel{eq::1.4}
\BKM@entry{id=19,dest={73756273656374696F6E2E322E322E33},srcline={669}}{50726F706965646164657320646520756E2063616D696E6F206465206672656375656E636961732E}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Propiedades de un camino de frecuencias.}{22}{subsection.2.2.3}\protected@file@percent }
\newlabel{proposicionSumaCaminos}{{2.2}{22}{}{proposicion.2.2}{}}
\BKM@entry{id=20,dest={73756273656374696F6E2E322E322E34},srcline={703}}{436F6E737472756363695C3336336E2064656C206F70657261646F722064652064697370657273695C3336336E2E}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Construcción del operador de dispersión.}{23}{subsection.2.2.4}\protected@file@percent }
\newlabel{def:S_barra}{{2.7}{23}{}{definicion.2.7}{}}
\BKM@entry{id=21,dest={73656374696F6E2E322E33},srcline={776}}{50726F70616761646F722064652064697370657273695C3336336E207920636F6E736572766163695C3336336E206465206C61204E6F726D61}
\BKM@entry{id=22,dest={73756273656374696F6E2E322E332E31},srcline={779}}{50726F6365736F2064652064697370657273695C3336336E2064656C2070726F70616761646F722E}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Propagador de dispersión y conservación de la Norma}{24}{section.2.3}\protected@file@percent }
\newlabel{ch:seccion13}{{2.3}{24}{Propagador de dispersión y conservación de la Norma}{section.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Proceso de dispersión del propagador.}{24}{subsection.2.3.1}\protected@file@percent }
\BKM@entry{id=23,dest={73756273656374696F6E2E322E332E32},srcline={824}}{4469666572656E6369617320792073696D696C69747564657320636F6E20756E6120434E4E}
\citation{lecun2015deep}
\newlabel{eq::1.5}{{2.7}{25}{Proceso de dispersión del propagador}{equation.2.3.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Un PD $U_J$ aplicado a un punto de una señal $f(x)$ calcula $U[\lambda _1]f(x)=|f(x)\ast \psi _{\lambda _1}|$ y como salida a la capa $m=0$ se promedian los coeficientes que han dado $0$ (por tener $2^j<2^-j$) obteniendo como salida $S_J[\emptyset ]f(x)=f(x)\ast \phi _{2^J}$ (como se puede ver en la flecha negra). Después se aplica de nuevo $U_J$ a cada coeficiente $U[\lambda _1]f(x)$ del paso anterior ($m=1$) $U[\lambda _1,\lambda 2]f(x)$ obteniendo como salida $S_J[\lambda _1]f(x)=U[\lambda _1]f(x) \ast \phi _{2^J}$. Se repite este proceso de manera recursiva para cada coeficiente $U[p]f(x)$ y obteniendo como resultado $S_J[p]f(x)=U[p]f(x) \ast \phi _{2^J}$. }}{25}{figure.2.4}\protected@file@percent }
\newlabel{fig:scattering_propagator}{{2.4}{25}{Un PD $U_J$ aplicado a un punto de una señal $f(x)$ calcula $U[\lambda _1]f(x)=|f(x)\ast \psi _{\lambda _1}|$ y como salida a la capa $m=0$ se promedian los coeficientes que han dado $0$ (por tener $2^j<2^-j$) obteniendo como salida $S_J[\emptyset ]f(x)=f(x)\ast \phi _{2^J}$ (como se puede ver en la flecha negra). Después se aplica de nuevo $U_J$ a cada coeficiente $U[\lambda _1]f(x)$ del paso anterior ($m=1$) $U[\lambda _1,\lambda 2]f(x)$ obteniendo como salida $S_J[\lambda _1]f(x)=U[\lambda _1]f(x) \ast \phi _{2^J}$. Se repite este proceso de manera recursiva para cada coeficiente $U[p]f(x)$ y obteniendo como resultado $S_J[p]f(x)=U[p]f(x) \ast \phi _{2^J}$}{figure.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Diferencias y similitudes con una CNN}{25}{subsection.2.3.2}\protected@file@percent }
\@writefile{brf}{\backcite{lecun2015deep}{{25}{2.3.2}{subsection.2.3.2}}}
\BKM@entry{id=24,dest={73756273656374696F6E2E322E332E33},srcline={840}}{52656C6163695C3336336E20636F6E2068657272616D69656E74617320636C5C333431736963617320646520766973695C3336336E20706F7220636F6D70757461646F72}
\citation{DistinctiveImageFeatures}
\citation{Daisy}
\BKM@entry{id=25,dest={73756273656374696F6E2E322E332E34},srcline={844}}{4F70657261646F72206E6F20657870616E7369766F2E}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Relación con herramientas clásicas de visión por computador}{26}{subsection.2.3.3}\protected@file@percent }
\@writefile{brf}{\backcite{DistinctiveImageFeatures}{{26}{2.3.3}{subsection.2.3.3}}}
\@writefile{brf}{\backcite{Daisy}{{26}{2.3.3}{subsection.2.3.3}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Operador no expansivo.}{26}{subsection.2.3.4}\protected@file@percent }
\newlabel{proposicion::NoExpansiva}{{2.3}{26}{}{proposicion.2.3}{}}
\BKM@entry{id=26,dest={73756273656374696F6E2E322E332E35},srcline={915}}{436F6E736572766163695C3336336E206465206C61206E6F726D612E}
\MT@newlabel{eq::1.5}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Conservación de la norma.}{28}{subsection.2.3.5}\protected@file@percent }
\newlabel{lema::Cota_inferior}{{2.3}{28}{}{lema.2.3}{}}
\newlabel{eq::1.6}{{2.8}{28}{}{equation.2.3.8}{}}
\newlabel{eq::1.7}{{2.9}{28}{}{equation.2.3.9}{}}
\newlabel{lema::Admisibilidad}{{2.4}{28}{}{lema.2.4}{}}
\MT@newlabel{eq::1.7}
\citation{GroupInvariantScattering}
\newlabel{eq::1.9}{{2.10}{29}{}{equation.2.3.10}{}}
\@writefile{brf}{\backcite{GroupInvariantScattering}{{29}{2.3.5}{equation.2.3.10}}}
\newlabel{teoremaOndeletasAdmisibles}{{2.2}{29}{}{teorema.2.2}{}}
\MT@newlabel{eq::1.6}
\MT@newlabel{eq::1.9}
\newlabel{eq::1.8}{{2.11}{30}{Conservación de la norma}{equation.2.3.11}{}}
\MT@newlabel{eq::1.8}
\BKM@entry{id=27,dest={73756273656374696F6E2E322E332E36},srcline={1102}}{436F6E636C7573696F6E6573206578747261696461732064656C2074656F72656D61}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.6}Conclusiones extraidas del teorema}{31}{subsection.2.3.6}\protected@file@percent }
\BKM@entry{id=28,dest={636861707465722E33},srcline={3}}{496E76617269616E7A6120706F7220547261736C6163696F6E6573}
\BKM@entry{id=29,dest={73656374696F6E2E332E31},srcline={7}}{4E6F20657870616E736976696461642064656C206F70657261646F722064652076656E74616E6120656E20636F6E6A756E746F732064652063616D696E6F73}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Invarianza por Traslaciones}{33}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:seccion14}{{3}{33}{Invarianza por Traslaciones}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}No expansividad del operador de ventana en conjuntos de caminos}{33}{section.3.1}\protected@file@percent }
\newlabel{eq::1.10}{{3.1}{33}{}{equation.3.1.1}{}}
\MT@newlabel{eq::1.10}
\newlabel{eq::1.11}{{3.2}{33}{No expansividad del operador de ventana en conjuntos de caminos}{equation.3.1.2}{}}
\MT@newlabel{eq::1.11}
\MT@newlabel{eq::1.10}
\MT@newlabel{eq::1.11}
\BKM@entry{id=30,dest={73656374696F6E2E332E32},srcline={141}}{496E76617269616E7A6120706F7220747261736C6163696F6E6573}
\MT@newlabel{eq::1.11}
\citation{SchurLemma}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Invarianza por traslaciones}{36}{section.3.2}\protected@file@percent }
\MT@newlabel{eq::1.7}
\newlabel{lema::constante}{{3.2}{36}{}{lema.3.2}{}}
\@writefile{brf}{\backcite{SchurLemma}{{36}{3.2}{lema.3.2}}}
\newlabel{invarianzaTraslaciones}{{3.1}{38}{}{teorema.3.1}{}}
\MT@newlabel{eq::1.7}
\MT@newlabel{eq::1.9}
\BKM@entry{id=31,dest={636861707465722E34},srcline={3}}{436F6E636C7573696F6E6573}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Conclusiones}{41}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\BKM@entry{id=32,dest={73656374696F6E2E342E31},srcline={52}}{456C656D656E746F732064656C20746578746F}
\BKM@entry{id=33,dest={73756273656374696F6E2E342E312E31},srcline={56}}{4C6973746173}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Elementos del texto}{42}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Listas}{42}{subsection.4.1.1}\protected@file@percent }
\BKM@entry{id=34,dest={73756273656374696F6E2E342E312E32},srcline={79}}{5461626C617320792066696775726173}
\BKM@entry{id=35,dest={73656374696F6E2E342E32},srcline={103}}{456E746F726E6F73206D6174656D5C3334317469636F73}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Tablas y figuras}{43}{subsection.4.1.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Ejemplo de tabla}}{43}{table.4.1}\protected@file@percent }
\newlabel{tb:ejemplo-tabla}{{4.1}{43}{Ejemplo de tabla}{table.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Logotipo de la Universidad de Granada}}{43}{figure.4.1}\protected@file@percent }
\newlabel{fig:logo-ugr}{{4.1}{43}{Logotipo de la Universidad de Granada}{figure.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Entornos matemáticos}{43}{section.4.2}\protected@file@percent }
\newlabel{thm:teorema}{{4.1}{43}{}{teorema.4.1}{}}
\BKM@entry{id=36,dest={73656374696F6E2E342E33},srcline={141}}{4269626C696F677261665C333535612065205C3335356E64696365}
\MT@newlabel{eq:identidad-pitagorica}
\newlabel{eq:identidad-pitagorica}{{4.1}{44}{Entornos matemáticos}{equation.4.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Bibliografía e índice}{44}{section.4.3}\protected@file@percent }
\BKM@entry{id=37,dest={706172742E32},srcline={242}}{4C6F63616C697A6163695C3336336E206465206C616E646D61726B7320636566616C6F6D5C333531747269636F7320706F72206D6564696F20646520745C333531636E69636173206465206665772D73686F74206C6561726E696E67}
\@writefile{toc}{\contentsline {part}{\numberline {II}Localización de landmarks cefalométricos por medio de técnicas de few-shot learning}{45}{part.2}\protected@file@percent }
\BKM@entry{id=38,dest={636861707465722E35},srcline={4}}{496E74726F64756363695C3336336E}
\BKM@entry{id=39,dest={73756273656374696F6E2E352E302E31},srcline={15}}{4465736372697063695C3336336E2064656C2070726F626C656D61}
\citation{Huete2015PastPA}
\BKM@entry{id=40,dest={73756273656374696F6E2E352E302E32},srcline={28}}{4D6F7469766163695C3336336E}
\BKM@entry{id=41,dest={73756273656374696F6E2E352E302E33},srcline={29}}{4F626A657469766F73}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Introducción}{47}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:Introduccion_informatica}{{5}{47}{Introducción}{chapter.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.1}Descripción del problema}{47}{subsection.5.0.1}\protected@file@percent }
\@writefile{brf}{\backcite{Huete2015PastPA}{{47}{5.0.1}{subsection.5.0.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.2}Motivación}{47}{subsection.5.0.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.3}Objetivos}{47}{subsection.5.0.3}\protected@file@percent }
\BKM@entry{id=42,dest={636861707465722E36},srcline={2}}{46756E64616D656E746F732054655C3336337269636F732079204D5C333531746F646F73}
\BKM@entry{id=43,dest={73656374696F6E2E362E31},srcline={6}}{417072656E64697A616A65204175746F6D5C3334317469636F}
\BKM@entry{id=44,dest={73756273656374696F6E2E362E312E31},srcline={34}}{417072656E64697A616A6520537570657276697361646F}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Fundamentos Teóricos y Métodos}{49}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Aprendizaje Automático}{49}{section.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Aprendizaje Supervisado}{50}{subsection.6.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1.1}Regresión}{50}{subsubsection.6.1.1.1}\protected@file@percent }
\newlabel{section::Regresion}{{6.1.1.1}{50}{Regresión}{subsubsection.6.1.1.1}{}}
\BKM@entry{id=45,dest={73756273656374696F6E2E362E312E32},srcline={90}}{417072656E64697A616A65206E6F20537570657276697361646F}
\BKM@entry{id=46,dest={73756273656374696F6E2E362E312E33},srcline={95}}{4E75657374726F2050726F626C656D61}
\BKM@entry{id=47,dest={73756273656374696F6E2E362E312E34},srcline={106}}{4772616469656E74652044657363656E64656E7465}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1.2}clasificación}{51}{subsubsection.6.1.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}Aprendizaje no Supervisado}{51}{subsection.6.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.3}Nuestro Problema}{51}{subsection.6.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.4}Gradiente Descendente}{51}{subsection.6.1.4}\protected@file@percent }
\BKM@entry{id=48,dest={73656374696F6E2E362E32},srcline={162}}{566973695C3336336E20706F7220436F6D70757461646F72}
\citation{rosenfeld1988computer}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.4.1}Gradiente Descendente Estocástico}{52}{subsubsection.6.1.4.1}\protected@file@percent }
\BKM@entry{id=49,dest={73656374696F6E2E362E33},srcline={173}}{44656570204C6561726E696E67}
\BKM@entry{id=50,dest={73756273656374696F6E2E362E332E31},srcline={178}}{5265646573204E6575726F6E616C6573}
\citation{Goodfellow-et-al-2016}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Visión por Computador}{53}{section.6.2}\protected@file@percent }
\@writefile{brf}{\backcite{rosenfeld1988computer}{{53}{6.2}{section.6.2}}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Deep Learning}{53}{section.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Redes Neuronales}{53}{subsection.6.3.1}\protected@file@percent }
\newlabel{sub:redes_neuronales}{{6.3.1}{53}{Redes Neuronales}{subsection.6.3.1}{}}
\@writefile{brf}{\backcite{Goodfellow-et-al-2016}{{53}{6.3.1}{subsection.6.3.1}}}
\citation{sharma2017activation}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Red neuronal con una capa oculta. Formalmente podría describirse como $f'(x)=(f_3(f_2(f_1(x))))$, dónde $f_1$ hace referencia a la capa de entrada a la red, $f_2$ a la capa oculta y $f_3$ a la capa de salida.}}{54}{figure.6.1}\protected@file@percent }
\newlabel{fig:red_neuronal_capa_oculta}{{6.1}{54}{Red neuronal con una capa oculta. Formalmente podría describirse como $f'(x)=(f_3(f_2(f_1(x))))$, dónde $f_1$ hace referencia a la capa de entrada a la red, $f_2$ a la capa oculta y $f_3$ a la capa de salida}{figure.6.1}{}}
\@writefile{brf}{\backcite{sharma2017activation}{{54}{6.3.1}{figure.6.1}}}
\BKM@entry{id=51,dest={73756273656374696F6E2E362E332E32},srcline={251}}{4261636B2050726F7061676174696F6E}
\citation{StanfordCourse}
\citation{StanfordCourse}
\citation{StanfordCourse}
\citation{StanfordCourse}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Back Propagation}{55}{subsection.6.3.2}\protected@file@percent }
\BKM@entry{id=52,dest={73656374696F6E2E362E34},srcline={285}}{5265646573204E6575726F6E616C657320436F6E766F6C7563696F6E616C6573}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Ejemplo de Grafo computacional junto con la salida para una entrada concreta $x=-2$,$y=5$, $z=-4$. La imagen ha sido extraida del curso \cite  {StanfordCourse}.}}{56}{figure.6.2}\protected@file@percent }
\@writefile{brf}{\backcite{StanfordCourse}{{56}{6.2}{figure.6.2}}}
\newlabel{fig:GrafoComputacional}{{6.2}{56}{Ejemplo de Grafo computacional junto con la salida para una entrada concreta $x=-2$,$y=5$, $z=-4$. La imagen ha sido extraida del curso \cite {StanfordCourse}}{figure.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Como podemos ver en la imagen superior, en primer lugar se renombra la salida de la operación $x+y$ por $q$, de manera que $f=qz$. Tras esto se empiezan a calcular las derivadas parciales correspondientes desde el final hacia la entrada, aplicando cuando sea necesario la regla de la cadena hasta obtener la derivada de cada nodo en la imagen de abajo. Las imágenes han sido extraidas de \cite  {StanfordCourse}}}{56}{figure.6.3}\protected@file@percent }
\@writefile{brf}{\backcite{StanfordCourse}{{56}{6.3}{figure.6.3}}}
\newlabel{fig:Ejemplo BP}{{6.3}{56}{Como podemos ver en la imagen superior, en primer lugar se renombra la salida de la operación $x+y$ por $q$, de manera que $f=qz$. Tras esto se empiezan a calcular las derivadas parciales correspondientes desde el final hacia la entrada, aplicando cuando sea necesario la regla de la cadena hasta obtener la derivada de cada nodo en la imagen de abajo. Las imágenes han sido extraidas de \cite {StanfordCourse}}{figure.6.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Redes Neuronales Convolucionales}{57}{section.6.4}\protected@file@percent }
\BKM@entry{id=53,dest={73756273656374696F6E2E362E342E31},srcline={337}}{4361706120436F6E766F6C7563696F6E616C}
\citation{StanfordCourse}
\citation{StanfordCourse}
\citation{StanfordCourse}
\citation{StanfordCourse}
\BKM@entry{id=54,dest={73756273656374696F6E2E362E342E32},srcline={370}}{4361706120646520506F6F6C696E67}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Capa Convolucional}{58}{subsection.6.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}Capa de Pooling}{58}{subsection.6.4.2}\protected@file@percent }
\citation{StanfordCourse}
\citation{StanfordCourse}
\BKM@entry{id=55,dest={73756273656374696F6E2E362E342E33},srcline={391}}{4361706120546F74616C6D656E746520436F6E656374616461205C2846756C6C7920436F6E65637465645C29}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Ejemplo de cálculo de mapa de activación en una capa convolucional para un determinado volumen de entrada. El parámetro depth nos dice la cantidad de mapas de activación generamos para el volumen de entrada, o dicho de otro modo, el número de filtros que aplicamos. La imagen ha sido extraida de \cite  {StanfordCourse}}}{59}{figure.6.4}\protected@file@percent }
\@writefile{brf}{\backcite{StanfordCourse}{{59}{6.4}{figure.6.4}}}
\newlabel{fig:mapa_activacion}{{6.4}{59}{Ejemplo de cálculo de mapa de activación en una capa convolucional para un determinado volumen de entrada. El parámetro depth nos dice la cantidad de mapas de activación generamos para el volumen de entrada, o dicho de otro modo, el número de filtros que aplicamos. La imagen ha sido extraida de \cite {StanfordCourse}}{figure.6.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Sucesión de varias capas convolucionales + ReLU que describen la estructura básica de una CNN. Cabe destacar como la produndidad de los filtros es siemore la misma que la del volumen de entrada, de acuerdo a lo que hemos dicho anteriormente. La imagen han sido extraida de \cite  {StanfordCourse}}}{59}{figure.6.5}\protected@file@percent }
\@writefile{brf}{\backcite{StanfordCourse}{{59}{6.5}{figure.6.5}}}
\newlabel{fig:estructura_convnet}{{6.5}{59}{Sucesión de varias capas convolucionales + ReLU que describen la estructura básica de una CNN. Cabe destacar como la produndidad de los filtros es siemore la misma que la del volumen de entrada, de acuerdo a lo que hemos dicho anteriormente. La imagen han sido extraida de \cite {StanfordCourse}}{figure.6.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.3}Capa Totalmente Conectada (Fully Conected)}{59}{subsection.6.4.3}\protected@file@percent }
\BKM@entry{id=56,dest={73756273656374696F6E2E362E342E34},srcline={398}}{4261746368204E6F726D616C697A6174696F6E}
\BKM@entry{id=57,dest={73756273656374696F6E2E362E342E35},srcline={401}}{4F7074696D697A61646F72204164616D}
\BKM@entry{id=58,dest={73756273656374696F6E2E362E342E36},srcline={404}}{50726F6365736F20646520656E7472656E616D69656E746F20646520756E6120434E4E}
\BKM@entry{id=59,dest={73756273656374696F6E2E362E342E37},srcline={418}}{45766F6C7563695C3336336E206465206C617320434E4E}
\citation{EvolutionCNN}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Ejemplo de capa de pooling usando la operación del máximo. La imagen han sido extraida de \cite  {StanfordCourse}}}{60}{figure.6.6}\protected@file@percent }
\@writefile{brf}{\backcite{StanfordCourse}{{60}{6.6}{figure.6.6}}}
\newlabel{fig:pooling}{{6.6}{60}{Ejemplo de capa de pooling usando la operación del máximo. La imagen han sido extraida de \cite {StanfordCourse}}{figure.6.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.4}Batch Normalization}{60}{subsection.6.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.5}Optimizador Adam}{60}{subsection.6.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.6}Proceso de entrenamiento de una CNN}{60}{subsection.6.4.6}\protected@file@percent }
\citation{lecun1998gradient}
\citation{lecun1998gradient}
\citation{lecun1998gradient}
\citation{krizhevsky2012imagenet}
\citation{krizhevsky2012imagenet}
\citation{krizhevsky2012imagenet}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.7}Evolución de las CNN}{61}{subsection.6.4.7}\protected@file@percent }
\@writefile{brf}{\backcite{EvolutionCNN}{{61}{6.4.7}{subsection.6.4.7}}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.7.1}LeNet-5}{61}{subsubsection.6.4.7.1}\protected@file@percent }
\@writefile{brf}{\backcite{lecun1998gradient}{{61}{6.4.7.1}{subsubsection.6.4.7.1}}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Arquitectura de la red LeNet. Como podemos observar tiene capas convolucionales, capas de average pooling y capas totalmente conectadas al final. Imagen extraida de \cite  {lecun1998gradient}.}}{61}{figure.6.7}\protected@file@percent }
\@writefile{brf}{\backcite{lecun1998gradient}{{61}{6.7}{figure.6.7}}}
\newlabel{fig:LeNet}{{6.7}{61}{Arquitectura de la red LeNet. Como podemos observar tiene capas convolucionales, capas de average pooling y capas totalmente conectadas al final. Imagen extraida de \cite {lecun1998gradient}}{figure.6.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.7.2}AlexNet}{61}{subsubsection.6.4.7.2}\protected@file@percent }
\@writefile{brf}{\backcite{krizhevsky2012imagenet}{{61}{6.4.7.2}{subsubsection.6.4.7.2}}}
\citation{szegedy2015going}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces Arquitectura de la red AlexNet. Destaca que parece estar \emph  {``partida''} en dos mitades, esto es porque por primera vez se usaron las GPUs para entrenar la red de manera que una GPU realizaba la parte superior de la arquitectura y otra la parte inferior. Imagen extraida de \cite  {krizhevsky2012imagenet}.}}{62}{figure.6.8}\protected@file@percent }
\@writefile{brf}{\backcite{krizhevsky2012imagenet}{{62}{6.8}{figure.6.8}}}
\newlabel{fig:AlexNet}{{6.8}{62}{Arquitectura de la red AlexNet. Destaca que parece estar \entrecomillado {partida} en dos mitades, esto es porque por primera vez se usaron las GPUs para entrenar la red de manera que una GPU realizaba la parte superior de la arquitectura y otra la parte inferior. Imagen extraida de \cite {krizhevsky2012imagenet}}{figure.6.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.7.3}GoogLeNet}{62}{subsubsection.6.4.7.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces Ejemplos de módulos Inception.}}{62}{figure.6.9}\protected@file@percent }
\newlabel{fig:inception}{{6.9}{62}{Ejemplos de módulos Inception}{figure.6.9}{}}
\citation{simonyan2014very}
\citation{he2016deep}
\citation{he2016deep}
\citation{he2016deep}
\citation{StanfordCourse}
\citation{StanfordCourse}
\BKM@entry{id=60,dest={73656374696F6E2E362E35},srcline={532}}{4175746F656E636F64657273}
\@writefile{brf}{\backcite{szegedy2015going}{{63}{6.4.7.3}{figure.6.9}}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.10}{\ignorespaces Arquitectura de GoogLeNet usando módulos Inception.}}{63}{figure.6.10}\protected@file@percent }
\newlabel{fig:GoogLeNet}{{6.10}{63}{Arquitectura de GoogLeNet usando módulos Inception}{figure.6.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.7.4}VGG-16}{63}{subsubsection.6.4.7.4}\protected@file@percent }
\@writefile{brf}{\backcite{simonyan2014very}{{63}{6.4.7.4}{subsubsection.6.4.7.4}}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.7.5}ResNet}{63}{subsubsection.6.4.7.5}\protected@file@percent }
\@writefile{brf}{\backcite{he2016deep}{{63}{6.4.7.5}{subsubsection.6.4.7.5}}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.11}{\ignorespaces Arquitectura de VGG-16.}}{64}{figure.6.11}\protected@file@percent }
\newlabel{fig:VGG}{{6.11}{64}{Arquitectura de VGG-16}{figure.6.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.12}{\ignorespaces Bloque residual de una ResNet. Como vemos, se suma el tensor $x$ con el tensor $\mathcal  {F}(x)$ que surge unas etapas después. Imagen extraida de \cite  {he2016deep}.}}{64}{figure.6.12}\protected@file@percent }
\@writefile{brf}{\backcite{he2016deep}{{64}{6.12}{figure.6.12}}}
\newlabel{fig:Resnet}{{6.12}{64}{Bloque residual de una ResNet. Como vemos, se suma el tensor $x$ con el tensor $\mathcal {F}(x)$ que surge unas etapas después. Imagen extraida de \cite {he2016deep}}{figure.6.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.13}{\ignorespaces Resumen de los ganadores del concurso ILSVRC hasta 2015 con la aparición de Resnet. Imagen extraida de \cite  {StanfordCourse}.}}{64}{figure.6.13}\protected@file@percent }
\@writefile{brf}{\backcite{StanfordCourse}{{64}{6.13}{figure.6.13}}}
\newlabel{fig:ImageNet}{{6.13}{64}{Resumen de los ganadores del concurso ILSVRC hasta 2015 con la aparición de Resnet. Imagen extraida de \cite {StanfordCourse}}{figure.6.13}{}}
\BKM@entry{id=61,dest={73756273656374696F6E2E362E352E31},srcline={535}}{496E74726F64756363695C3336336E}
\citation{autoencoders2017}
\citation{autoencoders2017}
\citation{autoencoders2017}
\BKM@entry{id=62,dest={73756273656374696F6E2E362E352E32},srcline={561}}{45766F6C7563695C3336336E206465206C6F73204175746F656E636F64657273}
\citation{GAN}
\citation{autoencoders2017}
\citation{autoencoders2017}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Autoencoders}{65}{section.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}Introducción}{65}{subsection.6.5.1}\protected@file@percent }
\@writefile{brf}{\backcite{autoencoders2017}{{65}{6.5.1}{subsection.6.5.1}}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.14}{\ignorespaces Esquema de un Autoencoder básico. Imagen extraida de \cite  {autoencoders2017}.}}{65}{figure.6.14}\protected@file@percent }
\@writefile{brf}{\backcite{autoencoders2017}{{65}{6.14}{figure.6.14}}}
\newlabel{fig:Autoencoder}{{6.14}{65}{Esquema de un Autoencoder básico. Imagen extraida de \cite {autoencoders2017}}{figure.6.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.2}Evolución de los Autoencoders}{65}{subsection.6.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.2.1}Generative Adversarial Networks (GANs)}{65}{subsubsection.6.5.2.1}\protected@file@percent }
\@writefile{brf}{\backcite{GAN}{{65}{6.5.2.1}{subsubsection.6.5.2.1}}}
\citation{autoencoders2017}
\citation{autoencoders2017}
\citation{AAE}
\citation{AAE}
\@writefile{lof}{\contentsline {figure}{\numberline {6.15}{\ignorespaces Ejemplo de una Generative Network que pretende aprender la distribución de probabilidad de un conjunto de imágenes de perros. Imagen extraida de \cite  {autoencoders2017}.}}{66}{figure.6.15}\protected@file@percent }
\@writefile{brf}{\backcite{autoencoders2017}{{66}{6.15}{figure.6.15}}}
\newlabel{fig:Generative Network}{{6.15}{66}{Ejemplo de una Generative Network que pretende aprender la distribución de probabilidad de un conjunto de imágenes de perros. Imagen extraida de \cite {autoencoders2017}}{figure.6.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.16}{\ignorespaces Resumen del proceso de entrenamiento de una GAN. Imagen extraida de \cite  {autoencoders2017}.}}{66}{figure.6.16}\protected@file@percent }
\@writefile{brf}{\backcite{autoencoders2017}{{66}{6.16}{figure.6.16}}}
\newlabel{fig:Generative Network}{{6.16}{66}{Resumen del proceso de entrenamiento de una GAN. Imagen extraida de \cite {autoencoders2017}}{figure.6.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.17}{\ignorespaces Ejemplo de la arquitectura de una GAN para generar imágenes de dígitos manuscritos. Imagen extraida de \cite  {AAE}}}{66}{figure.6.17}\protected@file@percent }
\@writefile{brf}{\backcite{AAE}{{66}{6.17}{figure.6.17}}}
\newlabel{fig:GAN}{{6.17}{66}{Ejemplo de la arquitectura de una GAN para generar imágenes de dígitos manuscritos. Imagen extraida de \cite {AAE}}{figure.6.17}{}}
\citation{VAE}
\citation{VAE}
\citation{VAE}
\citation{VAE}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.2.2}Variational Autoencoder (VAE)}{67}{subsubsection.6.5.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.18}{\ignorespaces Diferencia en el tratamiento de los datos por parte del encoder en una VAE y un Autoencoder clásico. Imagen extraida de \cite  {VAE}.}}{67}{figure.6.18}\protected@file@percent }
\@writefile{brf}{\backcite{VAE}{{67}{6.18}{figure.6.18}}}
\newlabel{fig:VAE_1}{{6.18}{67}{Diferencia en el tratamiento de los datos por parte del encoder en una VAE y un Autoencoder clásico. Imagen extraida de \cite {VAE}}{figure.6.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.2.3}Adversarial Autoencoder(AAE)}{67}{subsubsection.6.5.2.3}\protected@file@percent }
\citation{AAE}
\citation{AAE}
\BKM@entry{id=63,dest={73656374696F6E2E362E36},srcline={650}}{545C333531636E6963617320656D706C6561646173}
\BKM@entry{id=64,dest={73756273656374696F6E2E362E362E31},srcline={653}}{4665772D73686F74204C6561726E696E6720792044617461204175676D656E746174696F6E}
\@writefile{lof}{\contentsline {figure}{\numberline {6.19}{\ignorespaces Diferencia en la función de pérdida de un Autoencoder clásico (imagen superior) y una VAE (imagen inferior). Destacamos el término KL regularizante que pretende que las distribuciones de los datos sigan una normal estándar. Imagen extraida de \cite  {VAE}.}}{68}{figure.6.19}\protected@file@percent }
\@writefile{brf}{\backcite{VAE}{{68}{6.19}{figure.6.19}}}
\newlabel{fig:VAE_2}{{6.19}{68}{Diferencia en la función de pérdida de un Autoencoder clásico (imagen superior) y una VAE (imagen inferior). Destacamos el término KL regularizante que pretende que las distribuciones de los datos sigan una normal estándar. Imagen extraida de \cite {VAE}}{figure.6.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Técnicas empleadas}{68}{section.6.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.20}{\ignorespaces Esquema de la arquitectura de un AAE. El Encoder sería la red a la que entra la imagen $x$, el vector $z$ sería el vector latente, que sirve de entrada al Discriminador $D_{gauss}$ y finalmente el vector $z$ es la entrada del Decoder que reconstruye la imagen. Imagen extraida de \cite  {AAE}.}}{69}{figure.6.20}\protected@file@percent }
\@writefile{brf}{\backcite{AAE}{{69}{6.20}{figure.6.20}}}
\newlabel{fig:AAE}{{6.20}{69}{Esquema de la arquitectura de un AAE. El Encoder sería la red a la que entra la imagen $x$, el vector $z$ sería el vector latente, que sirve de entrada al Discriminador $D_{gauss}$ y finalmente el vector $z$ es la entrada del Decoder que reconstruye la imagen. Imagen extraida de \cite {AAE}}{figure.6.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.1}Few-shot Learning y Data Augmentation}{69}{subsection.6.6.1}\protected@file@percent }
\newlabel{sub:data_augmentation}{{6.6.1}{69}{Few-shot Learning y Data Augmentation}{subsection.6.6.1}{}}
\BKM@entry{id=65,dest={636861707465722E37},srcline={2}}{45737461646F2064656C2041727465}
\BKM@entry{id=66,dest={73656374696F6E2E372E31},srcline={6}}{4C6F63616C697A6163695C3336336E206465206C616E646D61726B7320636566616C6F6D5C333531747269636F7320656E20696D5C33343167656E6573}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Estado del Arte}{71}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Localización de landmarks cefalométricos en imágenes}{71}{section.7.1}\protected@file@percent }
\BKM@entry{id=67,dest={73756273656374696F6E2E372E312E31},srcline={68}}{45766F6C7563695C3336336E20656E206C61206964656E74696669636163695C3336336E20666F72656E7365206465206C616E646D61726B7320636566616C6F6D5C333531747269636F73}
\citation{ibanez2011two}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Gráfica de publicaciones por año obtenida con la primera \textit  {keyword} el $14$ de Julio de $2022$. Destaca el notable incremento de papers a partir de 2012, año en que aparece la red AlexNet y comienza a ganar popularidad el Deep Learning en el tratamiento de imágenes.}}{72}{figure.7.1}\protected@file@percent }
\newlabel{fig:SCOPUS1}{{7.1}{72}{Gráfica de publicaciones por año obtenida con la primera \textit {keyword} el $14$ de Julio de $2022$. Destaca el notable incremento de papers a partir de 2012, año en que aparece la red AlexNet y comienza a ganar popularidad el Deep Learning en el tratamiento de imágenes}{figure.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Evolución en la identificación forense de landmarks cefalométricos}{72}{subsection.7.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1.1}Two diﬀerent approaches to handle landmark location uncertainty in skull-face overlay: coevolution vs fuzzy landmarks}{72}{subsubsection.7.1.1.1}\protected@file@percent }
\@writefile{brf}{\backcite{ibanez2011two}{{72}{7.1.1.1}{subsubsection.7.1.1.1}}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Gráfica de publicaciones por año obtenida con la segunda \textit  {keyword} el $14$ de Julio de $2022$. La tendencia es a un artículo por año, aunque destaca el pico de tres artículos en 2015 y el de cuatro en 2019.}}{73}{figure.7.2}\protected@file@percent }
\newlabel{fig:SCOPUS2}{{7.2}{73}{Gráfica de publicaciones por año obtenida con la segunda \textit {keyword} el $14$ de Julio de $2022$. La tendencia es a un artículo por año, aunque destaca el pico de tres artículos en 2015 y el de cuatro en 2019}{figure.7.2}{}}
\citation{asi2014automatic}
\citation{asi2014automatic}
\citation{asi2014automatic}
\citation{galvanek2015automated}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1.2}Automatic craniofacial anthropometry landmarks detection and measurements for the orbital region}{74}{subsubsection.7.1.1.2}\protected@file@percent }
\@writefile{brf}{\backcite{asi2014automatic}{{74}{7.1.1.2}{subsubsection.7.1.1.2}}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1.3}Automated facial landmark detection, comparison and visualization}{74}{subsubsection.7.1.1.3}\protected@file@percent }
\@writefile{brf}{\backcite{galvanek2015automated}{{74}{7.1.1.3}{subsubsection.7.1.1.3}}}
\citation{galvanek2015automated}
\citation{galvanek2015automated}
\citation{porto2019automatic}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces Ejemplo de filtros empleados en el artículo \cite  {asi2014automatic} de donde procede esta imagen.}}{75}{figure.7.3}\protected@file@percent }
\@writefile{brf}{\backcite{asi2014automatic}{{75}{7.3}{figure.7.3}}}
\newlabel{fig:asi2014}{{7.3}{75}{Ejemplo de filtros empleados en el artículo \cite {asi2014automatic} de donde procede esta imagen}{figure.7.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1.4}Automatic cephalometric landmarks detection on frontal faces: An approach based on supervised learning techniques}{75}{subsubsection.7.1.1.4}\protected@file@percent }
\@writefile{brf}{\backcite{porto2019automatic}{{75}{7.1.1.4}{subsubsection.7.1.1.4}}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Cara alineada con el plano horizontal de Frankfort. Imagen extraida de \url  {https://www.slideshare.net/NiharikaSupriya/cephalometrics-landmarkslines-and-planes-93890774 }}}{76}{figure.7.4}\protected@file@percent }
\newlabel{fig:Frankfort}{{7.4}{76}{Cara alineada con el plano horizontal de Frankfort. Imagen extraida de \url {https://www.slideshare.net/NiharikaSupriya/cephalometrics-landmarkslines-and-planes-93890774 }}{figure.7.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces Comparativa entre los landmarks marcados por el algoritmo (izquierda) y los marcados por un experto (derecha). Imagen extraida de \cite  {galvanek2015automated}.}}{76}{figure.7.5}\protected@file@percent }
\@writefile{brf}{\backcite{galvanek2015automated}{{76}{7.5}{figure.7.5}}}
\newlabel{fig:landmarks_comparativa}{{7.5}{76}{Comparativa entre los landmarks marcados por el algoritmo (izquierda) y los marcados por un experto (derecha). Imagen extraida de \cite {galvanek2015automated}}{figure.7.5}{}}
\citation{ImprovedfasterRCNN}
\BKM@entry{id=68,dest={73756273656374696F6E2E372E312E32},srcline={195}}{4E7565737472612070726F707565737461}
\citation{300W}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1.5}The Improved Faster R-CNN for Detecting Small Facial Landmarks on Vietnamese Human Face Based on Clinical Diagnosis}{77}{subsubsection.7.1.1.5}\protected@file@percent }
\@writefile{brf}{\backcite{ImprovedfasterRCNN}{{77}{7.1.1.5}{subsubsection.7.1.1.5}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}Nuestra propuesta}{77}{subsection.7.1.2}\protected@file@percent }
\citation{300W}
\citation{300W}
\citation{AFLW}
\citation{AFLW}
\citation{AFLW}
\@writefile{brf}{\backcite{300W}{{78}{7.1.2}{subsection.7.1.2}}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces Conjunto de landmarks anotados en el dataset 300W, en la imagen \textit  {a} contando el contorno del rostro son un total de 68 landmarks, en la imagen \textit  {b} son 51 en total. Imagen extraida de \cite  {300W}.}}{78}{figure.7.6}\protected@file@percent }
\@writefile{brf}{\backcite{300W}{{78}{7.6}{figure.7.6}}}
\newlabel{fig:300W}{{7.6}{78}{Conjunto de landmarks anotados en el dataset 300W, en la imagen \textit {a} contando el contorno del rostro son un total de 68 landmarks, en la imagen \textit {b} son 51 en total. Imagen extraida de \cite {300W}}{figure.7.6}{}}
\@writefile{brf}{\backcite{AFLW}{{79}{7.1.2}{figure.7.6}}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces Conjunto de landmarks anotados sobre un modelo $3$D que emplea el dataset AFLW. Imagen extraida de \cite  {AFLW}.}}{79}{figure.7.7}\protected@file@percent }
\@writefile{brf}{\backcite{AFLW}{{79}{7.7}{figure.7.7}}}
\newlabel{fig:AFLW}{{7.7}{79}{Conjunto de landmarks anotados sobre un modelo $3$D que emplea el dataset AFLW. Imagen extraida de \cite {AFLW}}{figure.7.7}{}}
\BKM@entry{id=69,dest={636861707465722E38},srcline={1}}{4461746F732079204D5C333531747269636173}
\BKM@entry{id=70,dest={73656374696F6E2E382E31},srcline={5}}{4461746F732064656C2070726F626C656D612079206672616D65776F726B20656D706C6561646F}
\BKM@entry{id=71,dest={73756273656374696F6E2E382E312E31},srcline={6}}{42617365206465206461746F732070726F706F7263696F6E616461}
\BKM@entry{id=72,dest={73756273656374696F6E2E382E312E32},srcline={33}}{52656420656D706C656164613A2033466162526563}
\citation{browatzki20203fabrec}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Datos y Métricas}{81}{chapter.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Datos del problema y framework empleado}{81}{section.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.1}Base de datos proporcionada}{81}{subsection.8.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces Histograma con la aparición de cada tipo de landmark en las imágenes del dataset.}}{82}{figure.8.1}\protected@file@percent }
\newlabel{fig:Histograma}{{8.1}{82}{Histograma con la aparición de cada tipo de landmark en las imágenes del dataset}{figure.8.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.2}Red empleada: 3FabRec}{82}{subsection.8.1.2}\protected@file@percent }
\@writefile{brf}{\backcite{browatzki20203fabrec}{{82}{8.1.2}{subsection.8.1.2}}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces Imagen resumen del framework 3FabRec. En ella podemos ver la estructura del \textit  {Adversarial Autoencoder}, dividido en un Encoder (región bajo la \textit  {E}) y un Generator (región bajo la \textit  {G}) }}{83}{figure.8.2}\protected@file@percent }
\newlabel{fig:3FabRec Resumen}{{8.2}{83}{Imagen resumen del framework 3FabRec. En ella podemos ver la estructura del \textit {Adversarial Autoencoder}, dividido en un Encoder (región bajo la \textit {E}) y un Generator (región bajo la \textit {G})}{figure.8.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.2.1}Arquitectura Adversarial Autoencoder}{83}{subsubsection.8.1.2.1}\protected@file@percent }
\BKM@entry{id=73,dest={73756273656374696F6E2E382E312E33},srcline={112}}{46756E63695C3336336E20646520705C3335317264696461}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces Bloques básicos que utilza la red ResNet-$18$ en sus capas. Se trata de una sucesión clásica de Convolución 3x3 + Batch Normalization + ReLU que se repite dos veces. En el primer caso los filtros de convolución no reducen las dimensiones del tensor añadiendo un padding de 1. En el segundo caso se reduce la dimensión del tensor a la mitad tras la primera convolución y se manteiene la dimensionalidad en la segunda. En el primer caso, la suma residual puede realizarse con el tensor x sin problema, en el segundo caso el tensor debe reducirse para que casen las dimensiones.}}{84}{figure.8.3}\protected@file@percent }
\newlabel{fig:bloque_encoder}{{8.3}{84}{Bloques básicos que utilza la red ResNet-$18$ en sus capas. Se trata de una sucesión clásica de Convolución 3x3 + Batch Normalization + ReLU que se repite dos veces. En el primer caso los filtros de convolución no reducen las dimensiones del tensor añadiendo un padding de 1. En el segundo caso se reduce la dimensión del tensor a la mitad tras la primera convolución y se manteiene la dimensionalidad en la segunda. En el primer caso, la suma residual puede realizarse con el tensor x sin problema, en el segundo caso el tensor debe reducirse para que casen las dimensiones}{figure.8.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.2.2}Interleaved Transfer Layer (ITL)}{84}{subsubsection.8.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.3}Función de pérdida}{84}{subsection.8.1.3}\protected@file@percent }
\BKM@entry{id=74,dest={73756273656374696F6E2E382E312E34},srcline={153}}{50726F6365736F20646520656E7472656E616D69656E746F206465206C6120726564}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.4}Proceso de entrenamiento de la red}{85}{subsection.8.1.4}\protected@file@percent }
\BKM@entry{id=75,dest={73756273656374696F6E2E382E312E35},srcline={177}}{4261736573206465206461746F732075736164617320706F7220656C206672616D65776F726B}
\BKM@entry{id=76,dest={73656374696F6E2E382E32},srcline={194}}{4D65747269636173}
\BKM@entry{id=77,dest={73756273656374696F6E2E382E322E31},srcline={198}}{5353494D}
\citation{wang2004image}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.5}Bases de datos usadas por el framework}{86}{subsection.8.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Metricas}{86}{section.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.1}SSIM}{86}{subsection.8.2.1}\protected@file@percent }
\@writefile{brf}{\backcite{wang2004image}{{86}{8.2.1}{subsection.8.2.1}}}
\BKM@entry{id=78,dest={73756273656374696F6E2E382E322E32},srcline={243}}{4176657261676520706978656C206572726F72}
\BKM@entry{id=79,dest={73756273656374696F6E2E382E322E33},srcline={246}}{4D5345}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.2}Average pixel error}{87}{subsection.8.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.3}MSE}{87}{subsection.8.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces Ejemplo de paso de una imagen a través del Encoder. Cabe destacar que a partir de la Layer 1, todos los bloques tienen downsample.}}{88}{figure.8.4}\protected@file@percent }
\newlabel{fig:Paso_encoder}{{8.4}{88}{Ejemplo de paso de una imagen a través del Encoder. Cabe destacar que a partir de la Layer 1, todos los bloques tienen downsample}{figure.8.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.5}{\ignorespaces En primer lugar se aplica una convolución transpuesta que duplica las dimensiones del tensor de entrada y tras esto se sigue la misma estructura que en el bloque básico de la ResNet-$50$, la segunda convolución $3\times 3$ mantiene las dimensiones. Como consecuencia, para sumar el tensor de entrada con la salida del bloque se aumentan las dimensiones de este.}}{89}{figure.8.5}\protected@file@percent }
\newlabel{fig:Bloque_Decoder}{{8.5}{89}{En primer lugar se aplica una convolución transpuesta que duplica las dimensiones del tensor de entrada y tras esto se sigue la misma estructura que en el bloque básico de la ResNet-$50$, la segunda convolución $3\times 3$ mantiene las dimensiones. Como consecuencia, para sumar el tensor de entrada con la salida del bloque se aumentan las dimensiones de este}{figure.8.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.6}{\ignorespaces Ejemplo del paso de un vector de $99$ dimensiones por el generador hasta reconstruirse la imagen de dimensiones $256 \times 256 \times 3$. La parte correspondiente al aprendizaje supervisado es la de los cuadrados azules, los cuadrados rojos corresponden a las \textit  {ITLS} de la parte supervisada que se intercalan entre cada dos capas y dan como resultado los mapas de calor de los landmarks predichos.}}{90}{figure.8.6}\protected@file@percent }
\newlabel{fig:Paso_Generator}{{8.6}{90}{Ejemplo del paso de un vector de $99$ dimensiones por el generador hasta reconstruirse la imagen de dimensiones $256 \times 256 \times 3$. La parte correspondiente al aprendizaje supervisado es la de los cuadrados azules, los cuadrados rojos corresponden a las \textit {ITLS} de la parte supervisada que se intercalan entre cada dos capas y dan como resultado los mapas de calor de los landmarks predichos}{figure.8.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.7}{\ignorespaces En la imagen superior vemos el discriminante que se emplea para los vectores producidos por el Encoder y en la imagen inferior vemos el discriminante que se emplea para las imágenes generadas por el Generador. En ambos casos se da como salida un valor entre $0$ y $1$ que hace referencia a la probabilidad de pertenecer a la distribución deseada en el primer caso o a seguir la distribución de los píxeles de las imágenes en el segundo caso.}}{91}{figure.8.7}\protected@file@percent }
\newlabel{fig:DGaussian}{{8.7}{91}{En la imagen superior vemos el discriminante que se emplea para los vectores producidos por el Encoder y en la imagen inferior vemos el discriminante que se emplea para las imágenes generadas por el Generador. En ambos casos se da como salida un valor entre $0$ y $1$ que hace referencia a la probabilidad de pertenecer a la distribución deseada en el primer caso o a seguir la distribución de los píxeles de las imágenes en el segundo caso}{figure.8.7}{}}
\BKM@entry{id=80,dest={636861707465722E39},srcline={1}}{506C616E696669636163695C3336336E206520696D706C656D656E746163695C3336336E}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Planificación e implementación}{93}{chapter.9}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\BKM@entry{id=81,dest={636861707465722E3130},srcline={1}}{4578706572696D656E746163695C3336336E}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}Experimentación}{95}{chapter.10}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\BKM@entry{id=82,dest={636861707465722E3131},srcline={1}}{436F6E636C7573696F6E657320792054726162616A6F732046757475726F73}
\BKM@entry{id=83,dest={617070656E6469782E2D31},srcline={256}}{41705C3335316E6469636573}
\@writefile{toc}{\contentsline {chapter}{\numberline {11}Conclusiones y Trabajos Futuros}{97}{chapter.11}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\BKM@entry{id=84,dest={617070656E6469782E416C706831},srcline={4}}{5072696D65722061705C3335316E64696365}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Primer apéndice}{99}{appendix.Alph1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ap:apendice1}{{A}{99}{Primer apéndice}{appendix.Alph1}{}}
\BKM@entry{id=85,dest={636861707465722A2E3132},srcline={5}}{476C6F736172696F}
\@writefile{toc}{\contentsline {chapter}{Glosario}{101}{chapter*.12}\protected@file@percent }
\BKM@entry{id=86,dest={424D2D5265666572656E636961732E2D31},srcline={272}}{5265666572656E636961732065205C3331356E6469636573}
\bibstyle{alpha}
\bibdata{library.bib}
\BKM@entry{id=87,dest={636861707465722A2E3133},srcline={2}}{4269626C696F677261665C33353561}
\bibcite{asi2014automatic}{AIA{$^{+}$}14}
\bibcite{bruna2013invariant}{BM13}
\bibcite{browatzki20203fabrec}{BW20}
\bibcite{autoencoders2017}{Der17}
\bibcite{StanfordCourse}{{Fei}17}
\bibcite{Goodfellow-et-al-2016}{GBC16}
\bibcite{galvanek2015automated}{GFCS15}
\bibcite{DigitalImageProcessing}{Gon17}
\bibcite{EvolutionCNN}{Gup20}
\bibcite{Huete2015PastPA}{HIWK15}
\bibcite{ImprovedfasterRCNN}{HNATT22}
\bibcite{he2016deep}{HZRS16}
\bibcite{ibanez2011two}{ICD11}
\bibcite{JBrunaOperatorsCommutingDiff}{J.B12}
\bibcite{krizhevsky2012imagenet}{KSH12}
\@writefile{toc}{\contentsline {chapter}{\nonumberline Bibliograf\'{\i }a}{103}{chapter*.13}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{AFLW}{KWRB11}
\bibcite{lecun1998gradient}{LBBH98}
\bibcite{lecun2015deep}{LBH15}
\bibcite{DistinctiveImageFeatures}{Low04}
\bibcite{MallatWavelets}{Mal00}
\bibcite{GroupInvariantScattering}{Mal12}
\bibcite{HaarBasis}{PJDoM06}
\bibcite{porto2019automatic}{PLF{$^{+}$}19}
\bibcite{SchurLemma}{QV18}
\bibcite{AAE}{Ras20}
\bibcite{GAN}{Roc19a}
\bibcite{VAE}{Roc19b}
\bibcite{rosenfeld1988computer}{Ros88}
\bibcite{szegedy2015going}{SLJ{$^{+}$}15}
\bibcite{sharma2017activation}{SSA17}
\bibcite{300W}{STZP13}
\bibcite{simonyan2014very}{SZ14}
\bibcite{Daisy}{TLF10}
\bibcite{doi:10.1137/S0036141002404838}{TY05}
\bibcite{wang2004image}{WBSS04}
\global\csname @altsecnumformattrue\endcsname
\global\@namedef{scr@dte@chapter@lastmaxnumwidth}{16.01686pt}
\global\@namedef{scr@dte@section@lastmaxnumwidth}{18.99994pt}
\global\@namedef{scr@dte@part@lastmaxnumwidth}{13.52026pt}
\global\@namedef{scr@dte@subsection@lastmaxnumwidth}{26.49994pt}
