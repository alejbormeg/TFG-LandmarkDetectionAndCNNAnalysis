\chapter{Experimentación}

\section{Separación en conjuntos de entrenamiento y validación}
    \noindent En primer lugar separaremos el conjunto de datos que se proporciona para la realización de los experimentos en conjuntos de entrenamiento, validación y test. Debido a que trabajaremos con un conjunto de datos reducido será necesario emplear \textbf{validación cruzada} para la elección entre los modelos que se prueben. Así pues, de las \textbf{164 imágenes} que usaremos en total, establecemos una semilla para que los procesos aleatorios sean replicables en cualquier máquina y con ayuda de la función \textit{random-shuffle} de python separamos los datos en: 
    \begin{itemize}
        \item \textbf{Conjunto de test}: 33 imágenes ($20\%$ del total de imágenes).
        \item \textbf{Conjunto de entrenamiento:} 131 imágenes ($80\%$ del total de imágenes).
    \end{itemize}

    \noindent Por otro lado, el conjunto de entrenamiento se subdivide a su vez en cinco subconjuntos, con la idea de realizar \textbf{5-fold cross-validation}. Cada subconjunto estará compuesto por \textbf{26 imágenes} exceptuando el último que tendrá \textbf{27}.
    
    \medskip

    \noindent Para esta técnica se realizan cinco ejecuciones independientes en la cual se toman cuatro de los cinco subconjuntos de entrenamiento para entrenar el modelo y se deja el último subconjunto para validar el modelo. Con esta técnica, en cada iteración se cambia el conjunto que se empleará para validación, con la finalidad de mejorar la calidad de las decisiones que se tomen sobre el rendimiento, pues la decisión estará mejor informada. 

    \medskip

    \noindent De esta manera, con cada propuesta de modelo que realicemos, se medirá su rendimiento usando \textbf{5-fold cross-validation}, obteniendo 5 salidas de validación para cada modelo. La elección del mejor será mediante el cómputo de la mediana de estos para cada mdelo.

\section{Preprocesamiento}

    \noindent Antes de poder entrenar el modelo, es necesario adecuar el dataset para que pueda servir de entrenamiento. El framework está pensado para recibir como entrada una imagen a la cual se le hace un recorte a la región que ocupa el rostro con ayuda de un bounding box predefinido o bien siguiendo los landmarks descritos por el fichero auxiliar asociado a cada imagen. En nuestro caso, debido a que la mayoría de landmarks son internos y no se suelen situar en los bordes de la cara, hemos pensado que la mejor manera de entrenar la red es que serealice el recorte de la imagen de entrada de acuerdo a un \textit{bounding box}. Para ello fue necesario emplear una red especializada en la detección de caras en imágenes y que nos proporcionaba como salida una lista con los \textit{bounding boxes} de todas las caras detectadas, la red se denomina \textbf{Facenet} y que se puede importar de la librería \textit{facenet pytorch} como \textit{mtcnn}.


    \medskip

    \noindent Así pues, se realizó un primer estudio para determinar los mejores parámetros para \textit{facenet}. Existen muchos parámetros editables, pero la mayoría se dejaron con sus valores por defecto, únicamente se consideró modificar: 

    \begin{enumerate}
        \item \textbf{image size}: Determina el tamaño de la imagen de salida.
        \item \textbf{select largest}: Si su valor es \textit{true}, se devuelve la cara más grande detectada, y si su valor es \textit{false}, se devuelve la de mayor confianza detectada.
    \end{enumerate}


    \noindent En nuestro caso, primero se probó con un \textit{image size} de $256 \times 256$, pero esto tenía problemas pues en ocasiones no podían construirse bounding boxes de este tamaño y producían error, por lo que se redujo a $128 \times 128$ generando buenos resultados, por esto finalmente se dejó este valor. Por otro lado, observando las imágenes del dataset de entrenamiento, nos dimos cuenta que en algunas imágenes aparecían varios sujetos, y en ocasiones la cara más \entrecomillado{grande} mo pertenecía al sujeto principal. Es por ello que se decidió establecer el parámetro \textit{select largest} a \textit{False}, y así obtener como salida una lista de \textit{bounding boxes} ordenados de mayor a menor según el nivel de confianza.

    \medskip

    \noindent Una vez establecidos los parámetros, se detectaron problemas en la identificación de caras en algunas imágenes. Debido a que todas las imágenes erróneas estaban en escalas de grises nos dimos cuenta de que la red sólo acepta como entrada imágenes en tres canales, por lo que cada imagen en escala de grises de un solo canal tuvimos que replicar tres veces su canal. 
    
    \medskip
    
    \noindent Tras esto, volvimos a ver que en algunas imágenes no se detectaban \textit{bounding boxes}. Esto nos llevó a realizar un breve estudio sobre si era buena idea continuar usando esta red o buscar otra. En este estudio clasificamos las imágenes en tres grupos y aplicamos la red a cada uno de ellos por separado: 

    \begin{enumerate}
        \item Imágenes de sujetos en posición \textbf{ frontal}: en total hay $87$ imágenes frontales de las cuales se extraen correctamente el \textbf{$100\%$} de los \textit{bounding boxes}.
        \item Imágenes de sujetos en posición de \textbf{$3/4$}: En este caso hay $57$ imágenes de las cuales el \textbf{$100\%$} de los \textit{bounding boxes} extraidos por la red son correctos.
        \item Imágenes de sujetos en posición de \textbf{perfil}: En este caso hay un total de $23$ imágenes y se clasifican correctamente $20$, lo que supone una precisión del \textbf{$87\%$}. Además, las imágenes que fallaban estaban en escala de grises y con malas condiciones de calidad e iluminación.
    \end{enumerate}

    \noindent Por lo tanto, debido al bajo número de ejemplos en los que la red falla, se decidió mantener la red \textbf{Facenet} para determinar los \textit{bounding boxes} y además se excluyeron los tres ejemplos dónde fallaba del dataset, lo que hizo que en total se usaran $164$ de las $167$ imágenes originales.

    \medskip 

    \noindent Además, durante el estudio anterior se observaron otros hechos dignos de mención. En primer lugar se identificaron dos imágenes repetidas, pero se decidió mantenerlas pues los landmarks presentes en una y otra eran diferentes, lo que podía ayudar al entrenamiento. Por otro lado, durante el entrenamiento se vió cómo había una imagen de un determinado sujeto en la que aprecían simultáneamente dos fotos, una  de frente y otra de perfil, sin embargo, al estar los landmarks anotados únicamente en la imagen de frente se tomó el bounding box de la imagen frontal desechando el otro para el perfil. En otras dos imágenes se pueden ver un conjunto de varias personas, y la red mostraba erróneamente los \textit{bounding boxes} de personas que no eran el sujeto de estudio, por lo que estudiar cuál de todos los \textit{bounding boxes} devueltos pertenecía al sujeto correcto.
    
    Además las imgágenes en blanco y nego dan problemas porque no tienen 3 canales.
    \noindent La manera en que procesamos las imágenes ... 
        - En primer lugar, las imágenes en Blanco y negro las pasamos a 3 canales triplicando la matriz 2d original.
        
    - Ante esto procedemos a realizar un estudio de lo bien que detecta caras en todas las imágenes clasificando por caras de perfil, 3/4 o frontales.
    - Identificamos una imagen repetida (21.jpg y 163.jpg) pero decidimos mantener ambas porque tienen distintos landmarks marcados. 
    - Las imagenes que usemos van a utilizar un id asociado a la posición que ocupan dentro del Dataset (los ids van de 0 a 163).
    - La imagen con id 122 tiene los landmarks puestos únicamente en la imagen frontal, por lo que la contamos como frontal y tomamos el bb de esta que nos da facenet.
    - La imagen con id 55 identifica una cara errónea, por lo que debemos seleccionar el segundo BB, que es el que se corresponde con la cara del sujeto.
    - La imagen con id 57 identifica cara errónea, por lo que seleccionamos tercer BB que es el que se corresponde con la cara del sujeto.
    - El porcentaje de acierto en las imágenes frontales es del 100% al igual que las de 3/4. La precisión baja al 87% en las de perfil, y tenemos que descartar las tres imágenes anteriores.
    - En conclusión, las BB se detectan muy bien en general, por lo que únicamente vamos a descartar las 3 imágenes.

\section{Experimentación}
    \subsection{Hipótesis iniciales}
    \noindent Debido a que partimos de \textbf{3fabRec}, un framework diseñado y probado de forma exahustiva por Björn Browatzki y Christian Wallraven, en Experimentación se va a respetar la arquitectura de la red en su totalidad, sin añadir ni eliminar capas. Del mismo modo debido a que la elección del optimizador \textbf{Adam} empleado por los autores ha sido fruto de un proceso de experimentación exahustiva por parte de los mismos no se va a probar a cambiarlo, además el optimizador Adam es adaptativo y tiene muy buen rendimiento en general, por lo que no tenemos ninguna razón que nos lleve a pensar que lograríamos grandes mejoras cambiando de optimizador.
    
    \subsection{Experimentación}
        \subsubsection{Modelo Base: M1}
            \noindent En primer lugar se va a buscar un \textbf{modelo base} con el que obtener los primeros resultados y que iremos refinando hasta obtener el modelo solución. El framework \textbf{3FabRec} viene por defecto con cuatro modelos, todos ellos han sido entrenados en la fase de aprendizaje no supervisado, pero uno viene sin entrenamiento posterior de landmarks en ningún dataset, y los otros tres vienen con un entrenamiento en los datasets \textit{300w}, \textit{AFLW} y \textit{WFLW}. Así pues, la primera decisión a tomar es si emplear un modelo preentrenado en alguno de los datasets anteriores o utilizar uno sin entrenar. 

            \medskip

            \noindent Para tomar la anterior decisión se han repasado los diferentes datasets sobre los que se entrenó la red y se ha decidido que se usará como modelo base el que ha sido entrenado en \textit{AFLW}, debido a que el número de landmarks que predice este modelo es similar al de nuestro problema, ya que predice 21 landmarks, mientras que los demás modelos han sido entrenados para predecir 68 en el caso de \textit{300w} y 98 en el caso de \textit{WFLW}. Por otro lado, los landmarks que predice AFLW, pese a no tener una justificación biológica, son parecidos a los anotados en el dataset que se proporciona. Es por esto que consideramos una buena primera decisión emplear la red \textbf{preentrenada en AFLW}.

            %TODO INSERTAR IMAGEN DE LOS LANDMARKS DE AFLW vs LOS NUESTROS.
        \subsubsection{Modelo:M2}
        \subsubsection{Modelo:M3}
    \subsection{Comparación de resultados}
\endinput
%------------------------------------------------------------------------------------
% FIN DEL CAPÍTULO. 
%------------------------------------------------------------------------------------



