\chapter{Solución propuesta y experimentos realizados}

    \noindent El reconocimiento automático de landmarks faciales es una área de gran importancia en la actualidad en tareas como el reconocimiento de personas y que se está viendo muy desarrollada gracias a las CNN. La principal diferencia entre este enfoque y el forense radica en el tipo de landmarks que se utilizan. Los utilizados en antropología forense son landmarks con justificación biológica. Dichos landmarks se correpsonden con localizaciones concretas del cráneo del ser humano, y los landmarks cefalométricos intentan predecir la posición de estos puntos sobre la piel, a diferencia de los que se emplean en las tareas de reconocimiento facial, que generalmente atienden a puntos de interés de la cara para su correcto reconocimiento y son independientes del cráneo del sujeto. Así pues existen diversas bases de datos empleadas para este cometido con multitud de imágenes etiquetadas, destacamos entre ellas: 

    \begin{itemize}
    \item \textbf{300-W}: Se trata de un dataset compuesto por $3148$ imágenes de entrenamiento y $689$ imágenes de test \textit{in-the-wild}, es decir con multitud de poses, distinta iluminación y expresiones faciales. Está anotada por 51 landmarks o 68 landmarks si contamos el contorno del rostro \cite{300W}. Podemos ver los landmarks en la imagen \autoref{fig:300W}
    
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.5\textwidth]{img/33W.png}
        \caption{Conjunto de landmarks anotados en el dataset 300W, en la imagen \textit{a} contando el contorno del rostro son un total de 68 landmarks, en la imagen \textit{b} son 51 en total. Imagen extraida de \cite{300W}.}
        \label{fig:300W}
    \end{figure}
    
    \item \textbf{AFLW}: Se trata de un dataset de $24386$ imágenes \textit{in-the-wild} con un total de 21 landmarks anotados entre las cejas y el mentón, como podemos ver en la imagen \autoref{fig:AFLW}  \cite{AFLW}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\textwidth]{img/AFLW.png}
        \caption{Conjunto de landmarks anotados sobre un modelo $3$D que emplea el dataset AFLW. Imagen extraida de \cite{AFLW}.}
        \label{fig:AFLW}
    \end{figure}
    \end{itemize}

    \medskip

    \noindent El hecho es que existen actualmente CNN que son capaces de reconocer con un alto grado de precisión estos conjuntos de landmarks marcados por las bases de datos anteriores, lo que nos hace pensar que quizá podría emplearse el conocimiento adquirido por estas redes para reentrenarlas en un proceso de \textit{fine-tuning} sobre una base de datos forense con landmarks anotados por un experto para tratar de resolver el problema de la identificación automática de landmarks. De ahí nace nuestra propuesta, que en cierto modo trata hace como nexo de unión entre las dos vías de investigación. 


\section{Framework empleado: 3FabRec }
        \noindent La red empleada para la resolución del problema es la desarrollada por \textbf{Bjorn Browatzki et al} en $2020$ \cite{browatzki20203fabrec} denominada \textbf{3FabRec}. La red en cuestión es un \textbf{Adversarial Autoencoder} combiando con una red \textit{GAN} (por la presencia de un segundo discriminante propio de las GANs) que puede predecir landmarks gracias a la incorporación de unas capas convolucionales intermedias denominadas \textit{Interleaved Transfer Layers} (ITLs) en la etapa de reconstrucción. Todos estos elementos serán explicados en profundidad más adelante.      

        \medskip

        \noindent Esta red aplica un método \textit{semi-supervisado} en el cual:

        \begin{itemize}
            \item Hay una primera fase de \textbf{aprendizaje no supervisado}, donde se pretende adquirir conocimiento implícito sobre la estructura facial contenida en grandes conjuntos de imágenes de rostros de personas en diversas posiciones, iluminación y etnia. Para ello se codifica todo este conocimiento implícito en un vector de un espacio latente de baja dimensionalidad para posteriormente reconstruir la imagen. Este proceso se hace íntegramente en el \textit{Adversarial Autoencoder}, sin hacer uso de las ITLs.
            \item Posteriormente, en una segunda fase de \textbf{aprendizaje supervisado}, se entrena la red con un conjunto de imágenes etiquetadas con landmarks faciales que la red tratará de predecir. Para ello se intercalan entre las capas del generador capas de convolución. Estas son las ITLs que mencionamos anteriormente, y se encargan de reutilizar el conocimiento estructural aprendido y fijado en la primera fase del entrenamiento para adaptar el generador a la tarea de predecir los mapas de calor.
            \item Finalmente, se puede incluir una tercera fase de \textit{finetuning} en la cual se entrena el Encoder para mejorar el rendimiento en la predicción de landmarks.
        \end{itemize} 

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.98\textwidth]{img/3fabrec_arquitectura.png}
            \caption{Imagen resumen del framework 3FabRec. En ella podemos ver la estructura del \textit{Adversarial Autoencoder}, dividido en un Encoder (región bajo la \textit{E}) y un Generator (región bajo la \textit{G}). En rojo podemos ver las ITLs que se intercalan entre cada capa del Generador y dan como resultado un conjunto de mapas de calor. Imagen extraída de \cite{browatzki20203fabrec}.}
            \label{fig:3FabRec Resumen}
        \end{figure}

        \subsection{Arquitectura Adversarial Autoencoder}
            \noindent Para la construcción del \textit{Adversarial Autoencoder} utilizan:
            
            \begin{itemize}
                \item \textbf{Encoder}: emplean una ResNet-$18$  hasta codificar la entrada en un vector de $99$ dimensiones. Está pensado para imágenes de res $256 \times 256 \times 3$, aunque se adapta también a imágenes de dimensiones $512 \times 512 \times 3$.
                \item \textbf{Decoder}: emplean la misma red ResNet-$18$ pero invertida.
            \end{itemize}

            \noindent Para una mejor comprensión hemos realizado unos diagramas con la herramienta \textit{diagrams.net}. En la \autoref{fig:bloque_encoder} podemos ver la estructura básica de los bloques de la ResNet-$18$. Por otro lado en la \autoref{fig:Paso_encoder} podemos ver el paso de una imagen de entrada de tres canales y resolución $256 \times 256$ por el encoder.

            \begin{figure}[htbp]
                \centering
                \includegraphics[width=0.95\textwidth]{img/bloque_basico_encoder.png}
                \caption{Bloques básicos que utilza la red ResNet-$18$ en sus capas. Se trata de una sucesión clásica de Convolución 3x3 + Batch Normalization + ReLU que se repite dos veces. En el primer caso los filtros de convolución no reducen las dimensiones del tensor añadiendo un padding de 1. En el segundo caso se reduce la dimensión del tensor a la mitad tras la primera convolución y se manteiene la dimensionalidad en la segunda. En el primer caso, la suma residual puede realizarse con el tensor x sin problema, en el segundo caso el tensor debe reducirse para que casen las dimensiones.}
                \label{fig:bloque_encoder}
            \end{figure}

            \begin{figure}[H]
                \centering
                \includegraphics[width=0.7\textwidth]{img/3FabRec-Page-2.drawio.png}
                \caption{Ejemplo de paso de una imagen a través del Encoder. Cabe destacar que a partir de la Layer 1, todos los bloques tienen downsample.}
                \label{fig:Paso_encoder}
            \end{figure}

            \medskip

            \noindent En la \autoref{fig:Bloque_Decoder} podemos ver la estructura básica de un bloque en el Generador \textit{Inverse ResNet}, y un ejemplo del paso de un vector por el generador podemos verlo en la \autoref{fig:Paso_Generator}

            \begin{figure}[H]
                \centering
                \includegraphics[width=0.96\textwidth]{img/bloque_invresnet.png}
                \caption{En primer lugar se aplica una convolución transpuesta que duplica las dimensiones del tensor de entrada y tras esto se sigue la misma estructura que en el bloque básico de la ResNet-$18$, la segunda convolución $3\times 3$ mantiene las dimensiones. Como consecuencia, para sumar el tensor de entrada con la salida del bloque se aumentan las dimensiones de este.}
                \label{fig:Bloque_Decoder}
            \end{figure}

            \begin{figure}[H]
                \centering
                \includegraphics[width=0.95\textwidth]{img/paso_generator.png}
                \caption{Ejemplo del paso de un vector de $99$ dimensiones por el generador hasta reconstruirse la imagen de dimensiones $256 \times 256 \times 3$. La parte correspondiente al aprendizaje supervisado es la de los cuadrados azules, los cuadrados rojos corresponden a las \textit{ITLS} de la parte supervisada que se intercalan entre cada dos capas y dan como resultado los mapas de calor de los landmarks predichos.}
                \label{fig:Paso_Generator}
            \end{figure}

            \noindent Finalmente, necesitamos un Discriminador, que procure que los vectores del espacio vectorial latente sigan una determinada distribución. Dicha distribución será una normal multivariante estándar. Por otro lado, se añadirá un segundo discriminador propio de las redes \textbf{GAN} que nos dirá si la imagen reconstruida procede de la distribución que siguen los píxeles de la imagen de inicio. Este discriminador permite que la red genere imágenes más realistas. En la \autoref{fig:DGaussian} las redes neuronales que definen ambos discriminadores.

            \begin{figure}[H]
                \centering
                \includegraphics[width=0.95\textwidth]{img/DGaussian.png}
                \caption{En la imagen superior vemos el discriminante que se emplea para los vectores producidos por el Encoder y en la imagen inferior vemos el discriminante que se emplea para las imágenes generadas por el Generador. En ambos casos se da como salida un valor entre $0$ y $1$ que hace referencia a la probabilidad de pertenecer a la distribución deseada en el primer caso o a seguir la distribución de los píxeles de las imágenes en el segundo caso.}
                \label{fig:DGaussian}
            \end{figure}

            \subsubsection{Interleaved Transfer Layer (ITL)}
                \noindent Se denominan ITLs a las capas convolucionales encargadas de la predicción de landmarks. Se tratan de una solución novedosa porque premiten adaptar una AAE pensado originalmente para tareas de aprendizaje no supervisado a un problema de \textbf{regresión}, típico del aprendizaje supervisado.

                \medskip
                
                \noindent Se tratan de simples capas convolucionales que se intercalan entre las capas del Generador. La última de estas capas proporciona como salida un conjunto de mapas de calor, uno por cada landmark predicho. Estos mapas de calor luego se emplean para representar en la imagen reconstruida los landmarks. La arquitectura de esta etapa podemos verla en la \autoref{fig:Paso_Generator}.
            
        \subsection{Función de pérdida}

            \noindent La red utiliza dos funciones de pérdida, la primera que presentaremos se emplea en el entrenamiento del \textit{Adversarial Autoencoder}, en la parte no supervisada, y la segunda se utiliza tanto en la parte de aprendizaje supervisado como en la de \textit{fine-tuning} del Encoder. Debido al interés que tiene en el estudio del framework se van a presentar y explicar ambas funciones de pérdida. Cabe resaltar, que debido a que el problema que vamos a resolver es de \textbf{regresión}, en la fase de experimentación únicamente tendrá interés la segunda función de coste, asociada al entrenamiento de la parte supervisada.

            \medskip

            \noindent La función de pérdida empleada para el entrenamiento del \textit{Adversarial Autoencoder} en la parte de aprendizaje no supervisado es la siguiente: 
            
            \begin{align*}
                \min_{E,G} \max_{D_z,D_x} & \mathcal{L}_{AE}(E,G,D_z,D_x) = \\
                & \lambda_{rec} \mathcal{L}_{rec}(E,G) + \lambda_{cs}\mathcal{L}_{cs}(E,G) \\
                & + \lambda_{enc}\mathcal{L}_{enc}(E,D_z)+ \lambda_{adv} \mathcal{L}_{adv}(E,G,D_x)
            \end{align*}

            \noindent En la cual $\lambda_{enc}$ y $\lambda_{adv}$ toman el valor $1.0$ mientras que $\lambda_{rec}$ y $\lambda{cs}$ se establecen en $1.0$ y $60.0$ respectivamente. El valor de la función de coste se propagará por los pesos de $E$,$G$,$D_z$ y $D_x$ actualizándolos mediante \textit{back-propagation}. 

            \medskip

            \noindent Como podemos observar en la expresión anterior, se trata de una combinación lineal de cuatro funciones de coste distintas: 

            \begin{itemize}
                \item \textit{Error de reconstrucción}: Se trata de un error encargado de asegurarse de que la red reconstruya de forma apropiada las imágenes, para ello calcula la distancia $L_1$ entre la imagen original y la reconstruida pixel a pixel. Tiene la siguiente expresión, que coincide con la presentada en el apartado anterior de métricas.
                \begin{equation}
                    \mathcal{L}_{rec}(E,G)= \mathbb{E}_{x~p(x)}\left[\|x- G(E(x)) \|_1\right]
                \end{equation}
                \item \textit{Error estructural de la imagen}: Se trata de una variante del \textbf{SSIM} encargado de que la imagen real y la reconstruida tengan una estructura similar. La expresión de la función $cs$ que aparece en la fórmula se detalla más adelante.
                \begin{equation}
                    \mathcal{L}_{cs}(E,G)= \mathbb{E}_{x~p(x)}\left[cs(x, G(E(x)))\right]
                \end{equation}
                \item \textit{Error del Encoder}: es el error empleado comunmente en este tipo de redes y que se asegura que el vector latente que genera el encoder siga la distribución impuesta en el discriminador $D_z$. Su expresión es:
                \begin{equation}
                    \mathcal{L}_{enc}(E,D_z)=\mathbb{E}_{z^*~p(z)}[\log(D_z(z^*))] + \mathbb{E}_{x~p(x)}[\log(1-D_z(E(x)))]
                \end{equation}
                \item \textit{Error adversario de imagen}: normalmente las imágenes generadas por los Autoencoders suelen generar imágenes emborronadas. Es por ello que se introduce este error, que añade un discriminador típico de una red \textit{GAN} que pretende discernir entre si la imagen que recibe como salida de la red pertenece al conjunto de imágenes de entrenamiento o si es una imagen generada por la red, de esta forma se generan imágenes más realistas. Su expresión es la siguiente: 
                \begin{equation}
                    \mathcal{L}_{adv}(E,D_z)=\mathbb{E}_{x~p(x)}[\log(D_x(x))] + \mathbb{E}_{x~p(x)}[\log(1-D_x(G(E(x))))]
                \end{equation}
            \end{itemize}

            \medskip

            \noindent Por otro lado, la función de pérdida que se empleará en el \textbf{entrenmiento de las \textit{ITLs}} será la siguiente: 

            \begin{equation} \label{eq::L2}
                \mathcal{L}_H(ITL) = \mathbb{E}_{x ~ p(x)} \left[ \| H-ITL(a)\|_2 \right]
            \end{equation}

            \noindent Dónde $a$ serían los mapas de activación que genera la ResNet invertida para la imagen codificada $z=E(x)$ (siendo $x$ la imagen de entrada a la red). En este caso se computa la distancia \textbf{$L_2$} entre los \textit{Heat-maps} de los lanmarks originales de la imagen $x$ de entrada, y los predichos por las \textit{ITLs}. Propagando el error por los pesos de las \textit{ITLS} solamente, forzando a adaptarse a los pesos ya aprendidos del generador.

            \medskip

            \noindent A modo de aclaración, las imágenes etiquetadas con landmarks a la red suelen proporcionarse con el siguiente formato: 

            \begin{itemize}
                \item Un archivo con la imagen sin etiquetar. 
                \item Un archivo de texto plano con las coordenadas de cada landmark en la imagen.
            \end{itemize}

            \noindent Dados estos archivos, el framework, calcula para cada landmark proporcionado un mapa de calor en una imagen de tamaño $128 \times 128$ en el caso de que las imágenes de entrada sean de $256 \times 256$. Y es a ese conjunto de imágenes (una por cada landmark) a las que denominamos $H$ en la función anterior.

            \medskip

            \noindent Finalmente, en la etapa de \textit{fine-tuning} se computa la misma función de pérdida de antes, con la salvedad de que el error se propaga tanto por las \textit{ITLs} como por los pesos del \textit{Encoder}. Esto permite que el \textit{Encoder} se codifiquen con mayor precisión las imágenes y que se eliminen factores irrelevantes para la predicción de landmarks como son el género, el color de piel o la iluminación. Por otro lado se evita el overfitting ya que durante esta última etapa los pesos del \textit{Decoder} no se actualizan.

        \subsection{Proceso de entrenamiento de la red}

            \subsubsection{Entrenamiento no-supervisado}
                \noindent El framework que empleamos ha sido entrenado durante $50$ épocas con imágenes de tamaño $256x256$ y con un tamaño de batch de $50$. En todas las etapas del entrenamiento se ha empleado un optimizador tipo Adam, el cual durante el entrenamiento del \textit{Adversarial Autoencoder} usó $\beta_1=0.0$ y $\beta_2=0.999$ con un \textit{learning rate} de $2\times 10^{-5}$.

                \medskip

                \noindent Por otra parte, se aplicaron técnicas de \textit{data-augmentation} a las imágenes de entrada como giros horizontales, traslaciones, \textit{resizing} o rotaciones.

            \subsubsection{Entrenamiento supervisado}
                \noindent Para el entrenamiento de la parte supervisada, las imágenes de entrada se recortan de acuerdo a un \textit{bounding-box} creado por el framework a partir de unas coordenadas de entrada o bien de acuerdo a los landmarks que se proporcionan. Tras el recorte, se reescala la imagen hasta tener un tamaño de $256\times256$. 
                
                \medskip

                \noindent Por otro lado se crean los \textit{Heat-maps} para cada landmark. Para esto se crea una imagen de tamaño $128 \times 128$ por cada landmark marcando el punto con ayuda de una distribución normalde dos dimensiones centrada en las coordenadas del landmark y usando una desviación típica de $\sigma=7$.

                \medskip

                \noindent Tras esto se entrenan las cuatro \textit{ITLs}. A los datos de entrada se les aplican técnicas de \textit{data-augmentation} como rotaciones, traslaciones, reescalados y oclusiones. Para esta etapa se usa también un optimizador Adam con un \textit{learning rate} de $0.001$ y los mismos valores para $\beta_1$ y $\beta_2$.

                \medskip

                \noindent Finalmente, durante la etapa de \textit{fine-tuning} se establece un \textit{learning-rate} de $0.0001$ en las \textit{ITLs} mientras que el del \textit{Encoder} se mantiene en su valor por defecto de $2 \times 10^{-5}$ y cambiando el valor de $\beta_1 = 0.9$.

        \subsection{Bases de datos usadas por el framework}
            
            \noindent Para el entrenamiento no supervisado del \textit{AAE}, se emplearon los siguientes datasets unidos: 

            \begin{itemize}
                \item \textbf{VGGFace2} : Contiene un total de $3.3$ millones de imágenes de rostros en distintas poses, edad, iluminación, etnia, etc \ldots  Del dataset eliminaron las imágenes de rostros que tuviera una altura menor a $100$ píxeles, quedando un total de $1.8$ millones de caras.
                \item \textbf{AffectNet} : Se trata de un dataset de $228$ mil imágenes en una gran variedad de poses, iluminación, etc..
            \end{itemize}

            \noindent En total usaron unas $2.1$ millones de imágenes.

            \noindent Para el entrenamiento supervisado se emplearon los siguientes datasets: 

            \begin{itemize}
                \item \textbf{300-W} : une diversos datasets de rostros etiquetados con \textbf{$68$}landmarks de manera semi-automática como son \textbf{LFPW},\textbf{AFW}, \textbf{HELEN} y \textbf{XM2VTS}. Además de añadir datos propios. En total emplearon $3,148$ (aproximadamente el $80 \%$ )imágenes para el entrenamiento y $689$ para test, las cuales se dividieron en dos grupos, uno de $554$ imágenes considerado el grupo test estándar, y otro de $135$ imágenes difíciles.
                \item \textbf{AFLW} : contiene un total de $24,386$ imágenes \textit{in-the-wild} con un amplio rango de poses distintas. Se emplearon $20,000$ (aproximadamente el $80 \%$) imágenes para test y $4,386$ para entrenamiento. Las imágenes vienen etiquetadas con $21$ landmarks, pero en el framework se entrena la red para predecir \textbf{$19$}.
                \item \textbf{WFLW}: Es la más reciente de las empleadas y tiene un total de $10,000$ imágenes. Se usan $7,500$ para entrenamiento y $2,500$ para test. Las imágenes tienen un total de \textbf{$98$} landmarks anotados.
            \end{itemize}

            \medskip

            \noindent Como podemos observar, los conjuntos de datos disponibles para el entrenamiento de redes especializadas en la identificación de landmarks faciales son variados y con muchos ejemplos de entrenamiento. En nuestro caso concreto, no contamos con tantos ejemplos y apenas hay bases de datos forenses con landmarks cefalométricos, lo cual es una dificultad añadida al problema, pues solo podremos entrenar con los datos que se nos proporcionan y como veremos son muy pocos.

\section{Métricas}

\noindent En esta sección, se van a presentar las principales métricas de error que se emplearán para estudiar la bondad de los resultados que se obtengan en futuros capítulos.

    \subsection{Métricas usadas en el entrenamiento}
        \noindent Las métricas que se muestran durante el entrenamiento utilizadas por el modelo son:

        \subsubsection{MSE}
            \noindent Se trata del error cuadrático medio, una función empleada típicamente en problemas de regresión y que tiene la siguiente expresión: 

            \begin{equation}
                MSE = \frac{1}{N} \sum_{i=1}^{N} (y_i - \widehat{y}_i)^2
            \end{equation}

            \noindent En nuestro caso concreto, este error se calcula entre los mapas de calor de los landmarks reales y predichos a nivel de pixel, es decir $y_i$ en la fórmula anterior corresponde a cada pixel del heatmap del landmark real y por tanto $\widehat{y}_i$ es el valor del pixel correspondiente en el heatmap predicho. Dichas diferencias se suman en todos los heatmaps y se divide por $N$ que en este caso es el número total de píxeles juntando todos los Heat-maps.

            \medskip

            \noindent Este error se empleará para realizar \textit{backpropagation} a través de las ITLs de la red y será el responsable del aprendizaje de la red.

    \subsection{Métricas empleadas en validación y testing}

        \subsubsection{Error de reconstrucción}
            \noindent El valor del error de reconstrucción en las imágenes de validación nos llevará en la sección de experimentación a realizar hipótesis sobre la relación entre este error y el NME.

            \medskip

            \noindent Se trata de la función de coste \textbf{L1} que se aplica a la imagen original y la reconstruida  y nos porporciona una medida del error de reconstrucción a nivel de pixel. Su expresión es la siguiente:

            \begin{equation}
                Reconstruction \; Loss = \frac{\sum_{i=1}^n |p_i -q_i|}{N}
            \end{equation}

            \noindent Donde $p_i$ representa cada pixel de la imagen reconstruida por la red, $q_i$ el equivalente en la imagen real y $N$ el total de píxeles presentes en la imagen.

        \subsubsection{Normalized Mean Error}
            \noindent Este será el error utilizado para medir la bondad de los resultados. Tiene la siguiente expresión para cada landmark de los $30$: 

            \begin{equation}
                NME=\frac{\sqrt{\sum_{i=1}^{n}(p_{i,1} -q_{i,1})^2+ (p_{i,2} -q_{i,2})^2}}{N}
            \end{equation}


            \noindent Así, si en total se han identificado en las imágenes de validación un total de $n$ landmarks de cierto tipo en las imágenes de test o validación, se calcula la distancia $L_2$ entre el landmark de la imagen $i$ predicho y el real. Y tras esto se suman todos los valores obtenidos. Como podemos ver, idealmente $n$ debería ser el total de imágenes que hay en validación o test, pero no todos los landmarks aparecen en todas las imágenes, por lo tanto $n\leq M$ siendo $M$ el total de imágenes en validación.

            \noindent En dicha expresión $N$ es el término por el que se normaliza, y varía según el problema. En nuestro caso normalizaremos la media del numerador por el ancho de la imagen que es de $256$ píxeles, ya que la distancia máxima permitida entre dos landmarks sería esta, y así todos los valores quedarían entre $0$ y $1$ siendo $0$ una estimación perfecta del landmark, y $1$ un error total en el marcado. Es importante la normalización debido a la heterogeneidad de los tamaños de los datos de entrada. Así, el NME extraído en cada dato es comparable con los del resto.

            \subsubsection{SSIM}
            \noindent Se trata del \textit{structural similarity index} (SSIM) y da una medida de \textit{similaridad} entre dos imágenes \cite{wang2004image}, su expresión es la siguiente: 

            \begin{equation}
                SSIM(x,y)=[l(x,y)]^\alpha[c(x,y)]^{\beta}[s(x,y)]^{\gamma}
            \end{equation}

            \medskip

            \noindent Dónde $x,y$ son las dos imágenes que van a ser comparadas.

            \medskip

            \noindent La componente $c(x,y)$ hace referencia a la función de comparación del contraste de las dos imágenes y viene dada por la siguiente expresión: 

            \begin{equation}
                c(x,y)=\frac{2\sigma_x \sigma_y + C}{\sigma_x^2+ \sigma_y^2+C}
            \end{equation}

            \noindent Dónde $\sigma_x, \sigma_y$ hacen referencia a la desviación estándar de cada imagen.

            \medskip

            \noindent La componente $s(x,y)$ es la función de comparación estructural entre las dos imágenes y viene dada por la siguiente expresión:

            \begin{equation}
                s(x,y)=\frac{\sigma_{xy}+C/2}{\sigma_x \sigma_y +C/2}
            \end{equation}

            \noindent Dónde $\sigma_{xy}$ denota la covarianza.

            \medskip

            \noindent La componente $l(x,y)$ hace referencia a la \textit{luminosidad}, pero en el caso de 3FabRec se prescinde de esta componente, además los exponentes $\alpha, \beta , \gamma$ se igualan a $1$.

            \medskip

            \noindent Por otra parte siguen las indicaciones del paper original de SSIM que recomiendan usar estas comparaciones en regiones de la imagen y promediarlas en vez de aplicarlas sobre todo el conjunto de píxeles de la imagen, es por ello que la expresión final queda:

            \begin{equation}
                cs(x,y)=\frac{1}{|w|} \sum{c(x_w,y_w)s(x_w,y_w)}_w
            \end{equation}

            \noindent Dónde $w$ representa la ventana sobre la que se aplica la función y $|w|$ el total de ventanas. En nuestro caso se emplean ventanas de tamaño $31\times 31$.

\section{Preprocesamiento de los datos}
        
        \subsection{Identificación de caras en las imágenes}
            \noindent Antes de poder entrenar el modelo, es necesario adecuar el dataset para que pueda usarse modelo. El framework está pensado para recibir como entrada una imagen, a la cual, se le hace un recorte a la región que ocupa el rostro con ayuda de un \textit{bounding box} predefinido y determinado por las coordenadas en la imagen de las cuatro esquinas o bien siguiendo los landmarks descritos por el fichero auxiliar asociado a cada imagen. En nuestro caso, debido a que la mayoría de landmarks son internos y no se suelen situar en los bordes de la cara, hemos pensado que la mejor manera de entrenar la red es que se realice el recorte de la imagen de entrada de acuerdo a un \textit{bounding box}. Para ello fue necesario emplear una red especializada en la detección de caras en imágenes y que nos proporcionaba como salida una lista con los \textit{bounding boxes} de todas las caras detectadas. La red se denomina \textbf{Facenet}, y se puede importar de la librería \textit{facenet} de \textit{pytorch} como \textit{mtcnn}.

            \medskip

            \noindent Así pues, se realizó un primer estudio para determinar los mejores parámetros para \textit{facenet}. Existen muchos parámetros editables, pero la mayoría se dejaron con sus valores por defecto, únicamente se consideró modificar: 

            \begin{enumerate}
                \item \textbf{image\_size}: Determina el tamaño de la imagen de salida.
                \item \textbf{select\_largest}: Si su valor es \textit{true}, se devuelve la cara más grande detectada, y si su valor es \textit{false}, se devuelve la de mayor confianza detectada.
            \end{enumerate}


            \noindent En nuestro caso, primero se probó con un \textit{image\_size} de $256 \times 256$, pero esto tenía problemas pues en ocasiones no podían construirse \textit{bounding boxes} de este tamaño y producían error, por lo que se redujo a $128 \times 128$. Por otro lado, observando las imágenes del dataset de entrenamiento, nos dimos cuenta que en algunas imágenes aparecían varios sujetos, y en ocasiones la cara más \entrecomillado{grande} mo pertenecía al sujeto principal. Es por ello que se decidió establecer el parámetro \textit{select\_largest} a \textit{False}, y así obtener como salida una lista de \textit{bounding boxes} ordenados de mayor a menor según el nivel de confianza. No obstante, en algunos casos se tuvo que elegir con cuidado cuál de los bounding boxes proporcionados se correspondía con el sujeto principal.

            \medskip

            \noindent Una vez establecidos los parámetros, se detectaron problemas en la identificación de caras en algunas imágenes. Debido a que todas las imágenes erróneas estaban en escalas de grises nos dimos cuenta de que la red sólo acepta como entrada imágenes en tres canales, por lo que cada imagen en escala de grises de un solo canal tuvimos que replicar tres veces su canal. 
        
            \medskip
        
            \noindent Tras esto, volvimos a ver que en algunas imágenes no se detectaban \textit{bounding boxes}. Esto nos llevó a realizar un breve estudio sobre si era buena idea continuar usando esta red o buscar otra. En este estudio clasificamos las imágenes en tres grupos y aplicamos la red a cada uno de ellos por separado: 

            \begin{enumerate}
                \item Imágenes de sujetos en posición \textbf{ frontal}: en total hay $87$ imágenes frontales de las cuales se extraen correctamente el \textbf{$100\%$} de los \textit{bounding boxes}.
                \item Imágenes de sujetos en posición de \textbf{$3/4$}: En este caso hay $57$ imágenes de las cuales el \textbf{$100\%$} de los \textit{bounding boxes} extraidos por la red son correctos.
                \item Imágenes de sujetos en posición de \textbf{perfil}: En este caso hay un total de $23$ imágenes y se clasifican correctamente $20$, lo que supone una precisión del \textbf{$87\%$}. Además, las imágenes que fallaban estaban en escala de grises y con malas condiciones de calidad e iluminación.
            \end{enumerate}

            \noindent Por lo tanto, debido al bajo número de ejemplos en los que la red falla, se decidió mantener la red \textbf{Facenet} para determinar los \textit{bounding boxes} y además se excluyeron los tres ejemplos dónde fallaba del dataset, lo que hizo que en total se usaran $164$ de las $167$ imágenes originales.

            \medskip 

            \begin{figure}[H]
                \centering
                \includegraphics[width=0.7\textwidth]{img/imagenes_ejemplo_bb.png}
                \caption{Imágenes de ejemplo del dataset con los \textit{bounding boxes} marcados por \textit{Facenet}. Como podemos observar, aunque son correctos, los \textit{bounding boxes} marcados recortan excesivamente los límites del rostro, pudiendo incluso eliminar partes en las que hay landmarks marcados. Las imágenes han sido generadas con \textit{matplotlib}.}
                \label{fig:Ejemplo_bb}
            \end{figure}

            \noindent Durante el estudio anterior, se observaron otros hechos dignos de mención. En primer lugar se identificaron dos imágenes repetidas, pero se decidió mantenerlas pues los landmarks presentes en una y otra eran diferentes, lo que podía ayudar al entrenamiento. Por otro lado, durante el entrenamiento se vió cómo había una imagen de un determinado sujeto en la que aprecían simultáneamente dos fotos, una  de frente y otra de perfil. Sin embargo, al estar los landmarks anotados únicamente en la imagen de frente se tomó el \textit{bounding box} de la imagen frontal desechando el otro para el perfil. En otras dos imágenes se pueden ver un conjunto de varias personas, y la red mostraba erróneamente los \textit{bounding boxes} de personas que no eran el sujeto de estudio, por lo que estudiar cuál de todos los \textit{bounding boxes} devueltos pertenecía al sujeto correcto.

        \subsection{Creación del fichero annotations en el dataset}

            \noindent Una vez se identificaban correctamente todos los \textit{bounding boxes}, siguiendo como inspiración el dataset \textit{AFLW}, se generó un fichero denominado \textit{annotations.csv} dentro de la carpeta del proyecto en el que encuentra el dataset. En este fichero se almacenó \textbf{para cada imagen} una serie de datos: 

            \begin{itemize}
                \item El nombre del fichero.
                \item El índice de la imagen dentro del dataset. 
                \item Una lista con las coordenadas $2D$ de cada landmark en la imagen (incluidos los no presentes como situados en la posición $[-1,-1]$). 
                \item Una lista denominada máscara, que representa la visibilidad de cada landmark en la imagen. Los landmarks visibles en la imagen tienen asociado el valor $1$, mientras que los no visibles tienen asociado el valor $0$. Esta máscara será de gra utilidad a la hora de extraer las métricas, pues hay que realizar las medias de acuerdo al número de landmarks visibles de cada tipo, no se puede usar siempre el valor $30$.
                \item Finalmente, se almacena la posición en coordenadas de la esquina inferior izquierda del \textit{bounding box} junto con el ancho y el alto, para que el framework recorte la imagen quedándose con el rostro únicamente. 
            \end{itemize}

        \subsection{Reajuste de los bounding boxes}
            \noindent Finalmente, como parte del proceso de determinar los \textit{bounding boxes}, se vió que en todos los casos, dichos rectángulos cortaban parte de las caras de los sujetos (partes en las que había landmarks marcados). Es por ello, que se tuvo que realizar una transformación del \textit{bounding box} sugerido para que manteniendo el centro del mismo abarcase una mayor superficie y que contuviera el rostro completo del sujeto. 

            \medskip

            \noindent La transfomación consiste en desplazar la esquina superior izquierda del rectángulo una cantidad \textit{m} en el eje de ordenadas y abscisas y establecer el resto de esquinas de acuerdo a este parámetro como se muestra en la \autoref{fig:Transformacion_BB}.


            \begin{figure}[!h]
                \centering
                \includegraphics[width=0.8\textwidth]{img/Transformacion_rectangulo.png}
                \caption{Proceso seguido para la transformación.}
                \label{fig:Transformacion_BB}
            \end{figure}

            \begin{figure}[!h]
                \centering
                \includegraphics[width=0.7\textwidth]{img/imagenes_ejemplo_cropping.png}
                \caption{Recorte de las caras tras el reajuste del \textit{bounding box}.}
                \label{fig:Reajuste_bb}
            \end{figure}

            \medskip

            \noindent No obstante, el framework 3FabRec dispone de un método de recorte y redimensión automático de las caras empleando \textit{bounding boxes}. En un primer momento, forzamos al framework a respetar los \textit{bounding boxes} construidos durante el preprocesamiento, pero los errores de reconstrucción de estas imágenes eran muy elevados. No resultaba práctico para entrenar la red, pues el framework marca los puntos sobre localizaciones que no guardaban parecido con el lugar real del punto en la imagen original. Podemos ver algunos ejemplos de reconstrucción en la \autoref{fig:bb_custom}.


            \begin{figure}[!h]
                \centering
                \includegraphics[width=0.7\textwidth]{img/bounding_box_custom.png}
                \caption{Error estructural y de reconstrucción de las imágenes con el bounding box reajustado. El error de reconstrucción se encuentra en la esquina inferior izquierda, y el estructural encima del anterior. Como podemos observar, en todos los casos son muy elevados y la imagen reconstruida apenas guarda relación con la original.}
                \label{fig:bb_custom}
            \end{figure}

            \medskip

            \noindent Se probó entonces a aplicar al \textit{bounding box} definido, el reajuste que realiza 3FabRec. Este nuevo reajuste dejaba en la mayoría de casos mal marcado el \textit{vertex}, pues suele recortar la parte superior de la cabeza. No obstante los errores de reconstrucción eran mucho menores con esta alternativa y los landmarks predichos se localizaban en las imágenes reconstruidas sobre puntos que guardaban una estrecha relación con la localización real en la imagen original (véase la \autoref{fig:bb_3fabrec}). Se decidió entonces estudiar por qué ocurría esto.

            \begin{figure}[!h]
                \centering
                \includegraphics[width=0.7\textwidth]{img/bounding_box_3fabrec.png}
                \caption{Error estructural y de reconstrucción de las imágenes con el reajuste del bounding box de 3FabRec. Como podemos ver, sigue habiendo grandes errores de reconstrucción y estructurales, pero se consigue un mejor resultado de la estructura facial. No obstante, el vertex no se aprecia correctamente en las imágenes.}
                \label{fig:bb_3fabrec}
            \end{figure}

            \noindent El reajuste que realiza 3FabRec es el siguiente:
            \begin{enumerate}
                \item En primer lugar, se determina el tamaño de la región de interés (ROI, del inglés region of interest) en píxeles. Para ello, el framework por defecto calcula la diagonal del tamaño de las imágenes de entrada (en nuestro caso son $256 \times 256$), y lo redondea al entero superior más próximo (el resultado es de $363$ píxeles, en nuestro caso). De este valor se obtienen dos parámetros, el \textit{crop\_size} y el \textit{margin}, de manera que \textit{crop\_size + margin = roi} (en nuestro caso \textit{crop\_size}=$256$ y \textit{margin}=$107$).
                \item En segundo lugar, se reajusta el bounding box convirtiéndolo siempre en cuadrado de lado $\max(ancho,alto)$. Se calcula el factor de escalado $escala = \frac{crop\_size + margin}{crop\_size}$, para ampliar el bounding box original uniformemente, de acuerdo al margin. 
                \item  Finalmente se reescala el resultado a una imagen de tamaño $256\times 256$.
            \end{enumerate}

            \medskip

            \noindent Por otro lado, el entrenamiento no supervisado del \textit{AAE} hizo que la red aprendiera la distribución de un determinado dataset de imágenes. Estudiando minuciosamente los ficheros encargados de preparar los datos de entrenamiento (affectnet.py y vgg2face.py), se vio que en ambos, el tamaño de las imágenes es de $256 \times 256$, pero no se especifica un valor determinado para el ROI, por lo que este se calcula por el framework de la misma manera que hace con nuestras imágenes, realizando un ajuste de los bounding boxes idéntico al nuestro. Además, para el ajuste fino del encoder utilizando las imágenes de \textit{AFLW}, de nuevo se aplicaron estas transformaciones.
            
            \medskip
            
            \noindent Esto, puede explicar por qué en el primer caso no se reconstruyen bien las imágenes, ya que se fuerza a que no se apliquen estas transformaciones a las imágenes de entrada. De esta manera, la distribución de las imágenes de entrada cambia radicalmente con respecto a la distribución con la que fue entrenada la red y su posterior \textit{fine-tuning} con \textit{AFLW}. Sin embargo, si permitimos que 3FabRec realice las transformaciones descritas anteriormente, conseguimos que la distribución de los datos de entrada (pese a seguir siendo diferente) sea lo mas \entrecomillado{parecida} posible a la distribución sobre la que fue entrenada la red, mejorando la reconstrucción de las imágenes. 
            
            \medskip
            
            
            \noindent Por lo expuesto anteriormente, se decide continuar con el reajuste que realiza el framework para lograr una mejor reconstrucción, aunque se pierda el marcado correcto del \textit{vertex}.

    \subsection{Separación en conjuntos de entrenamiento y validación}
        \noindent Separamos la base de datos proporcionada en conjuntos de entrenamiento, validación y test. Por el limitado número de ejemplos que tenemos, será necesario emplear \textbf{validación cruzada} para la elección entre los modelos que se prueben. Así pues, de las \textbf{164 imágenes} que usaremos en total, establecemos una semilla para que los procesos aleatorios sean replicables en cualquier máquina y con ayuda de la función \textit{random-shuffle} de python separamos los datos en: 

        \begin{itemize}
            \item \textbf{Conjunto de test}: 33 imágenes ($20\%$ del total de imágenes).
            \item \textbf{Conjunto de entrenamiento:} 131 imágenes ($80\%$ del total de imágenes).
        \end{itemize}

        \noindent Por otro lado, el conjunto de entrenamiento se subdivide a su vez en cinco subconjuntos, con la idea de realizar \textbf{5-fold cross-validation}. Cada subconjunto estará compuesto por \textbf{26 imágenes} exceptuando el último que tendrá \textbf{27}.
        
        \medskip

        \noindent Para esta técnica se realizan cinco ejecuciones independientes en la cual se toman cuatro de los cinco subconjuntos de entrenamiento para entrenar el modelo y se deja el último subconjunto para validar el modelo. Así, en cada iteración se cambia el conjunto que se empleará para validación, con la finalidad de mejorar la calidad de las decisiones que se tomen sobre el rendimiento, pues la decisión estará mejor informada.

        \medskip

        \noindent Con cada propuesta de modelo, se medirá su rendimiento usando \textbf{5-fold cross-validation}, obteniendo 5 salidas de validación para cada modelo. Cada salida se compone de una tabla en la que aparece el NME medio por landmark (teniendo en cuenta sólamente los marcados en validación), y una tabla en la que para cada imagen de validación se obtiene su NME medio (haciendo la media del NME cometido por cada landmark), y el error de reconstrucción. Para la elección del modelo final se tendrá en cuenta: 

        \begin{itemize}
            \item Una tabla en la que se promedian las cinco tablas obtenidas con el NME medio por landmark.
            \item Una tabla en la para cada imagen del conjunto de entrenamiento, se tiene el NME cometido y el error de reconstrucción. Esta tabla se obtiene concatenando todas las predicciones de cada partición de cross-validation.
        \end{itemize}


\section{Experimentación}
    \subsection{Hipótesis iniciales}
        \noindent Debido a que partimos de \textbf{3FabRec}, un framework diseñado y probado de forma exahustiva por Björn Browatzki y Christian Wallraven, en los experimentos se va a respetar la arquitectura de la red en su totalidad, sin añadir ni eliminar capas. Del mismo modo, debido a que la elección del optimizador \textbf{Adam} empleado por los autores ha sido fruto de un proceso de experimentación exahustiva por parte de los mismos no, se va a probar a cambiarlo. Además, el optimizador Adam es adaptativo y tiene muy buen rendimiento en general, por lo que no tenemos ninguna razón que nos lleve a pensar que lograríamos grandes mejoras cambiando de optimizador.

        \medskip

        \noindent También, en la creación de los \textit{Heat-maps} se ha optado por mantener el valor de $\sigma =7$ que se recomienda en el paper \cite{browatzki20203fabrec}, ya que de nuevo ha sido probado minuciosamente por los autores del framework.
    
    \subsection{Modelo Base}
        \noindent Buscamos un \textbf{modelo base} con el que obtener los primeros resultados, y que iremos refinando hasta obtener la solución. El framework \textbf{3FabRec} viene por defecto con cuatro modelos. Todos ellos han sido entrenados en la fase de aprendizaje no supervisado con la unión explicada de los datasets \textit{VGG2Face} y \textit{Affectnet}. Uno de ellos viene sin entrenamiento posterior de landmarks en ningún dataset, y los otros tres vienen con un entrenamiento y \textit{fine-tuning} del encoder een los datasets \textit{300w}, \textit{AFLW} y \textit{WFLW}.

        \medskip

        \noindent Se ha optado por usar como modelo base el que ha sido entrenado en \textit{AFLW}, porque el número de landmarks que predice este modelo es similar al de nuestro problema (21 landmarks), y la estructura de estos landmarks (visibles y no visibles) es similar a la nuestra.

        \medskip

        \noindent Por otro lado, debido a que no conocemos todavía el efecto que tiene el entrenamiento con la etapa de \textit{ajuste fino} del encoder sobre los datos, vamos a realizar un entrenamiento únicamente de las \textbf{ITLs} durante un total de \textbf{60 épocas}, hasta la convergencia del \textit{mse} (que recordamos, es la medida de error entre los \textit{Heat-maps} predichos por el modelo y los reales), y no utilizaremos técnicas de \textit{data augmentation}. Por otro lado, los parámetros que se han usado han sido los que sugieren por defecto en el framework, ya que aún no tenemos información suficiente sobre el rendimiento como para cambiarlos. Dichos valores son: 

        \begin{itemize}
            \item El \textit{learning rate} para el optimizador Adam usado en el entrenamiento de las ITLs se establece en $0.001$. 
            \item El valor del parámetro $\beta_1$ se establece en $0.9$.
            \item El valor del parámetro $\beta_2$ se establece en $0.999$.
        \end{itemize}

        \subsubsection*{Análisis cuantitativo}

        \noindent La predicción por imagen, que resulta de concatenar la salida de \textbf{cross validation 5-fold } para cada partición, se pueden encontrar en \autoref{ap:apendiceA}. La información relativa a la distribución de los errores, tanto NME como reconstrucción, pueden verse en la \autoref{fig:boxplot_ModeloBase_NME} y la \autoref{table:ModelBase_images_results}.

        \begin{table}[!ht]
            \centering
            \caption{Métricas obtenidas de la distribución de los errores cometidos en el conjunto de entrenamiento con el modelo base.}
            \begin{tabular}{|l|l|l|l|l|l|l|}
            \hline
                \cellcolor{gray!25}\textbf{Error} & \cellcolor{gray!25}\textbf{P25} & \cellcolor{gray!25}\textbf{P50} & \cellcolor{gray!25}\textbf{P75} & \cellcolor{gray!25}\textbf{RI} & \cellcolor{gray!25}\textbf{Media} & \cellcolor{gray!25}\textbf{Valores atípicos} \\ \hline
                NME & 1.839 & 2.18 & 2.860 & 1.021 & 2.974 & 12 \\ \hline
                Reconstrucción & 30.02 & 37.104 & 44.38 & 14.35 & 38.04 & 4 \\ \hline
            \end{tabular}
            \label{table:ModelBase_images_results}
        \end{table}
         
        \medskip

        \noindent Como podemo ver, la distribución del error NME es compacta, lo cual es un buen indicador, pues buscamos un modelo robusto con un rendimiento similar en la mayoría de imágenes y casos del problema. Por otro lado, los outliers representan el $9,16 \%$ de los datos, un porcentaje no muy elevado pero que dada la poca cantidad de datos de la que disponemos, intentaremos reducir. Estos valores suelen corresponderse con imágenes especialmente complejas (de perfil y de baja calidad), en las que el error de reconstrucción también es muy elevado. En cambio, en lo que respecta al error de reconstrucción, la distribución no está tan concentrada, los valores son en general altos, pero el número de outliers es mucho menor ($3.05\%$).

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\textwidth]{img/boxplot_modelo_base.png}
            \caption{Diagramas de cajas de los resultados de NME y error de validación en el conjunto de las imágenes de entrenamiento en el Modelo Base. Se han omitido los valores atípicos para una correcta visualización.}
            \label{fig:boxplot_ModeloBase_NME}
        \end{figure}

        \noindent Podemos concluir, que la reconstrucción a nivel de pixel no es adecuada, aunque como vemos en la \autoref{fig:Ejemplo_ModelBase}, la reconstrucción de las caras es apropiada en general, es decir, almacena conocimiento implícito sobre la estructura facial correctamente. Como el framework emplea el conocimiento aprendido sobre la estructura facial para la predicción de landmarks, sería ideal que esta reconstrucción fuese lo más fiel posible. No obstante, el error en el marcado de landmarks cometido es bajo.

        \begin{table}[!ht]
            \centering
            \caption{Media del error NME obtenido por landmark entre todas las particiones de cross-validation. Marcamos en amarillo el vertex pues no está correctamente marcado en todas las imágenes.}
            \begin{tabular}{|l|l|l|}
            \hline
                ~ & \cellcolor{gray!25}\textbf{Landmark} & \cellcolor{gray!25}\textbf{Media NME} \\ \hline
                0 & Menton & 4.176 \\ \hline
                1 & Gnathion & 2.629 \\ \hline
                2 & Pogonion & 2.509 \\ \hline
                3 & Prosthion & 1.178 \\ \hline
                4 & Labiale Superius & 1.965 \\ \hline
                5 & Subnasale & 2.056 \\ \hline
                6 & Nasion & 2.211 \\ \hline
                7 & Glabella & 2.562 \\ \hline
                \cellcolor{yellow!50}8 & \cellcolor{yellow!50}Vertex & \cellcolor{yellow!50}5.904 \\ \hline
                9 & Left Gonion & 5.652 \\ \hline
                10 & Right Gonion & 5.655 \\ \hline
                11 & Left Zygion & 5.351 \\ \hline
                12 & Right Zygion & 6.504 \\ \hline
                13 & Left Alare & 1.972 \\ \hline
                14 & Right Alare & 2.27 \\ \hline
                15 & Left Endocanthion & 1.675 \\ \hline
                16 & Right Endocanthion & 1.731 \\ \hline
                17 & Left Exocanthion & 1.7 \\ \hline
                18 & Right Exocanthion & 1.651 \\ \hline
                19 & Left Tragion & 4.15 \\ \hline
                20 & Right Tragion & 5.639 \\ \hline
                21 & Labiale inferius & 1.734 \\ \hline
                22 & Trichion & 4.445 \\ \hline
                23 & Supramentale & 1.933 \\ \hline
                24 & Left Frontotemporale & 2.37 \\ \hline
                25 & Right Frontotemporale & 2.97 \\ \hline
                26 & Left Frontozygomaticus & 1.566 \\ \hline
                27 & Right Frontozygomaticus & 2.483 \\ \hline
                28 & Left Midsurpaorbital & 1.168 \\ \hline
                29 & Right Midsupraorbital & 1.429 \\ \hline
            \end{tabular}
            \label{table:ModelBase_landmarkresume}
        \end{table}
        \medskip

        \noindent Los resultados por landmark se puden ver en la \autoref{table:ModelBase_landmarkresume} y son buenos en general para tratarse del modelo base. El landmark mejor marcado en media es el \textit{Prosthion}, y el peor el \textit{Right Zygion}. Si recordamos la \autoref{fig:Histograma}, sorprende el hecho de que el \textit{Prosthion} es de los landmarks que menos presencia tiene en las imágenes (aparece en menos de $40$ imágenes), esto nos demuestra la capacidad de 3FabRec de aprender con pocos ejemplos de entrenamiento. Por otro lado, el \textit{Right Zygion}, pese a aparecer en unas $100$ imágenes, resulta más dificil para la red predecirlo. Esto puede deberse a que las imágenes con mayor error de reconstrucción son las de perfil, y es en este tipo de imágenes en las que mejor se aprecia dicho landmark. Por otro lado, pese a que el \textit{Vertex} no se encuentra correctamente marcado en la mayoría de las imágenes, hemos optado por medir su rendimiento también.

        \medskip

        \noindent En la \autoref{fig:Curvas_modelbase} podemos observar las curvas de aprendizaje de cada partición. Podemos observar un rápido descenso del error en entrenamiento y validación hasta la época $30$-$40$, a partir de la cual tiende a estabilizarse el error.

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\textwidth]{img/curvas_aprendizaje_modelbase.png}
            \caption{Curvas de aprendizaje en cada partición del modelo base.}
            \label{fig:Curvas_modelbase}
        \end{figure}

        \noindent En resumen, los resultados son esperanzadores para tratarse de una primera aproximación. Sin embargo, consideramos que debido a la gran variabilidad del dataset tanto en posturas, como en calidad e iluminación de imagen, la reconstrucción de las imágenes se ha resentido, especialmente en las imágenes de perfil. Esto provoca que el modelo también marque peor los landmarks, por lo que vamos a intentar mejorar esto en los próximos modelos. 
        
        
        \subsubsection*{Análisis cualitativo}
        
        \medskip Podemos ver el rendimiento del modelo en algunas imágenes en la \autoref{fig:Ejemplo_ModelBase}, en ellas podemos apreciar algunos ejemplos a color y en escalas de grises. Como podemos apreciar, los errores más altos de reconstrucción se producen en las imágenes en escalas de grises y con menor calidad, mientras que los errores más bajos se producen en las imágenes a color, que por lo general tiene mayor resolución. Por otro lado, se puede apreciar que el error cometido por el modelo es bajo en todos los casos, aunque como anticipamos en la sección anterior, el \textit{Vertex} apenas se aprecia en ningún ejemplo.

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\textwidth]{img/image_basemodel.png}
            \caption{Imágenes pertenecientes a distintos conjuntos de validación para el \textbf{modelo base}. Las imágenes de la fila superior son las imágenes reales, y las de la fila inferior las reconstruidas por la red. El marcado se realiza sobre las reconstruidas y luego se lleva a las superiores. Podemos apreciar en verde los landmarks reales y  en azul los predichos. El valor que aparece en la esquina inferior derecha es el NME global de la imagen (la media de los NMEs de cada landmark) y en la esquina inferior izquierda aparece el error de reconstrucción y encima el SSIM.}
            \label{fig:Ejemplo_ModelBase}
        \end{figure}

    \subsection{Modelo con reentrenamiento del encoder}
        \noindent A la luz de los resultados anteriores, vamos a cambiar el modelo de para entrenar las ITLs (que seguirán usando el mismo optimizador con los mismos parámetros por defecto), junto con los pesos del \textit{encoder}. Con este enfoque, se pretende asociar a cada imagen un vector latente que capte las nuevas variaciones del dataset junto con el entrenamiento en la predicción de landmarks.

        \medskip

        \noindent Este proceso restringe el riesgo de producir \textit{overfitting}, pues durante las etapas de entrenamiento los pesos del \textit{decoder} permanecerán congelados, y serán únicamente los pesos del \textit{encoder} y las ITLs los que se actualicen con backpropagation. El optimizador empleado para el entrenamiento del \textit{encoder} es Adam, junto con sus parámetros por defecto $\beta_1=0.9$ y $\beta_2=0.999$. El \textit{learning rate} se establece a $2^{-6}$, en vez de $2^{-5}$ pues, durante la experimentación, el antiguo \textit{learning rate} quedaba atrapado en extremos locales con facilidad. El entrenamiento duró en total \textbf{100 épocas}, pues como veremos, el modelos mejora más lentamente en el reconocimiento de landmarks.

        \subsubsection*{Análisis Cuantitativo}

        \noindent Como podemos ver en la \autoref{table:ModelEncoder_images_results} y la \autoref{fig:boxplot_ModeloEncoder_NME}, la distribución de los errores de reconstrucción y NME es prácticamente idéntica a la anterior, con la única salvedad de que se ha reducido el número de outliers en una unidad. Las tablas con las predicciones por imagen pueden consultarse en el \autoref{ap:apendiceB}.

        \begin{table}[!ht]
            \centering
            \caption{Métricas obtenidas de la distribución de los errores cometidos en el conjunto de entrenamiento con el modelo de ajuste fino del encoder.}
            \begin{tabular}{|l|l|l|l|l|l|l|}
            \hline
            \cellcolor{gray!25}\textbf{Error} & \cellcolor{gray!25}\textbf{P25} & \cellcolor{gray!25}\textbf{P50} & \cellcolor{gray!25}\textbf{P75} & \cellcolor{gray!25}\textbf{RI} & \cellcolor{gray!25}\textbf{Media} & \cellcolor{gray!25}\textbf{Valores atípicos}\\ \hline
                NME & 1.804 & 2.163 & 2.912 & 1.107 & 2.978 & 11 \\ \hline
                Reconstrucción & 30.128 & 37.225 & 44.1135 & 13.98 & 38.006 & 3 \\ \hline
            \end{tabular}
            \label{table:ModelEncoder_images_results}
        \end{table}

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\textwidth]{img/boxplot_encoder.png}
            \caption{Diagramas de cajas de los resultados de NME y error de validación en el conjunto de las imágenes de entrenamiento del modelo de ajuste fino del encoder. Se han omitido los Outliers para una correcta visualización.}
            \label{fig:boxplot_ModeloEncoder_NME}
        \end{figure}

        \begin{table}[!ht]
            \centering
            \caption{Tabla comparativa entre los dos modelos explorados por ahora. Medimos el NME medio a nivel de landmark. En verde se resalta el mejor valor de marcado para cada landmark.}
            \begin{tabular}{|l|l|l|}
            \hline
            \cellcolor{gray!25}\textbf{Landmark} & \cellcolor{gray!25}\textbf{M. Base} & \cellcolor{gray!25}\textbf{M. Encoder} \\ \hline
                Menton & 4.176 & \cellcolor{green!25}3.997 \\ \hline
                Gnathion & 2.629 & \cellcolor{green!25}2.606 \\ \hline
                Pogonion & \cellcolor{green!25}2.509 & 2.546 \\ \hline
                Prosthion & 1.178 & \cellcolor{green!25}1.048 \\ \hline
                Labiale Superius & \cellcolor{green!25}1.965 & 2.04 \\ \hline
                Subnasale & 2.056 & \cellcolor{green!25}1.992 \\ \hline
                Nasion & 2.211 & \cellcolor{green!25}2.099 \\ \hline
                Glabella & \cellcolor{green!25}2.562 & 2.736 \\ \hline
                \cellcolor{yellow!50}Vertex &\cellcolor{yellow!50} 5.904 & \cellcolor{yellow!50}5.89 \\ \hline
                Left Gonion & \cellcolor{green!25}5.652 & 5.789 \\ \hline
                Right Gonion & 5.655 & \cellcolor{green!25}5.301 \\ \hline
                Left Zygion & 5.351 & \cellcolor{green!25}5.288 \\ \hline
                Right Zygion & 6.504 & \cellcolor{green!25}6.291 \\ \hline
                Left Alare & 1.972 & \cellcolor{green!25}1.959 \\ \hline
                Right Alare & 2.27 & \cellcolor{green!25}2.232 \\ \hline
                Left Endocanthion & \cellcolor{green!25}1.675 & 1.71 \\ \hline
                Right Endocanthion & 1.731 & \cellcolor{green!25}1.647 \\ \hline
                Left Exocanthion & \cellcolor{green!25}1.7 & 1.733 \\ \hline
                Right Exocanthion & \cellcolor{green!25}1.651 & 1.746 \\ \hline
                Left Tragion & 4.15 & \cellcolor{green!25}3.633 \\ \hline
                Right Tragion & 5.639 & \cellcolor{green!25}5.252 \\ \hline
                Labiale inferius & 1.734 & \cellcolor{green!25}1.688 \\ \hline
                Trichion & \cellcolor{green!25}4.445 & 4.762 \\ \hline
                Supramentale & \cellcolor{green!25}1.933 & 1.991 \\ \hline
                Left Frontotemporale & \cellcolor{green!25}2.37 & 2.452 \\ \hline
                Right Frontotemporale & \cellcolor{green!25}2.97 & 3.282 \\ \hline
                Left Frontozygomaticus & 1.566 & \cellcolor{green!25}1.499 \\ \hline
                Right Frontozygomaticus & \cellcolor{green!25}2.483 & 2.516 \\ \hline
                Left Midsurpaorbital & \cellcolor{green!25}1.168 & 1.178 \\ \hline
                Right Midsupraorbital & 1.429 & \cellcolor{green!25}1.348 \\ \hline
            \end{tabular}
            \label{table:Encode_landmarkresume}
        \end{table}

    \noindent Como podemos observar en la \autoref{table:Encode_landmarkresume}, los valores son muy similares a los del modelo base, pese a que este ha entrenado durante más épocas. El \textit{Prosthion} sigue siendo el landmark mejor marcado, con un valor del error NME ligeramente inferior al del modelo base. Se puede apreciar una mejora en los landmarks peor marcados, todos ellos asociados a los laterales del rostro y con mayor presencia en imágenes de perfil. Esto confirma la relación entre la mejora de la reconstrucción en imágenes de perfil y el NME asociado a los landmarks de las mismas.
    

    \medskip

    \noindent Finalmente podemos observar las curvas de aprendizaje para este modelo en la \autoref{fig:curvas_encoder}. A diferencia del modelo anterior, comienza a bajar la capacidad de generalización a partir de la época $20$-$30$ aproximadamente. El error de entrenamiento comienza a descender muy lentamente con cada época mientras que el de validación se mantiene constante.

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.9\textwidth]{img/curvas_encoder.png}
        \caption{Curvas de aprendizaje en cada partición del modelo de finetuning del encoder.}
        \label{fig:curvas_encoder}
    \end{figure}

    \medskip

    \noindent Viendo estos resultados, no parece muy prometedora esta línea de investigación. Bien es cierto que hemos mejorado el error de los dos landmarks peor marcados, pero el coste computacional que ha supuesto no compensa para la leve mejora global del modelo. Es por esto por lo que vamos a cambiar el enfoque sobre el \textit{fine-tuning} en el siguiente modelo. 
    
    \medskip
    
    \noindent No obstante, consideramos que un reentrenamiento de los pesos del \textit{AAE} en la fase de aprendizaje no supervisado con imágenes complicadas en escalas de grises y de perfil habría sido una buena alternativa, pero lamentablemente no disponemos de los datos necesarios para realizar este eperimento.
    
    
    \subsubsection*{Análisis Cualitativo}
    
    \noindent Podemos ver el comportamiento del modelo en algunas imágenes de validación de diversas particiones de \textit{cross-validation} en la \autoref{fig:Ejemplo_encoder}. Se puede ver una leve mejora del error de reconstrucción con respecto a las imágenes del modelo anterior. No obstante a nivel visual apenas se notan diferencias significativas tanto en el marcado de los landmarks como en la reconstrucción de las imágenes.

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.9\textwidth]{img/image_encoder.png}
        \caption{Imágenes pertenecientes a distintos conjuntos de validación para el \textbf{modelo de ajuste fino del encoder}.}
        \label{fig:Ejemplo_encoder}
    \end{figure}

    \subsection{Modelo con reentrenamiento del decoder}
        \noindent Del mismo modo en que se experimentó con el \textit{encoder} se hizo con el \textit{decoder}. En este enfoque asumimos que el framework sabe codificar correctamente las imágenes pero que no realiza lo suficientemente bien la reconstrucción de las mismas debido a la alta variabilidad del dataset. Esta etapa es completamente análoga a la del modelo anterior (incluyendo el mismo tipo de optimizadores, parámetros y regiones que permanecen congeladas de la red), pero actualizando los pesos del \textit{decoder} y las ITLs mientras se congelan los pesos del \textit{encoder}.

        \subsubsection*{Análisis Cuantitativo}

        \noindent Podemos observar los resultados obtenidos en la \autoref{fig:boxplot_ModeloDecoder_NME} y la \autoref{table:ModelDecoder_images_results}. Los resultados son muy similares a los anteriores modelos e incluso empeoran ligeramente en el marcado de landmarks, aumentando el número de outliers en la distribución de los errores NME a 13. Las tablas por partición se pueden consultar en \autoref{ap:apendiceC}.

    \begin{table}[!ht]
            \centering
            \caption{Métricas obtenidas de la distribución de los errores cometidos en el conjunto de entrenamiento con el modelo de ajuste fino del decoder.}
            \begin{tabular}{|l|l|l|l|l|l|l|}
            \hline
            \cellcolor{gray!25}\textbf{Error} & \cellcolor{gray!25}\textbf{P25} & \cellcolor{gray!25}\textbf{P50} & \cellcolor{gray!25}\textbf{P75} & \cellcolor{gray!25}\textbf{RI} & \cellcolor{gray!25}\textbf{Media} & \cellcolor{gray!25}\textbf{Valores atípicos}\\ \hline
                NME & 1.868 & 2.249 & 2.925 & 1.057 & 3.039 & 13 \\ \hline
                Reconstrucción & 30.02 & 37.104 & 44.382 & 14.357 & 38.043 & 3 \\ \hline
            \end{tabular}
            \label{table:ModelDecoder_images_results}
        \end{table}

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\textwidth]{img/boxplot_decoder.png}
            \caption{Diagramas de cajas de los resultados de NME y error de validación en el conjunto de las imágenes de entrenamiento del modelo de ajuste fino del decoder. Se han omitido los Outliers para una correcta visualización.}
            \label{fig:boxplot_ModeloDecoder_NME}
        \end{figure}

        \begin{table}[!ht]
            \centering
            \caption{Tabla comparativa entre los tres modelos probados. En este caso no se aprecia ninguna mejora considerable con respecto a las introducidas por el modelo de ajuste fino del encoder.}
            \begin{tabular}{|l|l|l|l|}
            \hline
                \cellcolor{gray!25}\textbf{Landmark} & \cellcolor{gray!25}\textbf{M. Base} & \cellcolor{gray!25}\textbf{M. Encoder} & \cellcolor{gray!25}\textbf{M. Decoder} \\ \hline
                Menton & 4.176 & 3.997 & \cellcolor{green!25}3.991 \\ \hline
                Gnathion & 2.629 &  \cellcolor{green!25}2.606 & 2.634 \\ \hline
                Pogonion &  \cellcolor{green!25}2.509 & 2.546 & 2.585 \\ \hline
                Prosthion & 1.178 &  \cellcolor{green!25}1.048 & 0.97 \\ \hline
                Labiale Superius &  \cellcolor{green!25}1.965 & 2.04 & 2.059 \\ \hline
                Subnasale & 2.056 &  \cellcolor{green!25}1.992 & 2.089 \\ \hline
                Nasion & 2.211 &  \cellcolor{green!25}2.099 & 2.275 \\ \hline
                Glabella & 2.562 &  \cellcolor{green!25}2.736 & 2.912 \\ \hline
                \cellcolor{yellow!50}Vertex &\cellcolor{yellow!50} 5.904 &  \cellcolor{yellow!50}5.89 & \cellcolor{yellow!50}6.137 \\ \hline
                Left Gonion & \cellcolor{green!25}5.652 & 5.789 & 5.665 \\ \hline
                Right Gonion & 5.655 & \cellcolor{green!25}5.301 & 5.468 \\ \hline
                Left Zygion & 5.351 & \cellcolor{green!25}5.288 & 5.29 \\ \hline
                Right Zygion & 6.504 & 6.291 & \cellcolor{green!25}6.14 \\ \hline
                Left Alare & 1.972 & \cellcolor{green!25}1.959 & 2.051 \\ \hline
                Right Alare & 2.27 & \cellcolor{green!25}2.232 & 2.295 \\ \hline
                Left Endocanthion & \cellcolor{green!25}1.675 & 1.71 & 1.827 \\ \hline
                Right Endocanthion & 1.731 & 1.647 & \cellcolor{green!25}1.61 \\ \hline
                Left Exocanthion & \cellcolor{green!25}1.7 & 1.733 & 1.809 \\ \hline
                Right Exocanthion & \cellcolor{green!25}1.651 & 1.746 & 1.734 \\ \hline
                Left Tragion & 4.15 & \cellcolor{green!25}3.633 & 4.163 \\ \hline
                Right Tragion & 5.639 & 5.252 & \cellcolor{green!25}4.791 \\ \hline
                Labiale inferius & 1.734 & 1.688 & \cellcolor{green!25}1.627 \\ \hline
                Trichion & \cellcolor{green!25}4.445 & 4.762 & 5.196 \\ \hline
                Supramentale & 1.933 & 1.991 & \cellcolor{green!25}1.839 \\ \hline
                Left Frontotemporale & \cellcolor{green!25}2.37 & 2.452 & 2.474 \\ \hline
                Right Frontotemporale & \cellcolor{green!25}2.97 & 3.282 & 3.009 \\ \hline
                Left Frontozygomaticus & 1.566 & \cellcolor{green!25}1.499 & 1.525 \\ \hline
                Right Frontozygomaticus & 2.483 & 2.516 & \cellcolor{green!25}2.395 \\ \hline
                Left Midsurpaorbital & \cellcolor{green!25}1.168 & 1.178 & 1.191 \\ \hline
                Right Midsupraorbital & 1.429 & \cellcolor{green!25}1.348 & 1.48 \\ \hline
            \end{tabular}
            \label{table:Decoder_landmarksresume}
        \end{table}
        \medskip 

        \noindent De nuevo los errores por landmark que se muestran en la tabla \autoref{table:Decoder_landmarksresume} es muy similar a la de los dos modelos anteriores, por lo que no hemos logrado una gran mejora con este modelo tampoco.

        \medskip

        \noindent Las curvas de aprendizaje se pueden ver en la \autoref{fig:curvas_decoder}, y vuelven a mostrar un ligero overfitting a partir de la época $30$-$40$, en la mayoría de casos. 
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\textwidth]{img/curvas_decoder.png}
            \caption{Curvas de aprendizaje en cada partición del modelo de finetuning del encoder.}
            \label{fig:curvas_decoder}
        \end{figure}

        \medskip

        \noindent Tras los resultados obtenidos se ha optado por abandonar también esta línea de investigación pues se obtienen resultados muy similares al modelo base y se debe entrenar por mucho más tiempo ($100$ épocas frente a $40$ del modelo base). 
        
        \subsubsection*{Análisis Cualitativo}
        
        Los resultados del modelo sobre algunas imágenes pueden verse en \autoref{fig:Ejemplo_decoder}. Las conclusiones a extraer son las mismas que en el modelo anterior: Apenas mejora el error de reconstrucción (incluso empeora en el caso de la imagen a color), y no se aprecian diferencias significativas en el marcado de landmarks con respecto a los modelos anteriores.

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\textwidth]{img/image_decoder.png}
            \caption{Imágenes pertenecientes a distintos conjuntos de validación para el \textbf{modelo de ajuste fino del decoder}.}
            \label{fig:Ejemplo_decoder}
        \end{figure}

    \subsection{Modelo con Data Augmentation}
        \noindent Vamos a optar a continuación por un nuevo enfoque. En lugar de mejorar el error de reconstrucción, vamos a centrarnos en mejorar el marcado de landmarks en las imágenes reconstruidas que genera el modelo preentrenado. Como pudimos ver en el modelo base, se consiguieron buenos resultados del \textit{NME} pese a haber entrenado sin realizar técnicas de \textit{data augmentation}. Es por ello que en este segundo experimento vamos a realizar los siguientes cambios en el entrenamiento: 
        
        \begin{itemize}
            \item Se realizará un entrenamiento del modelo similar al del modelo base durante $40$ épocas, ya que vimos que el error convergía tras la época 40.
            \item Tras esto, se entrenará el modelo con el dataset al cual se le aplicarán técnicas de data augmentation, para aumentar el número de datos de entrenamiento e incrementar aún más la variabilidad de los mismos, de manera que cada imagen del dataset sufrirá de manera aleatoria una traslación ($\pm 4\%$), un reescalado ($\pm 5\%$), una rotación ($\pm 30$ grados) y oclusiones parciales. El entrenamiento en este nuevo dataset será durante $40$ épocas tras acabar las de la etapa anterior.
            \item En total serán \textbf{$80$ épocas} de entrenamiento sumando las dos etapas anteriores.
        \end{itemize}

        \medskip

        \noindent De nuevo, el resto de parámetros continuarán con los valores por defecto de la etapa anterior, pues los resultados obtenidos han sido buenos hasta el momento y nada sugiere tener que cambiarlos.

        \medskip

        \noindent Los resultados por imagen de \textbf{cross-validation}, pueden consultarse en la \autoref{table:ModelDaug_images_results} y la \autoref{fig:boxplot_ModeloDaug_NME}. En este caso, se aprecia una considerable mejora en el error del NME, compactando mucho la distribución, como vemos, el rango intercuartílico (RI) es de $0,371$. Los valores atípicos suben a los $13$, lo que suponen un $10\%$ de los datos aproximadamente. No obstante, podemos concluir que le modelo es robusto, a pesar de la gran variabilidad de los datos empleados. Por otro lado, la distribución del error de reconstrucción es idéntica a la del modelo base, ya que no entrenamos los pesos del encoder ni decoder.

        \begin{table}[!ht]
            \centering
            \caption{Métricas obtenidas de la distribución de los errores cometidos en el conjunto de entrenamiento con el modelo de data augmentation.}
            \begin{tabular}{|l|l|l|l|l|l|l|}
            \hline
            \cellcolor{gray!25}\textbf{Error} & \cellcolor{gray!25}\textbf{P25} & \cellcolor{gray!25}\textbf{P50} & \cellcolor{gray!25}\textbf{P75} & \cellcolor{gray!25}\textbf{RI} & \cellcolor{gray!25}\textbf{Media} & \cellcolor{gray!25}\textbf{Valores atípicos}\\ \hline
                NME & 1.1 & 1.253 & 1.472 & 0.371 & 1.706 & 13 \\ \hline
                Reconstrucción & 30.024 & 37.104 & 44.382 & 14.357 & 38.043 & 3 \\ \hline
            \end{tabular}
            \label{table:ModelDaug_images_results}
        \end{table}

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\textwidth]{img/boxplot_daug.png}
            \caption{Diagramas de cajas de los resultados de NME y error de validación en el conjunto de las imágenes de entrenamiento del modelo de data augmentation. Se han omitido los Outliers para una correcta visualización.}
            \label{fig:boxplot_ModeloDaug_NME}
        \end{figure}

        \begin{table}[!ht]
            \centering
            \caption{Tabla comparativa entre todos los modelos probados.}
            \begin{tabular}{|l|l|l|l|l|}
            \hline
                \cellcolor{gray!25}\textbf{Landmark} & \cellcolor{gray!25}\textbf{M. Base} & \cellcolor{gray!25}\textbf{M. Encoder} & \cellcolor{gray!25}\textbf{M. Decoder} & \cellcolor{gray!25}\textbf{M. Data Augmentation} \\ \hline
                Menton & 4.176 & 3.997 & 3.991 & \cellcolor{green!25}3.006 \\ \hline
                Gnathion & 2.629 & 2.606 & 2.634 & \cellcolor{green!25}1.497 \\ \hline
                Pogonion & 2.509 & 2.546 & 2.585 & \cellcolor{green!25}1.359 \\ \hline
                Prosthion & 1.178 & 1.048 & 0.97 & \cellcolor{green!25}0.815 \\ \hline
                Labiale Superius & 1.965 & 2.04 & 2.059 & \cellcolor{green!25}1.531 \\ \hline
                Subnasale & 2.056 & 1.992 & 2.089 & \cellcolor{green!25}1.272 \\ \hline
                Nasion & 2.211 & 2.099 & 2.275 & \cellcolor{green!25}1.241 \\ \hline
                Glabella & 2.562 & 2.736 & 2.912 & \cellcolor{green!25}1.545 \\ \hline
                \cellcolor{yellow!50}Vertex & \cellcolor{yellow!50}5.904 & \cellcolor{yellow!50}5.89 & \cellcolor{yellow!50}6.137 & \cellcolor{yellow!50}3.99 \\ \hline
                Left Gonion & 5.652 & 5.789 & 5.665 & \cellcolor{green!50}1.846 \\ \hline
                Right Gonion & 5.655 & 5.301 & 5.468 & \cellcolor{green!50}1.673 \\ \hline
                Left Zygion & 5.351 & 5.288 & 5.29 & \cellcolor{green!50}3.322 \\ \hline
                Right Zygion & 6.504 & 6.291 & 6.14 & \cellcolor{green!50}4.441 \\ \hline
                Left Alare & 1.972 & 1.959 & 2.051 & \cellcolor{green!25}1.585 \\ \hline
                Right Alare & 2.27 & 2.232 & 2.295 & \cellcolor{green!25}1.362 \\ \hline
                Left Endocanthion & 1.675 & 1.71 & 1.827 & \cellcolor{green!25}1.227 \\ \hline
                Right Endocanthion & 1.731 & 1.647 & 1.61 & \cellcolor{green!25}1.129 \\ \hline
                Left Exocanthion & 1.7 & 1.733 & 1.809 & \cellcolor{green!25}1.186 \\ \hline
                Right Exocanthion & 1.651 & 1.746 & 1.734 & \cellcolor{green!25}1.12 \\ \hline
                Left Tragion & 4.15 & 3.633 & 4.163 & \cellcolor{green!25}2.066 \\ \hline
                Right Tragion & 5.639 & 5.252 & 4.791 & \cellcolor{green!25}2.234 \\ \hline
                Labiale inferius & 1.734 & 1.688 & 1.627 & \cellcolor{green!25}1.032 \\ \hline
                Trichion & 4.445 & 4.762 & 5.196 & \cellcolor{green!25}3.37 \\ \hline
                Supramentale & 1.933 & 1.991 & 1.839 & \cellcolor{green!25}1.109 \\ \hline
                Left Frontotemporale & 2.37 & 2.452 & 2.474 & \cellcolor{green!25}1.386 \\ \hline
                Right Frontotemporale & 2.97 & 3.282 & 3.009 & \cellcolor{green!25}1.826 \\ \hline
                Left Frontozygomaticus & 1.566 & 1.499 & 1.525 & \cellcolor{green!25}1.036 \\ \hline
                Right Frontozygomaticus & 2.483 & 2.516 & 2.395 & \cellcolor{green!25}1.057 \\ \hline
                Left Midsurpaorbital & 1.168 & 1.178 & 1.191 &\cellcolor{green!25} 0.854 \\ \hline
                Right Midsupraorbital & 1.429 & 1.348 & 1.48 & \cellcolor{green!25}0.945 \\ \hline
            \end{tabular}
            \label{table:Daugmentation_landmarksresume}
        \end{table}

        \medskip

        \noindent Como podemos ver en la \autoref{table:Daugmentation_landmarksresume}, los errores se han reducido en todos los landmarks. Además, la red ha conseguido aprender a marcar con un alto grado de precisión los puntos que se ven en el perfil, siendo esta alternativa menos costosa que la de los dos modelos anteriores tanto en número de épocas como en parámetros que se actualizan de la red (en este nuevo modelo sólo se actualizan los pesos de las ITLs). El punto mejor marcado sería el \textit{prosthion} de nuevo, y el peor marcado el \textit{right zygion}. Sin embargo, la diferencia en términos de error entre ambos es mucho menor que en el caso del modelo base. 

        
        \subsubsection*{Análisis Cualitativo}
        \noindent Finalmente las curvas de aprendizaje obtenidas se pueden observar en \autoref{fig:curvas_daugmentation}. Como vemos presentan una gran capacidad de generalización, pues el error de entrenamiento y validación decrecen de forma continua y muy próxima. Algunas imágenes de ejemplo podemos verlas en la \autoref{fig:Ejemplo_daug}.

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\textwidth]{img/curvas_daugmentation.png}
            \caption{Curvas de aprendizaje en cada partición del modelo de data augmentation.}
            \label{fig:curvas_daugmentation}
        \end{figure}

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\textwidth]{img/image_daug.png}
            \caption{Imágenes pertenecientes a distintos conjuntos de validación para el \textbf{modelo data augmentation}.}
            \label{fig:Ejemplo_daug}
        \end{figure}

    \subsection{Elección de modelo y obtención de resultados}

        \noindent Una vez analizados los distintos modelos que se proponen para resolver el problema, vemos que la alternativa más prometedora resulta ser la última. Podemos ver una comparación de la distribución de los errores de NME y reconstrucción de todos los modelos en la \autoref{fig:boxplot_summary}, aquí podemos ver mejor la gran reducción del error NME lograda por el último modelo. Por otro lado, la distribución de los errores de reconstrucción apenas varía en los cuatro modelos probados. 
        

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\textwidth]{img/boxplot_sumarize.png}
            \caption{Diagramas de cajas de todos los modelos probados concatenados. Para una correcta visualización se eliminan los outliers del gráfico.}
            \label{fig:boxplot_summary}
        \end{figure}

        
        \subsubsection*{Análisis Cuantitativo}
        

        \noindent Una vez elegido el modelo, tomamos el conjunto de entrenamiento en su totalidad (sin realizar particiones) y vamos a usarlo para entrenar el modelo durante las mismas épocas que en la sección anterior y con los mismos parámetros y transformaciones. Posteriormente usaremos el conjunto de test de $32$ imágenes que separamos al principio de esta sección, y que hasta ahora no hemos visto para evitar \textit{data snooping}, para validar el modelo.


        \begin{table}[!ht]
            \centering
            \caption{Tabla comparativa entre los resultados del modelo de data augmentation en el conjunto de validación y test.}
            \begin{tabular}{|l|l|l|}
            \hline
                \cellcolor{gray!25}\textbf{Landmark} & \cellcolor{gray!25}\textbf{NME Test} & \cellcolor{gray!25}\textbf{NME Validación} \\ \hline
                \textbf{Menton} & \cellcolor{green!25}0.929 & 3.006 \\ \hline
                \textbf{Gnathion} & \cellcolor{green!25}1.13 & 1.497 \\ \hline
                \textbf{Pogonion} & \cellcolor{green!25}1.262 & 1.359 \\ \hline
                \textbf{Prosthion} & 1.24 & \cellcolor{green!25}0.815 \\ \hline
                \textbf{Labiale Superius} & \cellcolor{green!25}1.135 & 1.531 \\ \hline
                \textbf{Subnasale} & \cellcolor{green!25}0.863 & 1.272 \\ \hline
                \textbf{Nasion} & \cellcolor{green!25}0.871 & 1.241 \\ \hline
                \textbf{Glabella} & \cellcolor{green!25}1.161 & 1.545 \\ \hline
                \textbf{Vertex} & \cellcolor{green!25}2.558 & 3.99 \\ \hline
                \textbf{Left Gonion} & \cellcolor{green!25}1.406 & 1.846 \\ \hline
                \textbf{Right Gonion} & \cellcolor{green!25}1.242 & 1.673 \\ \hline
                \textbf{Left Zygion} & \cellcolor{green!25}1.585 & 3.322 \\ \hline
                \textbf{Right Zygion} & \cellcolor{green!25}1.607 & 4.441 \\ \hline
                \textbf{Left Alare} & \cellcolor{green!25}0.804 & 1.585 \\ \hline
                \textbf{Right Alare} & \cellcolor{green!25}0.835 & 1.362 \\ \hline
                \textbf{Left Endocanthion} & 1.377 & \cellcolor{green!25}1.227 \\ \hline
                \textbf{Right Endocanthion} & 1.656 & \cellcolor{green!25}1.129 \\ \hline
                \textbf{Left Exocanthion} & 1.772 & \cellcolor{green!25}1.186 \\ \hline
                \textbf{Right Exocanthion} & 2.132 & \cellcolor{green!25}1.12 \\ \hline
                \textbf{Left Tragion} & \cellcolor{green!25}1.47 & 2.066 \\ \hline
                \textbf{Right Tragion} & \cellcolor{green!25}1.557 & 2.234 \\ \hline
                \textbf{Labiale inferius} & 1.103 & \cellcolor{green!25}1.032 \\ \hline
                \textbf{Trichion} & \cellcolor{green!25}1.544 & 3.37 \\ \hline
                \textbf{Supramentale} & 1.241 & \cellcolor{green!25}1.109 \\ \hline
                \textbf{Left Frontotemporale} & \cellcolor{green!25}1.293 & 1.386 \\ \hline
                \textbf{Right Frontotemporale} & \cellcolor{green!25}1.17 & 1.826 \\ \hline
                \textbf{Left Frontozygomaticus} & 1.038 & \cellcolor{green!25}1.036 \\ \hline
                \textbf{Right Frontozygomaticus} & \cellcolor{green!25}1.031 & 1.057 \\ \hline
                \textbf{Left Midsurpaorbital} & 0.864 & \cellcolor{green!25}0.854 \\ \hline
                \textbf{Right Midsupraorbital} & 1.92 & \cellcolor{green!25}0.945 \\ \hline
            \end{tabular}
            \label{table:FinalModel_landmarks}
        \end{table}
        
        \medskip

        \noindent En primer lugar, como se puede apreciar en la \autoref{table:FinalModel_landmarks} obtenemos: 

        \begin{itemize}
            \item A primera vista los errores son similares en el conjunto de test que los obtenidos en validación, ligeramente mejores en algunos casos y peores en otros.  
            \item Realizamos un test de \textbf{Kolmogorov-Smirnov} para ver si ambas distribuciones de errores proceden de la misma distribución (que será nuestra hipótesis nula) o no con un $95\%$ de confianza. Para ello empleamos la función \textit{ks\_2samp
            } de la librería \textit{scipy} de python. Esta función nos permite realizar con facilidad el test descrito anteriormente obteniendo un $p-valor=0.594 > 0.05$, por lo que no tenemos evidencias para rechazar la hipótesis nula. Esto es algo deseable, pues los errores de test y validación  deberían seguir la misma distribución.

        \end{itemize}

        \medskip

        \noindent Por otro lado, las curvas de aprendizaje durante el entrenamiento del modelo se pueden observar en la \autoref{fig:curvas_FinalModel}. En ellas podemos observar una gran capacidad de generalización del modelo, pues en todo momento la curva de entrenamiento y la de validación descienden tras cada época y cuando convergen parecen hacerlo al mismo valor. 

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\textwidth]{img/curvas_FinalModel.png}
            \caption{Curvas de aprendizaje durante el entrenamiento del modelo de data augmentation validado en el conjunto de test.}
            \label{fig:curvas_FinalModel}
        \end{figure}
        
        \subsubsection*{Análisis Cualitativo}

        \noindent Podemos ver el rendimiento del modelo en algunas imágenes en \autoref{fig:Ejemplo_finalmodel}. Como podemo ver, la reconstrucción de las imágenes continua siendo adecuada y la precisión en el marcado de landmarks es muy elevada. Las imágenes más complicadas siguen siendo las de perfil, debido a la poca calidad de la reconstrucción del \textit{AAE} en este tipo de ejemplos. No obstante, los resultados en global son satisfactorios.

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.9\textwidth]{img/image_finalmodel.png}
            \caption{Rendimiento del modelo final elegido con data augmentation en imágenes del conjunto de test.}
            \label{fig:Ejemplo_finalmodel}
        \end{figure}

        \medskip

        \subsubsection*{Análisis de requisitos}

        \noindent Vamos a ver a continuación si el modelo elegido cumple con los requisitos deseables que nos habíamos impuesto al comienzo del trabajo: 
        \begin{itemize}
            \item Se trata de un modelo \textbf{robusto}, pues tiene un buen comportamiento en la mayoría de ejemplos de test empleados.
            \item Es capaz de operar con un \textbf{pequeño conjunto de datos}, ya que los resultados obtenidos son muy prometedores pese a haber sido entrenado con $131$ imágenes (recordemos que modelos para la predicción de landmarks faciales cuentan con miles de ejemplos de entrenamiento en los datasets).
            \item Proporciona una solución muy próxima a la \textbf{correcta}. Aunque falta la supervisión de un experto antropólogo forense para afirmarlo.
            \item Se trata de un proceso \textbf{eficaz} y \textbf{eficiente}, pues se obtienen buenos resultados en apenas unos segundos de ejecución.
        \end{itemize}

        \medskip

        \noindent Como podemos ver, en general el modelo cumple los requisitos que le habíamos impuesto, y podemos concluir que el modelo resuleve la tarea que nos habíamos propuesto con éxito. No obstante, insistimos en que el éxito real del modelo debe supervisarlo un experto forense, pues no sabemos si los errores cometidos en la identificación de landmarks son errores asumibles o si se requiere un mayor grado de precisión en el marcado.

    \section{Comparativa entre 3FabRec y HyperFace-ResNet101}
        \noindent Vamos a comparar ahora el modelo final obtenido a partir de \textbf{3FabRec} con otro que fue diseñado para el mismo propósito basado en \textbf{HyperFace-Resnet101}, otra red especializada en el marcado de landmarks, realizado por Guillermo Gómez y que supone el estado del arte en el campo.

        \subsection{Preprocesamiento y base de datos empleada}
            \noindent En lo que respecta al preprocesamiento de los datos, nuestro modelo basado en \textit{3FabRec} únicamente emplea como etapa de preprocesamiento la identificación de bounding boxes en el conjunto de datos inicial de cara a poder entrenar la red. Por otro lado, este estudio se limita a usar las $167$ imágenes proporcionadas.

            \medskip

            \noindent En cambio, el modelo basado en \textit{HF-ResNet} además de emplear también un detector de caras para los bounding boxes, contaba con un dataset auxiliar de modelos $3D$ de personas sobre las que estaban marcados $27$ de los $30$ landmarks que se predicen. Lo cual sirvió para aumentar considerablemente el conjunto de datos de entrenamiento realizando proyecciones $2D$ a partir de estos modelos.

        \subsection{Métricas empleadas}
            \noindent La métrica empleada en el trabajo dónde se presenta el modelo es el \textbf{RMSE}. Se trata de la raíz del error cuadrático medio y su expresión es la siguiente: 

            \begin{equation}
                RMSE(y,\widehat{y})= \sqrt{\frac{1}{N} \sum_{i=1}^{N} (y_i-\widehat{y}_i)^2}
            \end{equation}

            \noindent donde $y_i$ representa las coordenadas del landmark real y $\widehat{y_i}$ el predicho homólogo número $i$ del total de $N$ landmarks marcados. Esta métrica nos proporciona el error en píxeles cometido en la predicción de cada landmark.

            \medskip

            \noindent Para poder realizar una comparación entre ambos métodos, se extrae esta métrica del rendimiento del modelo final en el conjunto de test. 

        \subsection{Comparación de resultados}
            \noindent En primer lugar comparamos ambos modelos calculando la mediana del \textit{RMSE} producido en cada landmark, así como los percentiles $25$, $50$ y $75$. Podemos ver estos resultados en la \autoref{table:comparativa-global}.

            \begin{table}[!ht]
                \centering
                \caption{Tabla comparativa a nivel global entre los dos modelos. Como podemos observar, el modelo basado en \textbf{HyperFace-Resnet101} obtiene mejores resultados en global en todos los campos.}
                \begin{tabular}{|l|l|l|l|l|}
                \hline
                    \cellcolor{gray!25}\textbf{Modelo} & \cellcolor{gray!25}\textbf{RMSE} & \cellcolor{gray!25}\textbf{P25} & \cellcolor{gray!25}\textbf{P50} & \cellcolor{gray!25}\textbf{P75} \\ \hline
                    \textbf{Modelo HyperFace-Resnet101} & 3.4106 & \cellcolor{green!25}1.802 & 2.7569 & 4.2849 \\ \hline
                    \textbf{Modelo 3FabRec} & \cellcolor{green!25} 2.7666 & 1.9561 & \cellcolor{green!25}2.3812 & \cellcolor{green!25}3.1201 \\ \hline
                \end{tabular}
                \label{table:comparativa-global}
            \end{table}
            
            \medskip

            \noindent A nivel general, parece que el modelo basado en \textit{3FabRec} es mejor que el basado en \textit{HF-Resnet101}. Como podemos ver en los resultados de la \autoref{table:comparativa-global}, a nivel de distribución del error RMSE en las imágenes de test, el RI en el modelo de \textit{HF-Resnet101} es de $2.48$ píxeles, mientras que en el modelo de \textit{3FabRec} es de $1.164$ píxeles, lo que supone una mejora considerable. No obstante, vamos a realizar una comparativa a nivel de landmark entre los dos modelos. Podemos ver esta comparativa en la \autoref{table:comparativa-Landmarks}.

            \begin{table}[!ht]
                \centering
                \caption{Tabla comparativa a nivel de landmarks entre el modelo final basado en 3FabRec y el de HyperFace-REsNet101.}
                \begin{tabular}{|l|l|l|}
                \hline
                    \cellcolor{gray!25}\textbf{Landmark} & \cellcolor{gray!25}\textbf{RMSE 3FabRec} & \cellcolor{gray!25}\textbf{RMSE HyperFace-Resnet101} \\ \hline
                    \textbf{Menton} & \cellcolor{green!25}1.79 &  4.97 \\ \hline
                    \textbf{Gnathion} & \cellcolor{green!25}2.28 & 3.84 \\ \hline
                    \textbf{Pogonion} & \cellcolor{green!25}2.86 & 3.99 \\ \hline
                    \textbf{Prosthion} & \cellcolor{green!25}1.73 & 3.23 \\ \hline
                    \textbf{Labiale Superius} & \cellcolor{green!25}2.24 & 2.35 \\ \hline
                    \textbf{Subnasale} & \cellcolor{green!25}1.56 & 3.33 \\ \hline
                    \textbf{Nasion} & \cellcolor{green!25}1.51 & 3.12 \\ \hline
                    \textbf{Glabella} & \cellcolor{green!25}2.4 & 3.89 \\ \hline
                    \textbf{Vertex} & \cellcolor{green!25}4.3 & 8.83 \\ \hline
                    \textbf{Left Gonion} & \cellcolor{green!25}3.38 & 7.12 \\ \hline
                    \textbf{Right Gonion} & \cellcolor{green!25}2.3 & 6.11 \\ \hline
                    \textbf{Left Zygion} & \cellcolor{green!25}2.93 & 5.78 \\ \hline
                    \textbf{Right Zygion} & \cellcolor{green!25}3.23 & 6.94 \\ \hline
                    \textbf{Left Alare} & \cellcolor{green!25}1.17 & 3.50 \\ \hline
                    \textbf{Right Alare} & \cellcolor{green!25}1.42 & 2.84 \\ \hline
                    \textbf{Left Endocanthion} & 3.18 & \cellcolor{green!25}2.40 \\ \hline
                    \textbf{Right Endocanthion} & 4.56 & \cellcolor{green!25}2.32 \\ \hline
                    \textbf{Left Exocanthion} & 4.92 & \cellcolor{green!25}3.73 \\ \hline
                    \textbf{Right Exocanthion} & 6.98 & \cellcolor{green!25}3.62 \\ \hline
                    \textbf{Left Tragion} & \cellcolor{green!25}2.87 & 7.01 \\ \hline
                    \textbf{Right Tragion} & \cellcolor{green!25}2.65 & 6.41 \\ \hline
                    \textbf{Labiale inferius} & \cellcolor{green!25}2.02 & 3.11 \\ \hline
                    \textbf{Trichion} & \cellcolor{green!25}2.68 & 7.02 \\ \hline
                    \textbf{Supramentale} & \cellcolor{green!25}2.19 & 3.43 \\ \hline
                    \textbf{Left Frontotemporale} & \cellcolor{green!25}2.59 & 3.78 \\ \hline
                    \textbf{Right Frontotemporale} & \cellcolor{green!25}2.37 & 3.27 \\ \hline
                    \textbf{Left Frontozygomaticus} & \cellcolor{green!25}1.79 & 2.75 \\ \hline
                    \textbf{Right Frontozygomaticus} & \cellcolor{green!25}1.94 & 2.80 \\ \hline
                    \textbf{Left Midsurpaorbital} & \cellcolor{green!25}1.44 & 2.57 \\ \hline
                    \textbf{Right Midsupraorbital} & 5.46 & \cellcolor{green!25}2.24 \\ \hline
                \end{tabular}
                \label{table:comparativa-Landmarks}
            \end{table}

            \medskip
            
            \begin{itemize}
                \item En total $25$ landmarks se marcan con mayor precisión con el modelo basado en \textit{3FabRec}.
                \item En total $5$ landmarks se marcan mejor en con el modelo basado en \textit{HyperFace-ResNet101}.
            \end{itemize}

            \noindent Como podemos observar, en su mayoría los landmarks marcados por el modelo basado en \textit{3FabRec} son más precisos. Sin embargo el \textit{Endocathion} y \textit{Exocanthion} son mejor marcados por el modelo basado en \textit{HF-Resnet101}.
           
            \subsubsection*{Análisis Cualitativo}

            \noindent A continuación vamos a ver una comparativa en el rendimiento entre ambos modelos en el conjunto de text (véase la \autoref{fig:comparativa_estado_arte}). En esta imagen podemos ver cómo la distancia euclídea se reduce en ambos ejemplos. 
            
            \begin{itemize}
                \item En el primer caso, la imagen esta dentro del promedio del error cometido por el modelo de \textit{HF-ResNet101}. Como vemos la media de la distancia euclídea entre el punto real y predicho mejora levemente.
                \item En el segundo ejemplo, vemos una mejora más considerable, pues el error cometido en el marcado de los landmarks se reduce hasta $10$ veces en el modelo de \textit{3FabRec} frente al modelo de \textit{HF-ResNet101}. 
                \item Por otro lado, vemos cómo la visibilidad de algunos landmarks mejora en el caso del modelo de \textit{3FabRec}. Por ejemplo, en la segunda imagen el \textit{vertex} queda fuera en la imagen del modelo de \textit{HF-Resnet101}, mientras que en nuestro modelo se aprecia perfectamente.
            \end{itemize}

            \begin{figure}[h]
                \centering
                \includegraphics[width=0.9\textwidth]{img/compativa_cualitativa.png}
                \caption{A la izquierda podemos ver las imágenes obtenidas con el modelo basado en \textit{3FabRec}, a la derecha las extraidas del TFG de Guillermo Gómez utilizando \textit{HF-ResNet}.}
                \label{fig:comparativa_estado_arte}
            \end{figure}
            
        \subsection{Conclusiones extraídas}

            \noindent Hemos visto que en el problema del marcado de landmarks cefalométricos, la mayor dificultad reside en la obtención de datos etiquetados, es decir, con landmarks marcados por un experto. Para el modelo basado en \textit{HF-Resnet101} se logró ampliar notablemente el dataset gracias a las proyecciones $2D$ de un conjunto de $99$ modelos $3D$, y pese a estar sesgada la información de este dataset (en lo referente a condiciones de luminosidad, ángulo y expresión facial), lo cierto es que se logró ampliar notablemente el conjunto de entrenamiento. En cambio, no es frecuente que todos los laboratorios forenses dispongan de una base de datos de modelos $3D$ etiquetados con los que ampliar el conjunto de entrenamiento. Es ahí donde crece el interés por el modelo presentado en este trabajo, pues en nuestro caso nos hemos ceñido exclusivamente a un pequeño dataset de apenas $167$ imágenes, que es un volumen de datos más frecuente en este tipo de problemas.
            
            \medskip

            \noindent El rendimiento a nivel de landmark, es muy satisfactorio por parte del modelo presentado, pues mejor en global el error cometido por el modelo de \textit{HF-Resnet101} y en particular, la mayoría de landmakrs reducen su error. Por todo lo mencionado anteriormente, podemos considerar el modelo basado en \textit{3FabRec} como el estado del arte en el campo. 
            
            
            \medskip
            
            \noindent Sin embargo, considero que una buena alternativa para mejorar aún más los resultados sería conseguir un gran conjunto de datos de imágenes sin etiquetar con la misma variabilidad que la que teníamos presente en nuestro dataset y realizar un ajuste fino del \textit{AAE} con este dataset para aprender la distribución de imágenes que define este tipo de bases de datos. La obtención de este tipo de imágenes no es tan complicada como la de conseguir imágenes etiquetadas, la capacidad para \textit{few-shot} learning de $3FabRec$ considero probable que se mejoren estos resultados notablemente, en especial en las imágenes de perfil.
            

\endinput
%------------------------------------------------------------------------------------
% FIN DEL CAPÍTULO. 
%------------------------------------------------------------------------------------



