
\chapter{Fundamentos Teóricos y Métodos}

En esta sección se introducirán los conceptos teóricos más importantes en los que se sustenta el trabajo y sus resultados. Para ello se ha recurrido al conocimiento adquirido en asignaturas como \textbf{Visión por Computador} o \textbf{Aprendizaje Automático}, así como diversos artículos que se citarán dónde sea conveniente.

\section{Aprendizaje Automático}
    \noindent Actualmente, la \textbf{Inteligencia Artificial} (IA) es una rama de la informática de importancia creciente que pretende dotar a los ordenadores de una manera de razonar o solucionar problemas de forma inteligente. En este contexto, la IA ha explorado diversos métodos para conseguir este propósito, lo que dan lugar a un amplio árbol de investigación dónde podemos destacar el estudio de Metaheurísticas, la Ingeniería del Conocimiento y el \textbf{Aprendizaje Automático} (AA). Los métodos que empleamos en este trabajo pretenecen a la rama del AA, y por lo tanto es importante comenzar definiendo este concepto. Para ello, disponemos de diversas definiciones:

    \medskip
    
    \noindent La primera y más clásica nos la proporciona Arthur Samuel en $1959$, en la cual define el AA como \textbf{el área de conocimiento que da a los ordenadores la capacidad de aprender sin ser programados explícitamente}. Esta definición es poco precisa, pero nos proporciona una perspectiva global de lo que se pretende: dotar a los ordenadores de la capacidad de \entrecomillado{aprender} a resolver un determinad problema a partir de una base de datos de entrenamiento.
    
    \medskip
    
    \noindent Una definición más reciente de Tom Mitchell ($1998$) nos dice que: \textbf{Un programa de ordenador se dice que aprende de la experiencia E en una tarea T y alguna medida de rendimiento P, si su rendimiento en T, medido por P, mejora con la experiencia E}. Ésta nos permite identificar los elementos necesarios para poder resolver un problema mediante técnicas de AA.

    \medskip

    \noindent Así, los elementos necesarios serían una problema (T) que queremos resolver con ayuda de un ordenador, una experiencia (E) en esa tarea, que generalmente es una base de datos asociada, y una medida de rendimiento (P) que mide el rendimiento del algoritmo en la resolución del problema y que generalmente se asocia con una función objetivo que se pretende minimizar/maximizar.

    \medskip

    \noindent Tradicionalmente, los algoritmos de AA se dividen en dos conjuntos según la naturaleza de los datos con que se entrenan. De esta forma tenemos los siguientes conjuntos:

    \begin{itemize}
        \item Aprendizaje Supervisado.
        \item Aprendizaje no Supervisado.
    \end{itemize}

    \medskip 
    
    \noindent Además, en los últimos años han aparecido otras técnicas como el Aprendizaje por Refuezo que están siendo muy usadas, pero no vamos a profundizar en ellas pues no son necesarias para el trabajo que nos ocupa.

    \subsection{Aprendizaje Supervisado}
        \noindent Los algoritmos de AA que se emplean en este conjunto se caracterizan porque disponen de una base de datos \textbf{etiquetados} de manera que para cada dato $x$ conocemos su etiqueta asociada $y$, y el objetivo sería tratar de conocer la función $f$ que los relaciona, de manera que $f(x)=y$.

        \subsubsection{Regresión} \label{section::Regresion}
            \noindent En los problemas de regresión se pretende obtener la función $f$ que asocia correctamente a cada dato su etiqueta: 
            \begin{equation}
                f(x)=y \; \; con \; \; x\in \mathbb{R}^m \; \; y \in \mathbb{R}^n
            \end{equation}
            
            \noindent Generalmente, obtener la función $f$ de manera exacta es muy complicado, por lo que se pretende aproximar mediante una función $f'$, perteneciente generalmente a una familia de funciones parametrizadas que elegimos y que entrenaremos a partir de los datos etiquetados que se nos proporcionan. Volviendo a la definición de Tom Mitchell, en este tipo de problemas la asociación con los elementos presentes en la defnición sería:
            
            \begin{itemize}
                \item T= regresión (aproximar $f$)
                \item E= El conjunto de datos $X$ etiquetados que se proporcionan para entrenar el modelo $f'$.
                \item P= función de coste asociada (generalmente se emplea el error cuadrático medio) que nos mide lo \textit{bien} que nuestra función $f'$ aproxima a $f$.
            \end{itemize}
            
            \medskip 

            \noindent A modo de ejemplo, vamos a formalizar un ejemplo concreto en el cual intentamos predecir $f$ mediante un modelo lineal, la función $f'$ tendría la siguiente forma: 

            \begin{equation}
                f'(x)= w^T x \; \; x,w \in \mathbb{R}^m
            \end{equation}

            \noindent Dónde $w$ sería el conjunto de parámetros que se pretenden ajustar para aproximar lo mejor posible $f$. Además, disponemos de un conjunto de $N$ datos 
            
            \begin{equation}
                X= \lbrace x_1, x_2 , \ldots , x_N \rbrace \; \; x_i \in \mathbb{R}^m
            \end{equation}

            \noindent y de etiquetas 

            \begin{equation}
                Y= \lbrace y_1, y_2 , \ldots , y_N \rbrace \; \; y_i \in \mathbb{R}^n
            \end{equation}

            \noindent Además, pongamos por ejemplo que usamos como función de coste $J$ el error cuadrático medio,un error muy empleado en este tipo de aproximaciones:
            \begin{equation}
               J(\alpha)= \frac{1}{N} \sum_{i=1}^{N}(f'(x_i) - f(x_i))^2 = \frac{1}{N} \sum_{i=1}^{N}(y_i' - y_i)^2
            \end{equation}

            \noindent donde $y_i'$ es la etiqueta predicha por $f'$ para $x_i$.

            \medskip

            \noindent Con todos estos datos, nuestro objetivo sería encontrar el vector de pesos $w$ que minimice la función de coste $J$ y para ello utilizamos los datos de entrenamiento $X$.     
            
            
        \subsubsection{Clasificación}
            \noindent Por otro lado tenemos los problemas de \textbf{clasificación}, en los datos se encuentran agrupados en clases y se pretende clasificar cada dato de entrada en la clase correcta. Los casos más sencillos de este problema son los de \textbf{clasificación binaria}, en ellos se pretende predecir la petenencia o no a una determinada clase de un conjunto de datos que se codifica como $0$ y $1$. 

            \medskip

            \noindent En este tipo de problemas se pretende buscar una manera de definir lo mejor posible la frontera entre los diferentes tipos de datos que queremos separar. Algunos algoritmos de los más empleados en esta son los siguientes: 

             \begin{itemize}
                \item \textbf{K-Nearest Neighbours}(K-NN): En este algoritmo asociamos a cada dato la etiqueta del conjunto al que pertenece de acuerdo a los $K$ (con $K \in \mathbb{N}$) vecinos más cercanos.
                \item \textbf{Máquina de vector de soporte}(SVM): Se trata de buscar el hiperplano que tenga la mayor distancia (margen) con los pontos más cercanos a él de cada conjunto.
                \item \textbf{Redes Neuronales}, que explicaremos en detalle en futuras secciones y destacando el \textit{perceptrón multicapa} (MLP).
             \end{itemize}


        \subsubsection{Gradiente Descendente}
            \noindent Hasta ahora hemos hablado de qué es el AA y cómo se formalizan sus problemas para poder resolverlos. Sin embargo, no hemos hablado de ningún algoritmo que se use en la minimización de la función de coste. Es por ello que vamos a explicar a continuación el algoritmo que clásicamente se utiliza para esta tarea: el \textbf{Gradiente Descendente}. 
    
            \medskip
    
            \noindent El Gradiente descendente es un algoritmo clásico que persigue la idea intuitiva de que el gradiente de una función siempre \entrecomillado{apunta} hacia el máximo de esta, por lo que seguir la dirección contraria a este nos llevará al mínimo de la función. Más formalmente, si recuperamos la notación del apartado anterior tendríamos: 
    
            \medskip
            
            \noindent La función objetivo es: 
            \begin{equation}
                f(x)=y \; \; x \in \mathbb{R}^m \; \; y \in \mathbb{R}^n
            \end{equation}
    
            \noindent La función con la que vamos a intentar aproximar la función objetivo es:
            \begin{equation}
                f'(x,w)=w^T x=y \; \; x \in \mathbb{R}^m \; \; y \in \mathbb{R}^n \; \; w \in \mathbb{R}^d 
            \end{equation}
    
            \noindent La función de coste sería $J(w)$ que de alguna manera mide la distancia entre $f$ y $f'$ y que para poder aplicar el método debe ser derivable. Algunas funciones de coste usuales son: 
    
            \begin{itemize}
                \item La función \textbf{L2} (también conocida como error cuadrático medio): 
                \begin{equation}
                    J(w)=\frac{1}{N} \sum_{i=1}^N(f(x_i,w)-f'(x_i))^2
                \end{equation}
                \item La función \textbf{L1} (también conocida como error absoluto medio): 
                \begin{equation}
                    J(w)=\frac{1}{N} \sum_{i=1}^N \left|f(x_i,w)-f'(x_i)\right|
                \end{equation}
            \end{itemize}
    
            \medskip

            \noindent No obstante, la función de coste puede variar mucho de un problema a otro, y en ocasiones no tiene por qué ser una de las anteriores, puede ser combinación lineal de varias funciones distintas o bien una función única para el problema en cuestión.

            \medskip

            \noindent Una vez hemos formalizado el problema, el algoritmo \textbf{Gradiente Descendente} consiste en: 
    
            \begin{itemize}
                \item Se inicializa el vector de pesos $w$ de acuerdo a un criterio.
                \item En cada paso $i$ del entrenamiento, el vector de pesos del siguiente paso $i+1$ se calculan los nuevos pesos usados de acuerdo a la siguiente relación: 
                \begin{equation}
                    w_{i+1}=w_i-\eta \nabla J(w)
                \end{equation}
                \noindent Dónde $\eta$ es un factor conocido como \textbf{learning rate}(lr) que mide el \entrecomillado{tamaño} del paso que en cada iteración damos en búsqueda del mínimo. Y dónde $\nabla J(w)$ es el valor del gradiente de la función de coste para el vector de pesos $w$. 
            \end{itemize}
    
            \noindent Idealmente, con este método se encuentra un mínimo global de la función de coste en el caso en que esta sea convexa. En caso de no serlo podría caer en un mínimo local en su lugar. 
    
            \medskip
    
            \noindent Por otro lado, cabe destacar la importancia de una buena elección del \textbf{learning rate}. Si este es demasiado pequeño puede ocasionar una lenta convergencia al mínimo, y por lo tanto que se realice un gran número de iteraciones. Por otro lado, un valor excesivamente grande de este puede impedir la convergencia, pues los saltos serían tan grandes en la dirección del mínimo local o global que podría llegar a \entrecomillado{pasar por encima} de este siempre. Por lo tanto una técnica habitual aunque costosa de este algoritmo consiste en usar un learning rate adaptativo, que sea mayor en las primeras iteraciones y que vaya disminuyendo conforme se incremente el número de iteraciones (pues se entiende que se estará cerca del mínimo). Podemos ver reflejado esto en la \autoref{fig:learning_rate}.

            \begin{figure}[!h]
                \centering
                \includegraphics[width=0.9\textwidth]{img/lr-types.png}
                \caption{Pretendemos alcanzar el mínimo de la función representada en las imágenes de la primera fila. Podemos ver el impacto de la elección del learning rate, si es muy pequeño, harán falta muchas iteraciones hasta la convergencia (primera imagen), si tiene un tamaño adecuado, converge rápidamente a la solución (segunda imagen), si es grande, puede quedar \entrecomillado{atrapado} el algoritmo (tercera imagen), y si es demasiado grande podría incluso diverger. Imagen extraída de \cite{StanfordCourse}.}
                \label{fig:learning_rate}
            \end{figure}
            
    
        \subsubsection{Gradiente Descendente Estocástico}
    
            \noindent El algoritmo descrito anteriormente tiene el problema de ser costoso computacionalmete, debido a que en cada iteración se debe calcular la función de coste para todos los ejemplos del conjunto de entrenamiento $X$. Es por ello, que suele emplearse en su lugar una versión modificada y que sigue dando buenos resultados que consiste en actualizar los pesos en base a unos pocos ejemplos del conjunto de entrenamiento $X$ que se conoce como \entrecomillado{minibatch}.
        
    \subsection{Aprendizaje no Supervisado}
        \noindent Los algoritmos de Aprendizaje no Supervisado se caracterizan porque los datos que se proporcionan no están etiquetados, y no se busca una salida concreta, sino que se pretende analizar y extraer características de nuestro conjunto de datos. Así, por ejemplo, tareas que pueden resolverse con esta técnica pueden ser la agrupación de clientes de cierta compañía en distintas clases según sus características.
    
    \subsection{Aprendizaje Automático en este Trabajo}
        \noindent En nuestro problema, el framework del que disponemos resuelve un problema de aprendizaje supervisado y no supervisado, pues intenta predecir un conjunto de landmarks para una cierta imagen de entrada, un problema típico de aprendizaje supervisado, en este caso de regresión dónde pretendemos a partir de la imagen de entrada conocer la función que nos proporciona la salida correcta (la imagen con los landmarks marcados correctametne).

        \medskip

        \noindent Por otro lado, tiene una etapa de entrenamiento previa al problema de los landmarks en la cual mediante conjuntos de datos de imágenes sin etiquetar de rostros humanos, se pretende reconstruir imágenes preservando al máximo posible la estructura de la cara. Esto, como podemos ver, es un problema típico de aprendizaje no supervisado, porque no se busca obtener una etiqueta para cada imagen, sino analizar la estructura de los distintos elementos de los datos de entrada para ser capaces de reconstruirlos preservando su estructura. 
        
        \medskip
        
        \noindent Un ejemplo clásico de un problema similar es \textit{EigenFaces} \cite{savvides2004eigenphases}, utilizado en el reconocimiento de rostros. El objetivo era extraer los vectores propios de la matriz de covarianza del conjunto de rostros de la base de datos.

        \medskip

        \noindent En este trabajo, sin embargo, sólo nos centraremos en la resolución del problema de \textbf{regresión} en el cual trataremos de predecir un conjunto de landmarks \textbf{cefalométricos}, y por lo tanto distintos a los que la red sabe predecir, a partir de un  pequeño conjunto de imágenes etiquetadas pertenecientes a un conjunto de datos forense.

    \subsection{Visión por Computador}
        \noindent La \textbf{Visión por computador} es un área de conocimiento en el que se unen diversas disciplinas como la IA o el AA para un propósito común, que es el procesado de imágenes por medio de un ordenador con la finalidad de que la máquina pueda llegar a extraer información relativa a estas del mismo modo en que lo haría un ser humano \cite{rosenfeld1988computer}. 

        \medskip
        
        \noindent Problemas clásicos de la visión por computador son el reconocimiento de objetos o personas en imágenes, la segmentación o la clasificación. Así pues, podemos ver la relación directa que hay entre nuestro objetivo y esta disciplina, pues los frameworks que usaremos tendrán por objetivo extraer información de imágenes de rostros de personas para posteriormente tratar de identificar en ellos con el mayor grado de decisión posible una serie de landmarks cefalométricos que el sistema ha aprendido a base de unos ejemplos etiquetados (AA).

        \medskip

        \noindent Finalmente, en los últimos años esta rama ha experimentado un fuerte impulso en la comunidad científica debido al actual desarrollo del \textbf{Deep Learning} y su principal herramienta: las \textbf{redes convolucionales profundas}, que han permitido crear programas que obtienen un gran rendimiento en el tratamiento de imágenes.

    \subsection{Deep Learning}
        Irónicamente, algunas de las tareas más fáciles para los seres humanos como son el reconocimiento del habla o la identificación de objetos en imágenes han suponen un verdadero reto para un odernador, y no ha sido hasta los últimos años con el nacimiento del \textbf{Deep Learning} que se han empezado a obtener resultados satisfactorios en este campo.

        \medskip
        
        \noindent Por lo tanto, los algoritmos del Deep Learning se caracterizan por resolver estos problemas a partir de representaciones del mismo que se expresan en terminos de otras más simples. De esta manera, se pueden construir conceptos difíciles a partir de otros más sencillos. Este grafo puede ser profundo en ocasiones, por ello se le conoce como Deep Learning.

        \subsubsection{Redes Neuronales} \label{sub:redes_neuronales}
            La arquitectura básica de los modelos de Deep Learning viene descrita por las \textbf{redes neuronales}. Es por ello que vamos a profundizar un poco en esta estructura y partiendo de un ejemplo clásico como es el \textbf{Perceptrón multicapa}(MLP). Para esta sección vamos a seguir el capítulo 6 de \cite{Goodfellow-et-al-2016}.

            \medskip

            \noindent Si recordamos de secciones pasadas, las redes neuronales tienen por objetivo aproximar una función desconocida $f(x)=y$ que asocia a cada entrada $x$ una salida $y$ a partir de una función $f'(x;W)= \widehat{y}$ (dónde $\widehat{y}$ es la etiqueta predicha para la entrada $x$) que depende de unos parámetros $W$, de manera que el objetivo es aprender los valores de $W$ que mejor aproximan la función objetivo $f$.

            \medskip

            \noindent Los algoritmos empleados por las redes se denominan \textbf{feedforward} porque la información fluye desde la entrada $x$ a través de los cálculos intermedios que definen $f'$ hasta finalmente dar una salida $\widehat{y}$. Debido a la representación gráfica de estos algoritmos, se les conoce como \textbf{redes}, pues se representan como una composición de distintas funciones en cadena de manera que se van aplicando sucesivamente sobre la entrada $x$ hasta la salida. La \textbf{profundidad} de la red vendría dada por la cantidad de \textbf{capas ocultas} que esta tiene.

            \begin{figure}[!h]
                \centering
                \includegraphics[width=0.7\textwidth]{img/single_hidden_layer.png}
                \caption{Red neuronal con una capa oculta. Formalmente podría describirse como $f'(x)=(f_2(f_1(x)))$, donde $f_2$ hace referencia a la transformación de los datos de entrada a la capa oculta y $f_3$ de la capa oculta a la de salida.}
                \label{fig:red_neuronal_capa_oculta}
            \end{figure}
            \medskip

            \noindent Por otro lado, cada capa contiene un número determinado de \textbf{neuronas}, que se relacionan con las de la capa siguiente y las de la capa anterior mediante combinaciones lineales. Sin embargo, estas combinaciones lineales no son suficientes para que la red pueda aproximar funciones objetivo $f$ que sean no-lineales, para ello se emplean las llamadas \textbf{funciones de activación}. Se tratan de un conjunto de funciones no-lineales entre las que destacan las siguientes \cite{sharma2017activation}: 

            \begin{itemize}
                \item \textbf{Función Sigmoide}. Transforma los valores de entrada a un valor entre $0$ y $1$.
                \begin{equation}
                    Sigmoide(x)=\frac{1}{1+e^{-x}}
                \end{equation}
                \item \textbf{Función Tangente Hiperbólica}. Es similar a la función sigmoide, pero es simétrica respecto al origen. 
                \begin{equation}
                    tanh(x)=\frac{e^x - e^{-x}}{e^x + e^{-x}} 
                \end{equation}
                \item \textbf{Función ReLU}. Es una de las más usadas en redes neuronales por ser de las más eficientes, ya que permite que no se activen todas las neuronas a la vez. Aquellas cuya salida tras la combinación lineal con los pesos de la capa correspondiente sea 0 no serán activadas.
                \begin{equation}
                    ReLU(x)=\max(0,x)
                \end{equation}

                \noindent Existe el problema de que en algunos casos, el gradiente de la función sea $0$ debido a que los pesos no se actualicen durante el proceso que explicaremos más adelante de \textbf{backpropagation}.

                \item \textbf{Función Leaky ReLU}. Es una versión mejorada de la función anterior, ya que para los casos en los que la función anterior valía $0$, ahora se expresan como una componente lineal de la entrada $x$ muy pequeña, de esta manera se resuelve el problema de que el gradiente de la función sea $0$. 
                \begin{equation}
                    LeakyReLU(x)=\left\{ \begin{array}{lcc}
                        x &   si  & x \leq 0 \\
                        \\ ax &  si & 0 < x \\
                        \end{array}
                    \right.
                \end{equation}

                \noindent Dónde $a$ es un valor muy próximo a $0$.
                \item \textbf{Función SoftMax}. Es una combinación de múltiples funciones sigmoide. Mientras que la función Sigmoide se usa en problemas de clasificación binaria, la función SoftMax permite que se pueda realizar clasificación multiclase, ya que transforma un vector de entrada K-dimensional de valores reales en un vector K-dimensional de elementos entre $0$ y $1$, de manera que la componente más próxima a $1$ podría entenderse como la clase a la que pertenecería el elemento en un problema de clasificación multiclase.
                
                \begin{equation}
                    \sigma(x)_j= \frac{e^{z_j}}{\sum_{k=1}^{K}e^{z_k}} \; \; j=1, \ldots, K
                \end{equation}
            \end{itemize}

            \begin{figure}[!h]
                \centering
                \includegraphics[width=0.8\textwidth]{img/functions.png}
                \caption{Representación gráfica de las funciones de activación más empleadas. La imagen ha sido extraída de  \cite{sharma2017activation}.}
                \label{fig:GrafoComputacional}
            \end{figure}

            \noindent No existe una manera de elegir la mejor función para cada caso, pero de forma experimental se ha podido comprobar que la función ReLU en general da buenos resultados y si hubiera demasiadas neuronas muertas en la red podría cambiarse por la Leaky ReLU.

            \medskip

            \noindent De esta manera, en la capa $i$ de la red, la función $f_i$ suele tener la siguiente expresión: 

            \begin{equation}
                f_i(x)=\gamma(w_i^T x)
            \end{equation}

            \noindent Dónde $\gamma$ representa una función cualquiera del conjunto de funciones de activación que hemos descrito antes y $w_i$ el vector de pesos correspondiente a la capa $i$ de la red. De esta manera se consigue aproximar funciones no lineales.
        
        \subsubsection{Backpropagation}
            Todas las funciones de las capas intermedias de la red son derivables, por lo que se podría calcular de forma explícita su derivada en cada caso, lo que ocurre es que este proceso es costoso computacionalmete. En lugar de esto se aplica la técnica de \textbf{Backpropagation}.

            \medskip

            \noindent Para entender este proceso correctamente se debe introducir primero el concepto de \textbf{Grafo Computacional}, que no es otra cosa que representar una función mediante un grafo. Como por ejemplo \autoref{fig:GrafoComputacional}.

            \begin{figure}[!h]
                \centering
                \includegraphics[width=0.8\textwidth]{img/GrafoComputacional.png}
                \caption{Ejemplo de Grafo computacional junto con la salida para una entrada concreta $x=-2$,$y=5$, $z=-4$. La imagen ha sido extraída del curso \cite{StanfordCourse}.}
                \label{fig:GrafoComputacional}
            \end{figure}

            \medskip

            \noindent La idea del algoritmo de Backpropagation es ir calculando la derivada en cada nodo del grafo computacional mediante la aplicación de la regla de la cadena, de manera que si $f(x,y,z)=g(x,y)h(z)$ si quisiéramos calcular $\frac{\partial f}{\partial x}$ aplicando la regla de la cadena tendríamos que: 

            \begin{equation}
                \frac{\partial f}{\partial x}= \frac{\partial f}{\partial g} \frac{\partial g}{\partial x}
            \end{equation}

            
            \noindent Podemos entender mejor el proceso resolviendo el ejemplo de la \autoref{fig:GrafoComputacional}.

            \begin{figure}[!h]
                \centering
                \includegraphics[width=0.8\textwidth]{img/BP_ejemplo.png}
                \caption{Como podemos ver en la imagen superior, en primer lugar se renombra la salida de la operación $x+y$ por $q$, de manera que $f=qz$. Tras esto se empiezan a calcular las derivadas parciales correspondientes desde el final hacia la entrada, aplicando cuando sea necesario la regla de la cadena hasta obtener la derivada de cada nodo en la imagen de abajo. Las imágenes han sido extraídas de \cite{StanfordCourse}}
                \label{fig:Ejemplo BP}
            \end{figure}

            \noindent Como vemos en \autoref{fig:Ejemplo BP}, se produce un flujo desde la salida hacia la entrada en la que se van calculando los gradientes para cada parámetro. Una vez calculado el gradiente se procede a actualizar los pesos usando por ejemplo el algoritmo de \textbf{gradiente descendente}.
        
    \section{Redes Neuronales Convolucionales}
        \noindent Las \textbf{Redes Neuronales Convolucionales} (CNN, por el inglés \textit{Convolutional Neural Networks}) son la principal herramienta del Deep Learning y están inspiradas en las redes neuronales que hemos explicado anteriormente, con la salvedad de que ahora en vez de tener como entrada un vector de $\mathbb{R}^d$ ahora pueden entrar a la red volúmenes de datos en varias dimensiones, como por ejemplo imágenes que se representan como $\mathbb{R}^{w\times h\times c}$.
        
        \medskip

        \noindent La principal motivación para la creación de estas redes es el concepto de \textbf{conectividad local}, pues cuando se trabaja con entradas de grandes dimensiones como son las imágenes, resulta impráctico conectar cada neurona con todas las de la capa anterior (como suele ocurrir en las redes neuronales clásicas que hemos visto anteriormente). Es por ello que se decide conectar cada neurona con una región local del volumen de entrada. Por lo que vamos a emplear filtros que siempre van a tener la misma profundidad que el volumen de entrada y que vamos a ir desplazando a lo largo de $\mathbb{R}^{w\times h\times c}$. A continuación, presentamos la operación que realizaremos  este filtro y cuyo resultado alimenta la siguiente capa de la red.

        \medskip

        \noindent Se denominan \textbf{Convolucionales} debido a que la principal operación matemática que realiza en cada filtro es la convolución. La operación de convolución entre dos funciones de variable real viene descrita por la siguiente expresión:

        \begin{equation}
            f(t)=\int g(a) h(t-a) da = (g \ast h)(t)
        \end{equation}

        \noindent Dónde $t$ generalmente representa el paso del tiempo y $g$,$h$ son dos funciones de variable real.

        \medskip

        \noindent Sin embargo, la expresión anterior es la definición en el caso continuo, por lo que al trabajar en un ordenador, el tiempo y las funciones que empleamos deben ser discretizado, por ello consideraremos que $t$ toma solo valores enteros.De esta forma, definimos la \textbf{convolución discreta} en una dimensión como: 

        \begin{equation}
            f(t)= (g \ast h)(t) = \sum_{a=-\infty}^\infty g(a) h(t-a)
        \end{equation}

        \medskip

        \noindent Sin embargo, en nuestro caso aplicaremos las CNN a imágenes, que se discretizan como matrices $2$D, por lo que necesitamos extender la definición anterior al caso de dos dimensiones: 

        \begin{equation}
            f(n,m)= (g \ast h)(n,m) = \sum_{j=-\infty}^\infty \sum_{i=-\infty}^\infty g(i,j) h(n-i,m-j)
        \end{equation}

        \noindent La convolución tiene las siguientes propiedades: 

        \begin{itemize}
            \item \textbf{Conmutatividad}: 
            \begin{equation}
                f \ast g = g \ast f
            \end{equation}
            \item \textbf{Asociatividad}: 
            \begin{equation}
                f \ast (g \ast h) = (f \ast g) \ast h
            \end{equation}
            \item \textbf{Distributividad}: 
            \begin{equation}
                f \ast (g + h)= (f\ast g) + (f \ast h)
            \end{equation}
        \end{itemize}

        \noindent Con esto tenemos todos los ingredientes para empezar a describir la estructura de una CNN, que generalmente se compone de tres tipos de capas: \textbf{capas convolucionales}, \textbf{capas de Pooling} y \textbf{capas totalmente conectadas}.

        \subsection{Capa Convolucional}
            \noindent Se trata de la capa esencial de una CNN y consiste en desplazar un filtro por todo el volumen de entrada realizando la operación de convolución discreta en $2$D. Como dijimos antes, la profundidad del filtro que empleamos para esto coincide con la del volumen de entrada siempre. Vamos a discutir ahora qué volumen de salida genera cada capa convolucional que viene definido por tres parámetros: \textbf{depth}, \textbf{stride} y \textbf{padding}

            \begin{itemize}
                \item El parámetro \textbf{depth} se corresponde con el número de filtros que queremos aplicar al volumen de entrada, pues se pretende que cada uno de ellos aprenda algo distinto sobre este. Como cada filtro produce un volumen $m \times n$ de profundidad $1$ conocido como \textbf{mapa de activación}, este parámetro nos genera tantos mapas de activación como indique.
                \item El parámetro \textbf{stride} indica el paso con el que vamos desplazando cada filtro sobre el volumen de entrada, así un stride de $1$ indica que el filtro se desplaza de uno en uno por los píxeles, y un stride de $2$ indicaría que se desplaza de dos en dos. Cuanto mayor sea el valor de este parámemtro, menor será la dimensional del mapa de activación.
                \item El parámetro \textbf{padding} indica en cuantas filas y columnas se amplía el volumen de entrada antes de aplicar los filtros. Este parámetro se utiliza para controlar la dimensión de salida de los mapas de activación pues la convolución es una operación que reduce la dimensionalidad siempre, a menos que el filtro aplicado sea de $1 \times 1$.
            \end{itemize}

            \noindent Con todo esto en mente, la relación que determina el volumen de salida de una capa convolucional para un volumen de entrada de dimensión $W \times W \times d$ y empleando filtros de dimensión $F \times F \times d$ con un padding $P$, un stride $S$ sería: 

            \begin{equation}
                Output=(W-F + 2P)/S + 1
            \end{equation}

            \medskip
            
            \noindent Generalmente la salida de cada capa convolucional supone la entrada a una función de activación como las que se describieron en \autoref{sub:redes_neuronales}. Normalmente se utiliza la función ReLU, aunque también es común el uso de Leaky ReLU.

            \begin{figure}[!h]
                \centering
                \includegraphics[width=0.8\textwidth]{img/mapa_activacion.png}
                \caption{Ejemplo de cálculo de mapa de activación en una capa convolucional para un determinado volumen de entrada. El parámetro depth nos dice la cantidad de mapas de activación generamos para el volumen de entrada, o dicho de otro modo, el número de filtros que aplicamos. La imagen ha sido extraída de \cite{StanfordCourse}}
                \label{fig:mapa_activacion}
            \end{figure}

            \begin{figure}[!h]
                \centering
                \includegraphics[width=0.8\textwidth]{img/sucesion_conv_layer.png}
                \caption{Sucesión de varias capas convolucionales + ReLU que describen la estructura básica de una CNN. Cabe destacar como la produndidad de los filtros es siempre la misma que la del volumen de entrada, de acuerdo a lo que hemos dicho anteriormente. La imagen han sido extraída de \cite{StanfordCourse}}
                \label{fig:estructura_convnet}
            \end{figure}

        \subsection{Capa de Pooling}
            \noindent En las CNN es normal instertar de vez en cuando capas de Pooling. Esta capa reduce la dimensión del volumen de entrada y actúa independientemente del volumen de la profundidad que tenga el volumen de entrada. Generalmente se emplean filtros de dimensión $2 \times 2$ con un stride de $2$ que reduce a la mitad la dimensión del volumen de entrada y mantiene la profundad.

            \medskip

            \noindent El tipo de operación que se realiza con el filtro $2 \times 2$ ha sido objeto de estudio en los últimos años, y se han probado las siguientes funciones: 

            \begin{itemize}
                \item \textbf{Max Pooling}. Consiste en tomar el máximo de los cuatro elementos que ve el filtro del volumen de entrada. 
                \item \textbf{Average Pooling}. Consiste en realzar un promedio de los elementos que ve el filtro.
            \end{itemize}

            \noindent Generalmente, el \textbf{max pooling} parece tener un mayor rendimiento en la práctica, pero esto aún es objeto de estudio.

            \begin{figure}[!h]
                \centering
                \includegraphics[width=0.8\textwidth]{img/pooling.png}
                \caption{Ejemplo de capa de pooling usando la operación del máximo. La imagen han sido extraída de \cite{StanfordCourse}}
                \label{fig:pooling}
            \end{figure}
        
        \subsection{Capa Totalmente Conectada (Fully Conected)}
            \noindent Generalmente al final de las CNN suele encontrarse una capa totalmente conectada (FC) que tiene la misma estructura que una red neuronal clásica con una o dos capas ocultas generalmente, y por lo tanto está creada para un vector de entrada de una dimensión concreta.

            \medskip

            \noindent Como hemos visto en las capas anteriores, una CNN actua de manera independiente de las dimensiones del volumen de entrada a la red salvo en las capas totalmente conectadas, es por ello que estas capas son las que normalmente determinan el volumen de entrada a la red para que esta funcione correctamente.

        \subsection{Batch Normalization}
            \noindent Actualmente, es muy común en las CNN usar \textbf{Batch Normalization} que consiste en normalizar los datos de entrada a cada capa convolucional con la intención de hacer que las operaciones convolucionales sean independientes unas de otras, es decir, que la distribución de los datos de entrada a una capa no dependa de los parámetros aprendidos por la capa anterior. Además este procedimiento ayuda a prevenir el overfitting de la red, ayudando a la regularización.

        \subsection{Optimizador Adam}
            El proceso de aprendizaje en las CNN se realiza de manera similar al caso de las redes neuronales clásicas, mediante la técnica de Backpropagation. Sin embargo, el optimizador Gradiente Descendente que hemos presentado en secciones anteriores tiene un único learning rate para todos los pesos de la red y se mantiene fijo durante todo el entrenamiento. Es por ello que en su lugar generalmente se usa el optimizador Adam que permite establecer un learning rate distinto para cada parámetro y que además es adaptativo.

        \subsection{Proceso de entrenamiento de una CNN}

            \noindent Una vez aclarados todos los conceptos previos estamos en condiciones de comprender el proceso clásico de entrenamiento de una CNN en el que nos basaremos para entrenar nuestro modelo en secciones posteriores: 

            \medskip

            \noindent Generalmente se entrenan las imágenes por conjuntos denominados \textit{mini-batches}, los cuales: 

            \begin{itemize}
                \item En una primera instancia se propagan hacia adelante por toda la red calculando las activaciones y errores de salida en lo que se conoce como \textbf{forward pass}.
                \item Vamos de la salida a la entrada calculado los gradientes de cada unidad con lo que se conoce como \textbf{backward pass} aplicando el algoritmo de backpropagation.
                \item Se actualizan los pesos en base al gradiente calculado en el paso anterior y según el optimizador que se esté empleando (Adam, gradiente descendente, etc...)
            \end{itemize}

        % \subsection{Evolución de las CNN}
        % Una vez hemos presentado las CNN y sus principales componentes vamos a dar un breve repaso por la historia y evolución de su arquitectura \cite{EvolutionCNN}.

        % \subsubsection{LeNet-5}
        %     \noindent La primera arquitectura conocida que forma parte de las CNN fue desarrollada por LeCun \cite{lecun1998gradient} para el reconocimiento de dígitos manuscritos, a dicha red la llamaron \textbf{LeNet-5} y fue la que sirvió de inspiración para el desarrollo de las posteriores redes.

        %     \begin{figure}[!h]
        %         \centering
        %         \includegraphics[width=0.8\textwidth]{img/LeNet.png}
        %         \caption{Arquitectura de la red LeNet. Como podemos observar tiene capas convolucionales, capas de average pooling y capas totalmente conectadas al final. Imagen extraída de \cite{lecun1998gradient}.}
        %         \label{fig:LeNet}
        %     \end{figure}

        %     \noindent Algunas de sus características eran: 

        %     \begin{itemize}
        %         \item Su arquitectura consiste en: 
        %         $$INPUT \rightarrow CONV \rightarrow AVG\;POOL \rightarrow CONV \rightarrow AVG \; POOL \rightarrow FC \rightarrow FC \rightarrow OUTPUT.$$
        %         \item Las imágenes de entrada eran de dimensión $32 \times 32$ y pertenecían a una base de datos MNIST de dígitos manuscritos.
        %         \item Tiene alrededor de unos $60000$ parámetros para entrenar.
        %         \item Las dimensiones de la imagen de entrada descienden en cada paso mientras que la profundidad del tensor aumenta hasta llegar a las capas FC.
        %     \end{itemize}

        % \subsubsection{AlexNet}
        %     \noindent Tras el éxito de LeNet comenzaron a desarrollarse nuevas arquitecturas basadas en CNN para problemas de reconocimiento de objetos en imágenes, aunque a pesar del buen rendimiento que tenían, resultaban muy costosas de entrenar para grandes volúmenes de datos o en imágenes de alta resolución. De esta manera surgión la red AlexNet \cite{krizhevsky2012imagenet}.

        %     \begin{figure}[!h]
        %         \centering
        %         \includegraphics[width=0.8\textwidth]{img/AlexNet.png}
        %         \caption{Arquitectura de la red AlexNet. Destaca que parece estar \entrecomillado{partida} en dos mitades, esto es porque por primera vez se usaron las GPUs para entrenar la red de manera que una GPU realizaba la parte superior de la arquitectura y otra la parte inferior. Imagen extraída de \cite{krizhevsky2012imagenet}.}
        %         \label{fig:AlexNet}
        %     \end{figure}

        %     \noindent Destacan las siguientes propiedades:

        %     \begin{itemize}
        %         \item Tiene un total de ocho capas sin contar las de pooling, cinco convolucionales y tres FC.
        %         \item Emplea ReLU como función de activación en lugar de tanh pues se demuestra que acelera el proceso de entrenamiento $\times 6$ y obtiene un error del $25\%$ en el dataset CIFAR-10.
        %         \item Se emplearon múltiples GPUs para el entrenamiento dividiendo las neuronas en dos conjuntos. Esto aceleró aún más el entrenamiento.
        %         \item Emplean la técnica de Max Pooling en lugar de la de Average Pooling de LeNet. 
        %         \item Tienen 60 millones de parámetros que aprender, de manera que para evitar el Overfitting se usó la técnica de \textbf{dropout}, que consistía en \entrecomillado{apagar} neuronas en cada iteración del entrenamiento de acuerdo a una probabilida del $50\%$. También se usó \textbf{data augmentation} que explicaremos en \autoref{sub:data_augmentation}.
        %         \item El modelo ganó la competición ImageNet en $2012$ con una diferencia en precisión del $11\%$ con respecto al algoritmo que quedó en segundo lugar.
        %     \end{itemize}
        
        % \subsubsection{GoogLeNet}
        %     \noindent La arquitectura de GoogLeNet está basada en un módulo que se denomina \textbf{Inception} creado con la intención de reducir el coste computacional de las CNNs. Para ello se propone que en vez de construir una red muy profundas se realicen simultáneamente múltiples convoluciones en una sola capa. Con este modelo aparecen los primeros filtros de convolución $1\times 1$ encargados de reducir la profundidad del volumen de entrada manteniendo las dimensiones.

        %     \medskip

        %     \noindent Con este modelo, se permitía realizar simultáneamente diversas operaciones sobre un volumen de entrada y finalmente se concatenaban los mapas de activación.

        %     \begin{figure}[!h]
        %         \centering
        %         \includegraphics[width=0.98\textwidth]{img/inception_module.png}
        %         \caption{Ejemplos de módulos Inception. Imágenes extraídas de \cite{StanfordCourse}.}
        %         \label{fig:inception}
        %     \end{figure}

        %     \noindent Con este nuevo módulo se desarrolló en 2015 la red GoogLeNet \cite{szegedy2015going}, que consiguió ganar el concurso ImageNet con empate técnico con la red \textbf{VGG-19}. En vez de usar capas FC usaron la técnica de \textbf{golbal average pooling} mediante la cual reducían la dimensión de los tensores con convoluciones $1 \times 1$.

        %     \begin{figure}[!h]
        %         \centering
        %         \includegraphics[width=0.98\textwidth]{img/GoogLeNet.png}
        %         \caption{Arquitectura de GoogLeNet usando módulos Inception.Imagen extraída de \cite{StanfordCourse}}
        %         \label{fig:GoogLeNet}
        %     \end{figure}

        %     \noindent Destacan las siguientes propiedades:

        %     \begin{itemize}
        %         \item Tiene un total de 27 capas de profundidad, por lo que aumenta notablemente la profundidad con respecto a sus antecesores.
        %         \item El uso de convoluciones $1\times 1$ con 128 filtros ayudan a reducir la dimensionalidad en lugar de usar capas FC.
        %         \item Contiene una capa FC con 1024 unidades y que alimenta una ReLU. 
        %         \item Tiene una capa lineal con Softmax para clasificación multiclase.
        %     \end{itemize}

        % \subsubsection{VGG-16}
        %     \noindent Supuso un gran salto en rendimiento con respecto a los modelos anteriores. Los principales cambios con modelos anteriores como AlexNet es que dejaron de usar filtros de grandes dimensiones en las primeras etapas de la red para pasar a usar solo filtros $3 \times 3$. Esto ahorraba costes y permitía hacer una red más profunda \cite{simonyan2014very}.

        %     \begin{figure}[!h]
        %         \centering
        %         \includegraphics[width=0.8\textwidth]{img/VGG-16.jpeg}
        %         \caption{Arquitectura de VGG-16.Imagen extraída de \cite{StanfordCourse}.}
        %         \label{fig:VGG}
        %     \end{figure}

        %     \noindent Destacan las siguientes propiedades:

        %     \begin{itemize}
        %         \item Consigue en Imagenet una precisión del $92.7 \%$.
        %         \item Tiene unos 138 millones de parámetros entrenables que es más del doble de cualquiera de los modelos anteriores, lo que hace que VGG-16 sea muy lenta de entrenar.
        %     \end{itemize}

        % \subsubsection{ResNet}

        %     \noindent Tras los modelos anteriores, todavía quedaba un problema que tenían las CNN sin resolver, y es que cuanto más profundas eran, más problema había con el cálculo de los gradientes, pues tendían a cantidades infinitesimalmente pequeñas. Es por ello que surge la arquitectura \textbf{ResNet} \cite{he2016deep}, que introduce cortes en la red conectando un tensor con el que había unas cuantas etapas atrás, a esto se le denomina \textbf{bloque residual}. 

        %     \begin{figure}[!h]
        %         \centering
        %         \includegraphics[width=0.5\textwidth]{img/resnet_module.jpeg}
        %         \caption{Bloque residual de una ResNet. Como vemos, se suma el tensor $x$ con el tensor $\mathcal{F}(x)$ que surge unas etapas después. Imagen extraída de \cite{he2016deep}.}
        %         \label{fig:Resnet}
        %     \end{figure}
            
        %     \noindent Con estos bloques residuales impedimos que los gradientes tiendan a cero rápidamente, con lo que podemos crear redes mucho más profundas. De hecho la versión de ResNet con 152 capas ganó el concurso ILSVRC 2015 siendo la red más profunda en aquel entonces. 

        %     \begin{figure}[!h]
        %         \centering
        %         \includegraphics[width=0.9\textwidth]{img/ImageNet.png}
        %         \caption{Resumen de los ganadores del concurso ILSVRC hasta 2015 con la aparición de Resnet. Imagen extraída de \cite{StanfordCourse}.}
        %         \label{fig:ImageNet}
        %     \end{figure}

\section{Autoencoders}
    \noindent Hasta ahora hemos visto qué son las CNN y su gran importancia en Deep learning en tareas de reconocimiento de objetos en imágenes. A continuación, vamos a presentar un tipo de red que se emplea en tareas de \textit{Aprendizaje no supervisado} y que será de gran importancia en nuestro trabajo. Se tratan de las redes conocidas como \textbf{Autoencoders}.

    \subsection{Introducción}
        \noindent Los \textbf{Autoencoders} \cite{autoencoders2017} son un tipo específico de red cuya entrada \entrecomillado{coincide} con su salida. Se encargan de reducir las imágenes de entrada a un vector perteneciente a un espacio vectorial latente y que idealmente codifica los elementos más relevantes en la imagen para posteriormente reconstruir la imagen a partir de este vector con la intención de que sea lo más parecida posible a la de entrada.

        \medskip

        \noindent Los autoencoders tienen tres componentes principales: 

        \begin{itemize}
            \item \textbf{Encoder}: Suele ser una subred que se encarga de codificar la entrada a un vector de un espacio vectorial latente. 
            \item \textbf{Code}: es el vector que codifica la imagen de entrada. 
            \item \textbf{Decoder}: se trata de una subred que reconstruye la imagen a partir del vector.
        \end{itemize}

        \noindent Como podemos ver, no se necesita ningún tipo de etiqueta para reconstruir las imágenes de entrada, por lo que este tipo de redes se emplean en tareas de \textit{Aprendizaje no supervisado}.

        \begin{figure}[!h]
            \centering
            \includegraphics[width=0.8\textwidth]{img/Autoencoder.png}
            \caption{Esquema de un Autoencoder básico. Imagen extraída de \cite{autoencoders2017}.}
            \label{fig:Autoencoder}
        \end{figure}

        \medskip 

        \noindent Esta estructura ha ido evolucionando durante los últimos años, por lo que vamos a ver a continuación esta evolución hasta llegar a la red que emplearemos en nuestro trabajo, el \textbf{Adversarial Autoencoder}.

    \subsection{Evolución de los Autoencoders}
        \subsubsection{Generative Adversarial Networks (GANs)}
            \noindent Es una red cuyo fin es \entrecomillado{crear nuevo contenido} y que sea lo más similar posible al contenido de la base de datos que usamos para entrenar la red. Para ello se pretende aprender la distribución de probabilidad que siguen los píxeles de las imágenes del dataset, por lo que el objetivo realmente sería el re generar nuevos valores aleatorios de la distribución de probabilidad que siguen las imágenes \cite{GAN}. 

            \medskip

            \noindent Para esta tarea suele emplearse una CNN denominada \textit{Generative Network}, que tiene como objetivo recibir una imagen o un tensor que sigue una distribución conocida y producir como salida una imagen de las mismas dimensiones que la de entrada que siga la distribución desconocida de las imágenes del dataset a nivel de \textbf{pixel}. Esta red tiene la estructura de un \textbf{Autoencoder}.

            \begin{figure}[!h]
                \centering
                \includegraphics[width=0.8\textwidth]{img/Generative_network.png}
                \caption{Ejemplo de una Generative Network que pretende aprender la distribución de probabilidad de un conjunto de imágenes de perros. Imagen extraída de \cite{autoencoders2017}.}
                \label{fig:Generative Network}
            \end{figure}
            
            \medskip

            \noindent Por otro lado, se hace uso de una segunda CNN denominada \textit{Discriminador}, que recibe un conjunto de imágenes del dataset e imágenes generadas por el \textit{Genenerative Network} imagen de entrada y trata de clasificarlas en imágenes procedentes del dataset o imágenes generadas por la red. Por lo que el objetivo de la GAN en general podría resumirse en generar imágenes con la \textit{Genenerative Network} que sean capaces de engañar al \textit{Discriminador}, y para esto se pretende aprender lo mejor posible la distribución de probabilidad de las imágenes del dataset.

            \begin{figure}[!h]
                \centering
                \includegraphics[width=0.8\textwidth]{img/proceso_entrenamiento_GAN.png}
                \caption{Resumen del proceso de entrenamiento de una GAN. Imagen extraída de \cite{autoencoders2017}.}
            \end{figure}

            \begin{figure}[!h]
                \centering
                \includegraphics[width=0.98\textwidth]{img/GAN.png}
                \caption{Ejemplo de la arquitectura de una GAN para generar imágenes de dígitos manuscritos. Imagen extraída de \cite{AAE}}
                \label{fig:GAN}
            \end{figure}
 
        \subsubsection{Variational Autoencoder (VAE)}
            \noindent Un VAE puede definirse como un autoencoder cuyo entrenamiento es regularizado para evitar el overfitting y asegurarse que el espacio vectorial latente tiene propiedades adecuadas para la generación de nuevos datos.

            \medskip

            \noindent La principal diferencia entre el Autoencoder y el Variational Autoencoder es que en vez de codificar cada elemento de entrada como un punto del espacio vectorial latente, va a codificarlo como una distribución sobre el espacio latente buscando la distribución de los datos, no de los píxeles. El modelo de entrenamiento sería:
            
            \begin{itemize}
                \item La entrada se codifica como una distribución sobre el espacio vectorial latente.
                \item Un punto del espacio latente es muestreado por la distribución. 
                \item Se decodifica el punto y se calcula el error de reconstrucción.
                \item Se usa backpropagation con el error anterior.
            \end{itemize}

            \begin{figure}[!h]
                \centering
                \includegraphics[width=0.98\textwidth]{img/vae_1.png}
                \caption{Diferencia en el tratamiento de los datos por parte del encoder en una VAE y un Autoencoder clásico. Imagen extraída de \cite{VAE}.}
                \label{fig:VAE_1}
            \end{figure}

            \noindent El modo de proceder habitual es tomar las distribuciones de los datos codificados como normales, así puede devolverse la media y matriz de covarianzas que describen la Gaussiana. La razón por la que la entrada se codifica como una distribución es porque esto permite expresar de forma natural la regularización del espacio vectorial latente ya que las distribuciones que salen del codificador se forzarán a que sean lo más similar posible a una distribución normal estandar, este término regulariazante se incluye en la función de coste como podemos ver en \autoref{fig:VAE_2}. Para ello utiliza la divergencia de \textit{Kulback-Leibler} (KL) como medida entre la diferencia entre dos distribuciones.

            \begin{figure}[!h]
                \centering
                \includegraphics[width=0.8\textwidth]{img/vae_2.png}
                \caption{Diferencia en la función de pérdida de un Autoencoder clásico (imagen superior) y una VAE (imagen inferior). Destacamos el término KL regularizante que pretende que las distribuciones de los datos sigan una normal estándar. Imagen extraída de \cite{VAE}.}
                \label{fig:VAE_2}
            \end{figure}
        
        \subsubsection{Adversarial Autoencoder(AAE)}
            \noindent Un Adversarial Autoencoder no es más que la unión de la arquitectura de un Autoencoder con el concepto de Adversarial Loss y el Discrimnante introducido por la GAN. Por otro lado utiliza la idea del VAE para regularizar el espacio vectorial latente pero en lugar de la divergencia de KL emplea el adversarial loss, y en lugar de muestrear una distribución con diferentes parámetros para cada imagen de entrada, lo que pretende es que todas las imágenes de entrada, al ser codificadas, sigan la misma distribución prefijada.

            \medskip

            \noindent Para ello se añade un nuevo componente que actúa como Discriminador y el Encoder actuará como Generador(a diferencia de las GANs, dónde la salida del Generador era la imagen, no el vector del espacio vectorial latente). Se selecciona una distribución a seguir por los vectores del espacio vectorial latente (generalmente una distribución normal estándar). Así, los vectores generados por el Encoder tratarán de engañar al Discriminador, que tendrá que discernir entre si proceden de la distribución elegida o no. En otras palabras, si el vector latente es una muestra aleatoria de la distribución deseada o bien es un vector generado por el ecoder.

            \medskip

            \noindent Por lo tanto la arquitectura de un AAE presenta los siguientes componentes: 

            \begin{itemize}
                \item \textbf{Encoder}. Tomará la entrada y la transformará en un vector latente de baja dimensión.
                \item \textbf{Decoder}. Tratará de reconstruir la imagen a partir del vector latente generado por el Encoder. 
                \item \textbf{Discriminador}. Toma vectores de la distribución del espacio vectorial latente deseada (reales) y también vectores generados por el Encoder (fakes) y tratará de discernir entre si proceden de la distribución o no.
            \end{itemize}

            \medskip

            \noindent Podemos ver un esquema de esta arquitectura en la \autoref{fig:AAE}. Esta arquitectura será la que emplearemos en nuestro proyecto, aunque presenta algunas variaciones que introduciremos más adelante.
            
            \begin{figure}[!h]
                \centering
                \includegraphics[width=0.5\textwidth]{img/AAE.png}
                \caption{Esquema de la arquitectura de un AAE. El Encoder sería la red a la que entra la imagen $x$, el vector $z$ sería el vector latente, que sirve de entrada al Discriminador $D_{gauss}$ y finalmente el vector $z$ es la entrada del Decoder que reconstruye la imagen. Imagen extraída de \cite{AAE}.}
                \label{fig:AAE}
            \end{figure}

            

\section{Técnicas empleadas}
    \noindent En esta sección vamos a presentar las distintas técnicas que se emplearán en el trabajo durante el entrenamiento de la red que utilizaremos y que se presentará más adelante. 

    \subsection{Few-shot Learning y Data Augmentation} \label{sub:data_augmentation}
        \noindent Entendemos por \textbf{Few-shot learning} los problemas de AA en los que en la etapa de aprendizaje supervisado se dispone de muy pocos datos etiquetados. En nuestro caso disponemos de un dataset Forense con pocas imágenes etiquetadas, por ello lo consideramos un problema de few-shot learning.

        \medskip

        \noindent Por otro lado, cuando se presentan este tipo de problemas se aplican técnicas de \textbf{data augmentation}. Esta técnica consiste en \textit{crear} nuevos datos de entrenamiento a partir de las imágenes originales. Para ello se aplican \textit{deformaciones} a las imágenes del dataset, como \textit{rotaciones}, \textit{traslaciones} u \textit{oclusiones}. Este tipo de transformaciones pueden ser de gran utilidad para entrenar el modelo en datos más complicados que los originales y para añadir variabilidad al dataset.
        
\endinput
%------------------------------------------------------------------------------------
% FIN DEL CAPÍTULO. 
%------------------------------------------------------------------------------------

