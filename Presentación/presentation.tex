% !TeX document-id = {2870843d-1baa-4f6a-bd0a-a5c796104a32}
% !BIB TS-program = biber
% !TeX encoding = UTF-8
% TU Delft beamer template

\documentclass[aspectratio=43]{beamer}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{calc}
\usepackage[absolute,overlay]{textpos}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{comment}
\usepackage{siunitx}
\usepackage{MnSymbol,wasysym}
\usepackage{array}
\usepackage{qrcode}
\useoutertheme[subsection=false]{miniframes}

\setbeamertemplate{navigation symbols}{} % remove navigation symbols
\mode<presentation>{\usetheme[verticalbar=false]{tud}}

% BIB SETTINGS
\usepackage[
    backend=biber,
    giveninits=true,
    maxnames=30,
    maxcitenames=20,
    uniquename=init,
    url=false,
    style=authoryear,
]{biblatex}
\addbibresource{bibfile.bib}
\setlength\bibitemsep{0.3cm} % space between entries in the reference list
\renewcommand{\bibfont}{\normalfont\scriptsize}
\setbeamerfont{footnote}{size=\tiny}
\renewcommand{\cite}[1]{\footnote<.->[frame]{\fullcite{#1}}}
\setlength{\TPHorizModule}{\paperwidth}
\setlength{\TPVertModule}{\paperheight}

\newcommand{\absimage}[4][0.5,0.5]{%
	\begin{textblock}{#3}%width
		[#1]% alignment anchor within image (centered by default)
		(#2)% position on the page (origin is top left)
		\includegraphics[width=#3\paperwidth]{#4}%
\end{textblock}}

\newcommand{\mininomen}[2][1]{{\let\thefootnote\relax%
	\footnotetext{\begin{tabular}{*{#1}{@{\!}>{\centering\arraybackslash}p{1em}@{\;}p{\textwidth/#1-2em}}}%
	#2\end{tabular}}}}

\title[]{Localización de landmarks cefalométricos por medio de técnicas de few-shot learning y análisis de redes convolucionales}
\institute[]{\textbf{Tutores}: Pablo Mesejo Santiago, Javier Merí de la Maza \and Universidad de Granada, España}
\author{Alejandro Borrego Megías}
\date{\today}



%%%%%%%%%%%%% EMPIEZA LA PRESENTACION %%%%%%%%%%%%%
\begin{document}

{
\setbeamertemplate{footline}{\usebeamertemplate*{minimal footline}}
\frame{\titlepage}
}

% Parte de Matemáticas
\part{Análisis de Redes convolucionales}

%INDICE
\begin{frame}{Índice Primera Parte}
  \textcolor{tudCyan}{\textbf{Análisis de redes convolucionales}}
  \medskip
  \tableofcontents[part=1]
\end{frame}

% DIAPOSITIVA ANTES DE CADA SECCION
\AtBeginSection[ ]
{
\begin{frame}{Índice Primera Parte}
    \tableofcontents[currentsection]
\end{frame}
}

%INTRODUCCION
\section{Introducción}
  \begin{frame}[fragile]{Redes Neuronales Convolucionales}
    \begin{itemize}
      \item Buen rendimiento, comprobable empíricamente.
      \item Vía de estudio abierta en lo que se refiere a
      la modelización matemática y la justificación teórica de estos resultados.
    \end{itemize}

    Destacamos: 

    \begin{figure}
      \centering
      \includegraphics[width=0.4\textwidth]{imgs/translation_invariance.png}
    \end{figure}

    \begin{figure}
      \centering
      \includegraphics[width=0.7\textwidth]{imgs/Deformaciones.png}
    \end{figure}
  \end{frame}

  \begin{frame}{Invarianza por traslaciones}
    Trabajamos sobre el espacio de funciones $L^2(\mathbb{R}^d)$.

    \begin{block}{Definición de traslación}
       Sea $f\in L^2(\mathbb{R}^d)$, $L_cf(x)=f(x-c)$ es la traslación de $f$ por $c \in \mathbb{R}^d$.
    \end{block}

    \begin{block}{Invarianza por traslaciones de un operador $\Phi$}
      Decimos que un operador $\Phi$ sobre  $L^2(\mathbb{R}^d)$, es invariante por traslaciones si $\Phi(L_cf(x))=\Phi(f)$ para todo $f \in L^2(\mathbb{R}^d)$ y para todo $c \in \mathbb{R}^d$.
   \end{block}
  \end{frame}

  \begin{frame}{Invarianza frente a pequeñas deformaciones}
    \textcolor{tudCyan}{\textit{Deformación $\implies$ Difeomorfismo}}

    \textcolor{tudCyan}{\textit{Deformaciones pequeñas $\implies$ Difeomorfismo cercanos a traslaciones}}  

    \begin{block}{Definición}
      Denotemos $L_{\tau} f(x)=f(x-\tau(x))$ como la acción del difeomorfismo $1-\tau$ sobre $f$.

      Donde $\tau$ es el campo de desplazamiento. 
    \end{block}

    \textcolor{tudCyan}{\textbf{\centering Invarianza frente a pequeñas deformaciones \\ \centering $\Downarrow$ \\ \centering Lipschitz-continuidad frente a la acción de difeomorfismos}}  
  \end{frame}

  \begin{frame}{Invarianza frente a pequeñas deformaciones}

    \begin{block}{Condición de Lipschitz clásica}
      Sea $f: M \rightarrow N$ una función entre dos espacios métricos $M$ y $N$ con sus respectivas distancias $d_M$ y $d_N$. Se dice que $f$ satisface la condición de Lipschitz si $\exists C>0$ tal que: 
      $$d_N(f(x),f(y))\leq C d_M(x,y), \; \; \forall x,y \in M$$ 
    \end{block}

    En nuestro caso: 

    \begin{equation}
      \|\Phi(f) - \Phi(L_\tau f) \| \leq \|f\| · d(1, 1-\tau)
    \end{equation}

    Necesitamos una definición para la distancia entre dichos difeomorfismos.
  \end{frame}


  \begin{frame}{Invarianza frente a pequeñas deformaciones}
    \begin{block}{Distancia entre $1-\tau$ y $1$}
      Se define una distancia entre $1-\tau$ y $1$ en cualquier subconjunto compacto $\Omega$ de $\mathbb{R}^d$ como 
      \begin{equation} \label{eq::distancia}
        d_\Omega(1,1-\tau) = \sup_{x \in \Omega} |\tau (x)| + \sup_{x \in \Omega} |\nabla \tau (x)| + \sup_{x \in \Omega}|H \tau (x)|
      \end{equation}
    \end{block}

    La invarianza frente a pequeñas deformaciones de un operador $\Phi$ invariante por traslaciones viene determinada por: 

    \begin{equation}
      \| \Phi(f)-\Phi(L_{\tau}f)\|\leq C\|f\|(\|\nabla\tau\|_{\infty} + \|H \tau\|_\infty).
    \end{equation}

    Con $f\in L^2(\mathbb{R}^d)$ y $C>0$.
  \end{frame}

  \begin{frame}{Próximos pasos}
    \centering \textcolor{tudCyan}{\textbf{¿Qué operador $\Phi$ tomar que cumpla todo lo anterior?}}
  \end{frame}

\section{Modelización}

\begin{frame}{Módulo de la transformada de Fourier}
  \textcolor{tudCyan}{Vamos a probar con el módulo de la transformada de Fourier:}
  $$\Phi(f)=|\widehat{f}| \;\; f\in L^2(\mathbb{R}^2)$$.

  Con este operador observamos que: 
  \begin{itemize}
    \item Es invariante por traslaciones.
    \item No es Lipschitz continuo frente a pequeñas deformaciones.
  \end{itemize}

  Debemos buscar otro operador.
\end{frame}

\begin{frame}{Alternativa: Ondeletas}
  
  \begin{block}{Ondeleta Madre}
    Una ondeleta madre escalada por un factor $2^{j}$ con $j \in \mathbb{Z}$ y rotada por $r \in G$ siendo $G$ el grupo finito de rotaciones, se escribe: 
    $$\psi_{2^j r}(x)=2^{j} \psi(2^j r^{-1} x).$$
  \end{block}

  Usaremos ondeletas madre del tipo: 

  \begin{equation}
    \psi(x)=e^{i\eta x} \Theta(x)
  \end{equation}
  donde $\Theta(x)$ es una función real con soporte en una bola de baja frecuencia en $x=0$, cuyo radio es del orden de $\pi$.
\end{frame}

\begin{frame}{La transformada de Littlewood-Paley}
  Con esta generamos la siguiente base ortonormal de ondeletas: 
  \begin{equation}
    \lbrace \psi_\lambda (x) \rbrace_{\lambda= 2^j r \in 2^\mathbb{Z} \times G}
  \end{equation}

  \begin{block}{transformada de Littlewood-Paley}
    \begin{equation}
      \forall x \in  \mathbb{R}^d \;\; W[\lambda]f(x)= f \ast \psi_\lambda(x)=\int f(u)\psi_\lambda(x-u) du .
    \end{equation}
    donde $\lambda \in 2^j r \in 2^\mathbb{Z} \times G$
  \end{block}
\end{frame}

\begin{frame}{Problema: La escala}
  \begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{imgs/Relacion_escala_frecuencia.png}
  \end{figure}
  Fijada una escala $2^J \; \; J\in \mathbb{Z}$, se establece un umbral tal que solo se mantienen las ondeletas de escala $2^j > 2^{-J}$.
\end{frame}

\begin{frame}{Problema: La escala}
  Surge la necesidad de promediar las frecuencias no cubiertas por el factor de escala fijado:

  \begin{equation}
    A_Jf=f \ast \phi_ {2^J} \; \; \text{con} \quad \; \phi_ {2^J}(x)=2^{-J} \phi(2^{-J}x).
  \end{equation}

  Así, los coeficientes obtenidos, fijada una escala son:

  $$W_J f=\lbrace A_Jf,(W[\lambda]f)_{\lambda \in \Lambda_J} \rbrace$$ 

  con $\Lambda_J=\lbrace \lambda=2^jr:\;r\in G^{+}, \; 2^j>2^{-J}\rbrace$.
\end{frame}

\begin{frame}{Coeficientes unitarios}
  \begin{alertblock}{Condición $W_J$ unitario}
    Para cualquier $J \in \mathbb{Z}$ o $J=\infty$, $W_J$ es unitario en el espacio de funciones reales o complejas de $L^2(\mathbb{R}^d)$ si y solo si para casi todo $\omega \in \mathbb{R}^d$ se cumple: 
    \begin{equation}\label{eq::1.2}
        \beta \sum_{j=-\infty}^\infty \sum_{r \in G} |\widehat{\psi}(2^{-j}r^{-1}\omega)|^2=1 \; \; y
        \;\;|\widehat{\phi}(\omega)|^2= \beta \sum_{j=-\infty}^0 \sum_{r\in G} |\widehat{\psi}(2^{-j}r^{-1}\omega)|^2,
    \end{equation}
    donde $\beta=1$ para funciones complejas y $\beta=\frac{1}{2}$ para funciones reales.
  \end{alertblock}
\end{frame}

\begin{frame}{Convenios}
  Tranajaremos con funciones reales.
  \begin{itemize}
    \item $W_J$ es unitario. 
    \item $\widehat{\phi}(\omega)$ es real y simétrica, por lo que $\phi$ también lo será y $\phi(rx)=\phi(x) \;\; \forall r \in G$. 
    \item Las derivadas de $\phi$ pertenecen a $L^1(\mathbb{R}^d)$.
  \end{itemize}
\end{frame}

\begin{frame}{Operador de dispersión}
  \begin{alertblock}{Operador de dispersión}
    Sea $\mathcal{P}_\infty$ el conjunto de todos los caminos finitos. La transformada de dispersión de $f \in L^1(\mathbb{R}^d)$ se define para cualquier camino $p \in \mathcal{P}_\infty$ como:
    \begin{equation}
      \overline{S}f(p)=\int_{\mathbb{R}^d}U[p]f(x)dx 
    \end{equation}
  \end{alertblock}

  Siendo $\overline{S}f(p)$ invariante a traslaciones para un $f$ fijo.
\end{frame}


\begin{frame}{Operador de dispersión}
  \begin{alertblock}{Operador de dispersión}
    Sea \textcolor{tudGreen}{$\mathcal{P}_\infty$} el conjunto de todos los caminos finitos. La transformada de dispersión de $f \in L^1(\mathbb{R}^d)$ se define para cualquier camino \textcolor{tudGreen}{$p \in \mathcal{P}_\infty$} como:
    \begin{equation}
      \overline{S}f(\textcolor{tudGreen}{p})=\int_{\mathbb{R}^d}U[\textcolor{tudGreen}{p}]f(x)dx 
    \end{equation}
  \end{alertblock}
  
  Siendo $\overline{S}f(p)$ invariante a traslaciones para un $f$ fijo.
\end{frame}

\begin{frame}{Caminos de frecuencias}
  \begin{block}{Ondeleta Madre}
    Una secuencia ordenada $p=(\lambda_1,\lambda_2, \ldots , \lambda_m)$ con $\lambda_k \in \Lambda_\infty=2^{\mathbb{Z}} \times G^{+} $ se denomina \textbf{camino}. Al camino vacío se le denota por $p=\emptyset$. 
  \end{block}
  
  Usaremos caminos de frecuencias descendentes $p=(\lambda_k)_{k\leq m}$ en el cual $|\lambda_{k+1}| \leq |\lambda_k|$. Pues el propagador $U[\lambda]$ progresivamente lleva la energía de la señal a frecuencias cada vez menores.
\end{frame}

\begin{frame}{Operador de dispersión}
  \begin{alertblock}{Operador de dispersión}
    Sea $\mathcal{P}_\infty$ el conjunto de todos los caminos finitos. La transformada de dispersión de $f \in L^1(\mathbb{R}^d)$ se define para cualquier camino $p \in \mathcal{P}_\infty$ como:
    \begin{equation}
      \overline{S}f(p)=\textcolor{tudCyan}{\int_{\mathbb{R}^d}U[p]f(x)dx} 
    \end{equation}
  \end{alertblock}

  Siendo $\overline{S}f(p)$ invariante a traslaciones para un $f$ fijo.
\end{frame}


\begin{frame}{Propagador de dispersión}
  El operador $W[\lambda]f=f\ast \psi_\lambda$ es Lipschitz-continuo bajo la acción de difeomorfismos para $f \in L^2(\mathbb{R}^d)$ fijo. 

  \medskip

  \textcolor{tudCyan}{¿Pero invariante a traslaciones?}

  \medskip

  \begin{alertblock}{Condición para coeficientes I.T.}
    Si $U[\lambda]$ es un operador definido en $L^2(\mathbb{R}^d)$, no necesariamente lineal pero que conmuta con traslaciones, entonces $\int_{\mathbb{R}^d} U[\lambda]f(x)dx$ es invariante a traslaciones si es finito.
  \end{alertblock}
  \medskip
  Pero como $\int_{\mathbb{R}^d} \psi(x)dx=0 \implies \int_{\mathbb{R}^d} f \ast \psi(x) dx=0$. 
\end{frame}

\begin{frame}{Propagador de dispersión}
  Para obtener coeficientes invariantes por traslaciones. 
  $$U[\lambda]f= \textcolor{red}{M[\lambda]} W[\lambda]$$

  \medskip

  El operador más sencillo que garantiza coeficientes invariantes por traslaciones y Lipschitz-continuidad frente a difeomorfismos es:

  \begin{block}{Definición del operador $U[\lambda]$}
    $$U[\lambda]f=M[\lambda]W[\lambda]f=|f \ast \psi_\lambda|=\left | \int_{\mathbb{R}^d} f(u)\psi_\lambda(x-u) du \right|$$
  \end{block}
\end{frame}

\begin{frame}{Propagador de dispersión}
  Así, sobre un camino $p$ definimos el \textbf{propagador de dispersión} como: 

  $$  U[p]f=U[\lambda_m] \ldots U[\lambda_2]U[\lambda_1]=\left| |f \ast \psi_{\lambda_1} | \ast \psi_{\lambda_2} | \ldots | \ast \psi_{\lambda_m} \right|
  $$

  \medskip

  Además:

  $$ \int_{\mathbb{R}^d} U[p]f(x)dx < \infty $$
\end{frame}

\begin{frame}{operador de ventana}
  \begin{block}{Definición operador de ventana}
    Sea $J \in \mathbb{Z}$ y $\mathcal{P}_J$ el conjunto de caminos finitos $p=(\lambda_1,\lambda_2,...,\lambda_m)$ con $\lambda_k \in \Lambda_J$ y $|\lambda_k|=2^{k}>2^{-J}$. Una ventana de transformada de dispersión se define para todo $p \in \mathcal{P}_J$ por
    \begin{equation}
      S_J[p]f(x)=U[p]f \ast \phi_{2^J}(x)=\int_{\mathbb{R}^d}U[p]f(u)\phi_{2^J}(x-u)du.
    \end{equation}
    \begin{equation}
      S_J[p]f(x)=\left| |f \ast \psi_{\lambda_1} | \ast \psi_{\lambda_2} | \ldots | \ast \psi_{\lambda_m} \right| \ast \phi_{2^J}(x).
    \end{equation}
  \end{block}
\end{frame}


\begin{frame}{operador de ventana}
  \begin{figure}
    \centering
    \includegraphics[width=0.99\textwidth]{imgs/ScatteringPropagator.png}
  \end{figure}
\end{frame}

\begin{frame}{Diferencias y Similitudes con una Red convolucional}
  \textbf{\textcolor{tudCyan}{Similitudes:}}
  \begin{itemize}
    \item Cascada de convoluciones (operador $W[\lambda]$).
    \item Capas de \textit{pooling} (operador $M[\lambda]$ y $\phi_{2^J}$).
    \item Si $p$ tiene longitud $m$: $S_J[p]f(x)$ equivale al resultado de la capa $m$ de la red.
  \end{itemize}

  \textbf{\textcolor{tudCyan}{Diferencias:}}
  \begin{itemize}
    \item Los pesos no se aprenden.
  \end{itemize}
\end{frame}

\section{Invarianza por traslaciones}

\begin{frame}{Ondeletas admisibles}
  
  \begin{block}{ondeletas admisibles}
    Una ondeleta de dispersión se dice que es admisible si existe $\eta \in \mathbb{R}^d$ y una función $\rho \geq 0$, con $|\widehat{\rho}(\omega)| \leq |\widehat{\phi}(2\omega)|$ y $\widehat{\rho}(0)=1$, tal que la función: 

    \begin{equation}\label{eq::1.6}
      \widehat{\Psi}(\omega)=|\widehat{\rho}(\omega - \eta)|^2 - \sum_{k=1}^{+\infty} k(1-|\widehat{\rho}(2^{-k}(\omega - \eta))|^2)
    \end{equation}
      
    \noindent satisface: 
    \begin{equation} \label{eq::1.7}
      \alpha= \inf_{1\leq|w|\leq2} \sum_{j=-\infty}^{\infty} \sum_{r\in G} \widehat{\Psi} (2^{-j}r^{-1}\omega)|\widehat{\psi}(2^{-j}r^{-1}\omega)|^2>0.
    \end{equation}
  \end{block}


\end{frame}

\begin{frame}{No-expansividad y conservación de la norma}

  \begin{alertblock}{No-expansividad}
    \noindent Para $f,h \in L^2(\mathbb{R}^d)$ y $J\in \mathbb{Z}$ se cumple

    \begin{equation} \label{eq::1.10}
      || S_{J+1} [\mathcal{P}_{J+1}]f- S_{J+1}[\mathcal{P}_{J+1}]h || \leq ||S_J[\mathcal{P}_J]f - S_J[\mathcal{P}_J]h ||. 
    \end{equation}
  \end{alertblock}

  \begin{alertblock}{Conservación de la norma}
    Si las ondeletas son admisibles, entonces para toda $f\in L^2(\mathbb{R}^d)$ se tiene que
    \begin{equation}  
      \lim_{m\rightarrow\infty} \|U[\Lambda_J^m]f \|^2=\lim_{m\rightarrow\infty} \sum_{n=m}^{\infty} \|S_J[\Lambda_J^n]f\|^2=0
    \end{equation}
    y
    \begin{equation}
      \|S_J[\mathcal{P}_J]f\|=\|f\|.
    \end{equation}
  \end{alertblock}
\end{frame}

\begin{frame}{Invarianza por traslaciones}
  \begin{alertblock}{Invarianza por traslaciones}
    Para ondeletas de dispersión admisibles se tiene que 
    $$\forall f \in L^2(\mathbb{R}^d), \; \forall c\in \mathbb{R}^d \;\;\; \lim_{J\rightarrow \infty}||S_J[\mathcal{P}_J] f-S_J[\mathcal{P}_J] L_cf||=0.$$
  \end{alertblock}
\end{frame}

% Parte de Informática
\part{Localización de landmarks cefalométricos por medio de técnicas de few-shot learning}

\begin{frame}{Índice Segunda Parte}
  \textcolor{tudCyan}{\textbf{Localización de landmarks cefalométricos por medio de técnicas de few-shot learning}}
  \medskip
  \tableofcontents[part=2]
\end{frame}

% Current section
\AtBeginSection[ ]
{
\begin{frame}{Índice Segunda Parte}
    \tableofcontents[currentsection]
\end{frame}
}

\section{Definición del Problema}

\begin{frame}{Landmarks faciales}
  \begin{itemize}
    \item Puntos de referencia marcados sobre la cara.
    \item No guardan relación con la morfología del cráneo.
    \item Se emplean para reconocimiento facial.
  \end{itemize}

  \begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{imgs/aflw.png}
  \end{figure}

\end{frame}

\begin{frame}{Landmarks cefalométricos}
  \begin{itemize}
    \item Puntos de referencia que definen la morfología del cráneo.
    \item Correspondencia directa cráneo-cara.
    \item Se emplean en Antropología forense.
  \end{itemize}
  \begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{imgs/marcado_landmarks.png}
  \end{figure}
\end{frame}

\begin{frame}{Base de datos}
  \begin{itemize}
    \item 167 imágenes \textit{in-the-wild}.
    \item Imágenes a color y escala de grises.
    \item 30 landmarks a predecir.
  \end{itemize}
  \begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{imgs/imagenes_ejemplo_dataset.png}
  \end{figure}
  \medskip
\end{frame}

\begin{frame}{Objetivos}
  \begin{enumerate}
    \item Revisión del \textbf{estado del arte}.
    \item Investigación sobre los \textbf{Autoencoders} y \textbf{redes adversarias} existentes.
    \item Estudio y \textbf{preprocesamiento} de la base de datos.
    \item Estudio \textbf{experimental} usando diversos modelos que derivan del framework principal.
  \end{enumerate}
\end{frame}

\section{Fundamentos Teóricos}

\begin{frame}{Redes Neuronales Convolucionales}
  \begin{figure}
    \centering
    \includegraphics[width=0.99\textwidth]{imgs/ArquitecturaRedNeuronal.png}
    \caption{Imagen extraída de \cite{StanfordCourse}.}
  \end{figure}
\end{frame}

\begin{frame}{Capas Convolucionales}
  \begin{columns}[onlytextwidth]
    \begin{column}{.5\textwidth}
       \textbf{Características:}
      \begin{itemize}
        \item Operación de convolución.
        \item Menor número de pesos.
        \item Mayor profundidad de red.
      \end{itemize}
      Imágenes extraída de \cite{StanfordCourse}.
    \end{column}
    \begin{column}{.5\textwidth}
      \absimage{.75, .30}{.50}{imgs/mapa_activacion.png}
      \absimage{.75, .65}{.45}{imgs/sucesion_conv_layer.png}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Capa de Pooling}
  \begin{figure}
    \centering
    \includegraphics[width=0.99\textwidth]{imgs/pooling.png}
    \caption{Imagen extraída de \cite{StanfordCourse}.}
  \end{figure}
\end{frame}

\begin{frame}{Capa totalmente conectada}
  \begin{figure}
    \centering
    \includegraphics[width=0.99\textwidth]{imgs/single_hidden_layer.png}
  \end{figure}
\end{frame}

\begin{frame}{Arquitectura de una red neuronal convolucional}
  \begin{figure}
    \centering
    \includegraphics[width=0.99\textwidth]{imgs/ArquitecuraGeneral.png}
    \caption{Imagen extraída de \cite{StanfordCourse}.}
  \end{figure}
\end{frame}

\begin{frame}{Autoencoders}
  \begin{figure}
    \centering
    \includegraphics[width=0.99\textwidth]{imgs/Autoencoder.png}
    \caption{Imagen extraída de \cite{autoencoders2017}.}
  \end{figure}
\end{frame}

\begin{frame}{GAN}
  \begin{figure}
    \centering
    \includegraphics[width=0.99\textwidth]{imgs/GAN.png}
    \caption{Imagen extraída de \cite{autoencoders2017}.}
  \end{figure}
\end{frame}

\begin{frame}{Variational Autoencoder (VAE)}
  \begin{figure}
    \centering
    \includegraphics[width=0.99\textwidth]{imgs/Autoencoder.png}
    \caption{Imagen extraída de \cite{autoencoders2017}.}
  \end{figure}
\end{frame}

\begin{frame}{Adversarial Autoencoder (AAE)}
  \begin{figure}
    \centering
    \includegraphics[width=0.99\textwidth]{imgs/AAE.png}
    \caption{Imagen extraída de \cite{autoencoders2017}.}
  \end{figure}
\end{frame}
\section{Estado del arte}

\begin{frame}{Búsqueda Scopus}
  \begin{itemize}
    \item \textbf{Landmarks faciales}: Gran número y variedad de artículos usando en su mayoría Deep learning.
    \item \textbf{Landmarks cefalométricos}: Escasa literatura y gran variedad de enfoques.
  \end{itemize}
\end{frame}

\begin{frame}{Búsqueda Scopus}
  \begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{imgs/Scopus_1.png}
    \caption{Búsqueda sobre reconocimiento de landmarks faciales.}
  \end{figure}
\end{frame}

\begin{frame}{Búsqueda Scopus}
  \begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{imgs/Scopus_2.png}
    \caption{Búsqueda sobre reconocimiento de landmarks cefalométricos.}
  \end{figure}
\end{frame}

\begin{frame}{Taxonomía}
  \begin{figure}
    \centering
    \includegraphics[width=0.99\textwidth]{imgs/taxonomia.png}
  \end{figure}
\end{frame}

\section{Experimentos}

\begin{frame}{3FabRec}
  \begin{figure}
    \centering
    \includegraphics[width=0.99\textwidth]{imgs/3fabrec_arquitectura.png}
  \end{figure}
\end{frame}

\begin{frame}{3FabRec}
  \begin{block}{Función de pérdida parte no supervisada}
    \begin{align*}
      \min_{E,G} \max_{D_z,D_x} & \mathcal{L}_{AE}(E,G,D_z,D_x) = \\
      & \lambda_{rec} \mathcal{L}_{rec}(E,G) + \lambda_{cs}\mathcal{L}_{cs}(E,G) \\
      & + \lambda_{enc}\mathcal{L}_{enc}(E,D_z)+ \lambda_{adv} \mathcal{L}_{adv}(E,G,D_x)
    \end{align*}
  \end{block}
  \begin{block}{Función de pérdida parte supervisada}
    \begin{equation*}
      \mathcal{L}_H(ITL) = \mathbb{E}_{x ~ p(x)} \left[ \| H-ITL(a)\|_2 \right]
    \end{equation*}
  \end{block}
\end{frame}

\begin{frame}{3FabRec}
  \textcolor{tudCyan}{\textbf{Bases de datos de la parte no supervisada}}
  \begin{itemize}
    \item VGGFace2 + AffectNet
    \item En total $2.1$ millones de imágenes.
  \end{itemize}
  \textcolor{tudCyan}{\textbf{Bases de datos de la parte supervisada}}
  \begin{itemize}
    \item 300W $= 3.148$ imágenes.
    \item AFLW $= 24.386$ imágenes.
    \item WFLW $= 10.000$ imágenes.
  \end{itemize}
\end{frame}

\begin{frame}{Preprocesamiento}
  \begin{itemize}
    \item Realizamos detección + \textit{cropping} de caras en las imágenes del dataset.
    \item Empleamos red auxiliar \textcolor{tudCyan}{Facenet}.
  \end{itemize}
\end{frame}

\begin{frame}{Preprocesamiento}
  \begin{figure}
    \centering
    \includegraphics[width=0.55\textwidth]{imgs/imagenes_ejemplo_bb.png}
  \end{figure}
\end{frame}

\begin{frame}{Reajuste 3FabRec}
  \begin{itemize}
      \item Bounding box cuadrado de lado $\max(h,w)$.
      \item Ampliación uniforme por el factor $\frac{crop_size+margin}{crop_size}$.
      \item Reescalado a tamaño $256 \times 256$.
      \item \textbf{Problema}: Vertex suele quedar fuera.
  \end{itemize}
  \begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{imgs/bounding_box_3fabrec.png}
  \end{figure}
\end{frame}

\begin{frame}{Experimentos}
  \begin{itemize}
    \item \textbf{Modelo Base}: entrenamiento ITLs.
    \item \textbf{Ajuste fino del Encoder}: entrenamiento ITLs + Encoder.
    \item \textbf{Ajuste fino del Decoder}: entrenamiento ITLs + Decoder.
    \item \textbf{Data Augmentation}: entrenamiento ITLs con técnicas de Data Augmentation.
  \end{itemize}
\end{frame}

\begin{frame}{Análisis Cuantitativo}
  \begin{columns}[onlytextwidth]
    \begin{column}{.5\textwidth}
      \centering \textcolor{tudCyan}{\textbf{Análisis a nivel global}}
      % square filling the column
      % place an image
      % horizontal position = 73%
      % vertical position = 45%
      % width = 40% of page
      \absimage{.25, .50}{.45}{imgs/boxplot_sumarize.png}
    \end{column}
    \begin{column}{.5\textwidth}
      \centering \textcolor{tudCyan}{\textbf{Análisis a nivel de landmark}}
      % square filling the column
      % place an image
      % horizontal position = 73%
      % vertical position = 45%
      % width = 40% of page
      \absimage{.75, .50}{.45}{imgs/tabla_landmarks.png}
    \end{column}
  \end{columns}
\end{frame}


\begin{frame}{Análisis Cualitativo: Modelo Data Augmentation}
  \begin{figure}
    \centering
    \includegraphics[width=0.99\textwidth]{imgs/image_basemodel.png}
  \end{figure}
\end{frame}

\begin{frame}{Elección de modelo $\rightarrow$ Data Augmentation}

  \begin{columns}[onlytextwidth]
    \begin{column}{.35\textwidth}
      % square filling the column
      % place an image
      % horizontal position = 73%
      % vertical position = 45%
      % width = 40% of page
      \absimage{.23, .50}{.45}{imgs/curvas_FinalModel.png}
    \end{column}
    \begin{column}{.65\textwidth}
      % square filling the column
      % place an image
      % horizontal position = 73%
      % vertical position = 45%
      % width = 40% of page
      \absimage{.73, .50}{.5}{imgs/image_finalmodel.png}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Comparativa}
  \begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{imgs/tanle_comparativa1.png}
  \end{figure}
\end{frame}

\begin{frame}{Comparativa}
  \begin{figure}
    \centering
    \includegraphics[width=0.55\textwidth]{imgs/table_comparativa2.png}
  \end{figure}
\end{frame}

\begin{frame}{Comparativa}
  \begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{imgs/compativa_cualitativa.png}
  \end{figure}
\end{frame}

\section{Conclusiones}

\begin{frame}{Conclusiones}
  Todos los objetivos cumplidos
  \begin{enumerate}
    \item Investigación + comparación con el estado del arte.
    \item Estudio sobre los Autoencoders y redes adversarias.
    \item Estudio de errores y preprocesamiento de los dato.
    \item Estudio experimental.
  \end{enumerate}
\end{frame}

\begin{frame}{Conclusiones}
  Además, se han logrado otros objetivos adicionales:
  \begin{itemize}
    \item Entrenar 3FabRec con landmarks faltantes.
    \item Método robusto con few-shot learning.
  \end{itemize}

  Este método es importante pues: 
  \begin{itemize}
    \item Buenos resultados con pocas imágenes.
    \item Situaciones similares al día día de un Antrpólogo forense.
  \end{itemize}
\end{frame}


\begin{frame}[allowframebreaks,t]{\bibname}
	% the 'I' is caused by 'allowframebreaks'
	\AtNextBibliography{\footnotesize}% or in the preamble \AtBeginBibliography{\small}
	\printbibliography
\end{frame}

\end{document}

